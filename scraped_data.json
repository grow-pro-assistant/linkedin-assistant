[
    {
        "id": 63,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "About the model inference performance leaderboard\nIt's very impressive how fast is Groq in the space of 70B models significantly outperforming other inference services\nWe have already discussed Groq here, see their paper about their processors and systems",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 64,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "The challenge of quantifying the likelihood associated with the outputs of Natural Language Processing (NLP) systems—whether pertaining to generation, classification, or extraction tasks—constitutes a fundamental obstacle in the deployment of NLP technologies within industrial settings for various reasons. These include the necessity to filter out potentially erroneous outputs, to accommodate the breadth of potential interpretations, and to furnish users with a machine-curated set of results that are not only relevant to their interests but also reflective of a nuanced understanding of all conceivable contexts, interpretations, and intentions. Drawing from empirical observations, it is evident that the implementation of NLP at a substantial scale in real-world applications is contingent upon the assignment of probabilistic, confidence, or analogous metrics to every output generated by these systems. Such metrics are crucial for subsequent processes that rely on these outputs, including information retrieval, result ranking, and the final presentation of outcomes. In this vein, the ASPIRE initiative—a collaborative effort between Google and the University of Wisconsin-Madison—aims to augment Large Language Models (LLMs) with the capability to generate a 'selective score.' This score serves as an indicator of the probabilistic validity of a given response, thereby enhancing the reliability and applicability of LLMs in practical scenarios.",
        "time": "21h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 65,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "„Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. Claude 2 and GPT-4 solve a mere 4.8% and 1.7% of instances respectively, even when provided with an oracle retriever“",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 66,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I received questions about the evaluation of image and video-generating models. Two papers caught my eye, each setting new benchmarks in the text-to-image and text-to-video domains. Let's dive into what makes them stand out, especially for us in the business world keen on leveraging AI's full potential.\nFirst up, we've got the HEIM benchmark for text-to-image models. This paper is a game-changer for how we evaluate AI's ability to turn words into visuals. The authors didn't just set a benchmark; they dissected the evaluation process into digestible pieces that matter for businesses. It's a smart move, considering how varied our needs can be across different industries. They've also hit the nail on the head by emphasizing the need for human touch in evaluations – a reminder that AI hasn't quite mastered the art of nuanced judgment just yet.\n\nOn the flip side, the VBench benchmark is making waves in the text-to-video space. Here, the focus is on assessing video quality and how well these creations match up with human instructions. It assesses multiple aspects of video semantics, aesthetic quality and others.\n\nBoth benchmarks, HEIM for images and VBench for videos are open-source contributions (links are in the papers), offering both the tools and data needed for anyone to run such or similar tests for their models. Both papers contain a detailed review of other contributions in evaluation of TTI and TTV models\nBoth text-to-image and text-to-video types of models will be highly used in industries producing value for customers in e-commerce and retail, home-improving services and landscaping, fashion/apparel, etc, and many other industries, and having good evaluation practices is crucial to make them successful.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 67,
        "person_name": "https://www.linkedin.com/in/sarahxguo",
        "text_description": "Congratulations to Sridhar Ramaswamy and outgoing CEO Frank Slootman after an amazing run.\n\nThere is no better person to run $SNOW now -- Sridhar is a technical pioneer, deep understanding of data infrastructure/AI, and proven ambitious, disciplined leadership in $100B+ revenue business.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 68,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Sounds awesome \n„The AI assistant has had 2.3 million conversations, two-thirds of Klarna’s customer service chats\nIt is doing the equivalent work of 700 full-time agents\nIt is on par with human agents in regard to customer satisfaction score\nIt is more accurate in errand resolution, leading to a 25% drop in repeat inquiries\nCustomers now resolve their errands in less than 2 mins compared to 11 mins previously\nIt’s available in 23 markets, 24/7 and communicates in more than 35 languages\nIt’s estimated to drive a $40 million USD in profit improvement to Klarna in 2024“",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 69,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "AnyScale/Ray launched a leaderboard dedicated to evaluating the inference performance of large language models (LLMs) across different providers. This leaderboard systematically assesses various performance metrics relative to model size for an array of LLM inference services, facilitating comprehensive comparisons.\nThe leaderboard, which is continually updated, along with the software's source code, are both publicly accessible. The advent of LLMs and generative AI has significantly underscored the importance of infrastructure engineering. This is because an organization's throughput and cost-efficiency, its ability to scale LLM applications, innovate and test new LLMs and LLM apps, and manage complex systems built upon LLM technology are all heavily influenced by its underlying infrastructure. As LLMs become increasingly integral to a wide range of applications, the demand for skilled infrastructure engineers is set to rise sharply.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 70,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "it's quite exciting. The Information reports that OpenAI is shifting its AI focus to software that operates tasks (the article is from Feb 7) - the Information is a very reliable source for such information. \nThere are various dimensions to consider when defining 'agents' and the range of actions they are capable of performing. It is becoming increasingly apparent that equipping Large Language Models (LLMs) with the ability to learn actions beyond mere language tasks represents a significant advancement. For instance, the development of ToolFormer by MetaAI, followed by subsequent research in LLMs that involve 'tools' (such as APIs and functions), illustrates this shift. Historically, most systems have been designed with a focus on information retrieval rather than action-oriented tasks (or other text -> text tasks). This is largely due to the complexities involved in developing user-friendly interfaces that facilitate control over actions. Consider mapping applications, which currently specialize in search and recommendation functions. However, real-world tasks often require both discovery and search capabilities aimed at achieving broader objectives. For example, planning a trip from Vancouver, CA, to Portland might involve a detailed search for specific locations. Presently, this process is manual and cumbersome. A task-driven action interface that enables discovery, collection, planning, and organization would greatly enhance user convenience and directly address their needs. The challenge has been that designing intuitive controls for action-oriented tasks is difficult, which is why even when big tech companies introduce enhancements that add significant utility and value for some users, these features are not widely adopted due to their complexity.\nNow, with advancements in LLM technology, there is potential to develop easy and intuitive interfaces that can execute a series of complex logic actions based on simple human interactions. This opens up possibilities not only for creating new applications but also for significantly improving traditional search/discovery platforms used in web search, travel and vacation planning, e-commerce, and retail, among others. Potential approaches might include the ToolFormer method (learning API calls) or alternative strategies (such as learning to write Domain-Specific Language (DSL) specifications based on natural language interactions).\nIt is exciting to note OpenAI's focus on this strategy, given its potential to dramatically impact various industries through the creation of new applications and significant enhancements to user utility, value, and ease of use.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 71,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Advanced video models, once made accessible in open source and capable of operating efficiently with minimal cloud expenses and on compact devices, are poised to become significant catalysts for the development and exploration of new applications. The potential for application spans a wide range, from in-home assistance to automotive integration, among numerous other possibilities.",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 72,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Following the webinar on the evaluation of Large Language Models (LLMs), I have received numerous inquiries on various aspects of this topic. These include inquiries on statistical techniques aimed at reducing sample size, comparisons between graded and ungraded LLMs, the use of software libraries for evaluation purposes, and specific methodologies for assessing LLMs designed for executing tasks within the external environment, among other subjects.\nIn the industry, I find myself missing the culture of 'Summer Schools,' which are popular in academia. For instance, I particularly appreciated the European Summer School on Logic, Language, and Computation, among others. The comprehensive evaluation of LLMs is a substantial field that undoubtedly merits a detailed exploration. Ideally, this would encompass a series of webinars totaling at least five hours or, more effectively, a combination of five hours of theoretical instruction coupled with five hours of practical exercises. Such a program would provide participants with actionable knowledge that could be directly applied to the development of their own LLMs and LLM applications.",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 73,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 74,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "In the past few weeks, I have had the opportunity to engage with leaders from various companies across Europe and Asia. One surprising revelation from these discussions is the ongoing challenge of developing high-quality search engines in non-English languages, including French, Arabic, Japanese, and German, and others. Despite their critical role as major revenue generators in numerous industry sectors such as travel and hospitality, e-commerce and retail, advertising, and food delivery, the absence of robust open-source tools for creating search engines in these languages remains a significant hurdle.\nFrom this perspective, the emergence of open-source, multilingual large language models (LLMs), like the French-English LLM I recently highlighted, is indeed a positive development. However, it's important to recognize that LLMs, even when applied in search engines, represent only a fraction of the overall solution and there's still a long way to go. A major challenge lies in the fact that many companies are forced to develop numerous components from scratch due to the lack of comprehensive support for non-English languages in existing open-source search tools. This situation is quite surprising and underscores the need for further advancements in this area.",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 75,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I'll present a webinar \"Evaluating LLM Models for Production Systems: Methods and Practices\" on Tuesday (Feb 27)\nPlease, join us for a discussion and a presentation\n600 people have signed up already",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 76,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "„This begs an obvious question. How did Perplexity, a company founded by four people (it has since grown to roughly 40) less than two years ago, cut through the problems that seemingly made AI terrible for search?“\n„Perplexity initially hoped to build an AI-powered Text-to-SQL tool. But something different started brewing in the company’s Slack channels.“",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 77,
        "person_name": "https://www.linkedin.com/in/ericalunalee",
        "text_description": "Join us on Mar 7 in Culver City for an event on \nhashtag\n#GenerativeAI! Hosted by my nonprofit WomenOfAI.org with Scopely featuring our keynote speaker Faith 'Aya' U. of ImmerseMe & a special intro by Tricia Bertero of Scopely!\n\nTalk won’t be recorded or live streamed so try to join in person if local. This low cost ticket has 100% proceeds going to our nonprofit.\n\nSign Up:",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 78,
        "person_name": "https://www.linkedin.com/in/philipp-schmid-a6a2bb196",
        "text_description": "Gemma an open Gemini LLM released by Google! 🤯 Google just released Gemma, their first open LLM based on Gemini, which outperforms Mistral AI 7B! 🤯 Gemma comes in 2 different sizes, 2B & 7B, and can be commercially used! 🔥\n\n𝗧𝗟;𝗗𝗥\n🧮 2B & 7B parameter Instruction and base versions, not multimodal\n🔠 Trained on 2T tokens (2B) and 6T tokens (7B)\n💰Commercial use allowed ✅\n🪟 8192 default context window\n🥇 7B scores 64.56 on MMLU and avg. of 63.75 on the Leaderboard\n⚖️ Used RLHF for instruction model\n🧠 7B trained on 16 TPUv5e pods for 23 days\n📱 2B can run on Phones\n🤗 Available on\n☁ 1-click deployment tofrom Hugging Face\n\nBlog:\nModels:\n\nIt's exciting to see Google committing to open AI with their first open LLM release. 🤗 Let’s show them why we should stay open! 🫶🏻",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 79,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "There is an old compendium of numbers to be known by every computer engineer presented by Jeff Dean some time ago (they changed over time, see a link below)\nCertainly, most numbers from that table are typically in the nanosecond-microsecond range\nNow, with the advent of LLM, there is a set of numbers that need to be known, that are quite from the opposite range, for example, typical LLM training is around 10^24 FLOPS",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 80,
        "person_name": "https://www.linkedin.com/company/oxforduni/",
        "text_description": "Yesterday, Oxford University hosted Prof Geoffrey Hinton, the ‘Godfather of AI’, to deliver its annual Romanes Lecture at the Sheldonian Theatre.\n\nWatch in full ⬇️",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 81,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "A team of researchers from Stanford has released an open-source framework to evaluate and probe LLM negotiation capabilities. The framework is especially interesting since practical products and use cases based on LLMs will be in goal-reaching scenarios similar to negotiations. The research studies various LLMs, including GPT and Claude families, on how they can handle negotiations. The paper published a lot of interesting insights about the behavior of several LLMs. Designing the proper way to evaluate LLMs for such complex settings is important to build evaluation frameworks for many other goal-based systems.\n(there is a link to GitHub rep in the paper)",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 82,
        "person_name": "https://www.linkedin.com/in/pieromolino",
        "text_description": "Today we are releasing \nhashtag\n#LoRA Land: 25 fine-tuned \nhashtag\n#mistral-7b \nhashtag\n#llm that outperform \nhashtag\n#gpt4 on task-specific applications.\n\nAll 25 fine-tuned models\n📈 Outperform GPT-4, GPT-3.5-turbo, and mistral-7b-instruct for specific tasks\n⚡️ Are cost-effectively served from a single\nthrough\n\n💰 Were trained for less than $8 each on average\n\nThis is pretty important imho. It shows how smaller, task-specific models are the best option for deploying LLM applications in a cost effective manner after you are done building a prototype with GPT-4 and you want to keep costs in check and keep control of your models.\n\nYou can prompt all of the fine-tuned models today and compare their results to mistral-7b-instruct in real time in our LoRA Land UI.\n\nAnd you can also download all of them through!\n\nLinks in the comments.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 83,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "We will see how much innovation will come from new neural architectures\nThe authors report recurrent architecture (Receptance Weighted Key Value (RWKV)) that has an accuracy similar to the transformer models of the same size (14B) but requires 10X less computational resources (may go to 100X)",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 84,
        "person_name": "https://www.linkedin.com/in/dr-zachary-s-brown",
        "text_description": "This is a really neat approach!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 85,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 86,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/g7RkaPX6",
        "time": "2w ",
        "url_links": [
            "https://lnkd.in/g7RkaPX6"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 87,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Very impressive \nCohere AI releases an open source LLM for 101 languages. \nIt will help to many companies to develop better search , recommendations , call AI etc in many markets across the world",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 88,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "270M parameter transformer model thatcid trained on 10m games. Achieve 2700 elo rating ( Großmeister level ). Solves complicated chess problems \nI see it is a very big achievement as many practical problems can be represented as games and potentially this results is applicable well beyond chess",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 89,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 90,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Enjoyed this Stanford class from 2023 \nAll instructors are great",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 91,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Quite interesting talk \nHow ‚low level‘ features are important in implementation of ‚high level‘ functionality",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 92,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Quite a comprehensive survey of results in multi modal LLMs",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 93,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "UC Berkeley is well known for its open-source projects that are hugely influential and become big companies (Databricks) or influential in academic and industrial research \nThere is a new project from Berkeley that can become such a project\nIt's a system to control interactions with LLMs (much needed) SGLang (Structured Generation Language)\nit's a backend system to implement the control and the language to specify the instructions\n\nthe control is over the compilation, batching, cashing, parallelsm etc\nSee improvements in throughput of LLM inferencing vs vLLM\nplenty of models are supported in the initial release, see the project page\nAlready supports multi-modal inputs\nA lot of work on optimization.\nsee\n\"During the development of the SGLang runtime, we identified a crucial optimization opportunity for complex LLM programs, which are poorly handled by current systems: KV cache reuse. KV cache reuse means different prompts with the same prefix can share the intermediate KV cache and avoid redundant memory and computation. In a complex program that involves multiple LLM calls, there can be various KV cache reuse patterns. Figure 3 below illustrates four such patterns, which are common in LLM workloads. While some systems are capable of handling KV cache reuse in certain scenarios, this often necessitates manual configurations and ad-hoc adjustments. Moreover, no existing system can automatically accommodate all scenarios, even with manual configurations, due to the diversity of possible reuse patterns.\"\nand after",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 94,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gMTDzcwr",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/gMTDzcwr"
        ],
        "url_texts": "\nThe request is blocked.\nThe request is blocked.\n"
    },
    {
        "id": 95,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Results of Google and MIT on tuning and using LLMs to multi\nModal data where other data includes time series ( sensor data and other ). Performs well on five tasks: mental health, activity tracking, metabolism, sleep, and cardiology.\nResults are practically interesting because this type of multi modality is very common in many other domains and areas of business. Such tuning of llm will have very practical consequences in applying LLM well beyond textual tasks to solve real problems",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 96,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "The phenomenal story of Google Maps (and Pokemon , frankly speaking, never played )\nMaps/Local Search is one of the most impactful products of these times with an impact on lives of billions of people every day and millions of businesses use it through APIs, etc",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 97,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "insights how Perplexity AI works and how this work was inspired by FreshLLM paper\nand a story from Perplexity founders about the evolution of the system how it started as a wrapper over Bing and OpenAI and now it has its own index and LLMs",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 98,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "DeepMind paper about the geometric problem solver contains a lot of very interesting insights about how they define the task, prepared the data to solve this particular task. Perhaps, if they would try to solve the generic prover problem, they would not succeed. And , again as in many other recent successful projects in the LLM area, synthetic data were important . And integration with external solvers LLM does not solve the problem. It guides the symbolic deduction solvers through branching points helping to find the solution in very large branching symbolic space\n\"We focus on Euclidean plane geometry and exclude topics such as geometric inequalities and combinatorial geometry. \"\n\"we extracted 100 million synthetic theorems and their proofs, many with more than 200 proof steps, four times longer than the average proof length of olympiad theorems. We further define and use the concept of dependency difference in synthetic proof generation, allowing our method to produce nearly 10 million synthetic proof steps that construct auxiliary points, reaching beyond the scope of pure symbolic deduction\"\n\"We pretrain a language model on all generated synthetic data and fine-tune it to focus on auxiliary construction during proof search, delegating all deduction proof steps to specialized symbolic engines\"",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 99,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Six bit quantization to reduce size of LLMs without loss of models’ performance for a set of tasks \n\n„Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU, achieving 1.69x-2.65x higher normalized inference throughput than the FP16 baseline“",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 100,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Two hydrogen atoms walk into a bar. One says, \"I think I've lost an electron.\" The other asks, \"Are you sure?\" The first one replies, \"Yes, I'm positive!\"\nTwo neutrinos walk into a bar. The bartender says, \"We don't serve your kind here.\" The neutrinos respond, \"Don't worry, we're just passing through.\"\nA neutron walks into a bar and asks the bartender, \"How much for a drink?\" The bartender replies, \"For you, no charge!\"\nA superconductor walks into a bar. The bartender says, \"Get out! We don't serve your kind here.\" The superconductor leaves without any resistance.\nThe bartender says to the Higgs boson, \"We don't allow your kind in here.\" The Higgs boson replies, \"But without me, you can't have mass appeal!\"\nchatGPT",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 101,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I came across a paper that discusses how instruction tuning can improve Language Model (LLMs) performance for search tasks. The authors explore how this technique can enhance query and document understanding and matching. The results are obtained on different LLMs, and it looks like the technique is generic and LLM-independent. I expect that the impact of this paper both on LLM and search/IR community will be very big\n\nThe paper provides a lot of useful information on instruction design, volume, and other important parameters to make such tuning practical. The authors also share their data, making it easier for companies building search/ QA / engines to create their own datasets.\n\nSearch/IR/QA is expected to be the biggest application of LLMs both in monetary impact and the number of users for a very long time. I believe this paper is an important development with a big impact, and I highly recommend giving it a read. Check it out here:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 102,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "From ft.com\n\"Dall-E, Stable Diffusion and Midjourney have made what might have taken an extremely skilled illustrator or animator a week to do into something any of us can commission in a few moments. There is no doubt those jobs are at terminal risk. Architects are already using AI to handle mundane tasks from distributing parking spaces and bathrooms to arranging blocks on an urban plan.\"",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 103,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Impressive performance for a small model. Outperforms Phi2 and other models of similar size",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 104,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Evolution\nFrom min to nano :)\nminGPT \n\n->\nnanoGPT\n\ninteresting to follow such projects of rewriting large chunks of code\nin new directions",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 105,
        "person_name": "https://www.linkedin.com/in/ondrej-linda-56654810",
        "text_description": "https://lnkd.in/g-27XjVp I am really proud to be able to share some of the impactful AI work out teams at Zillow are doing. We are all about innovation in the Real Estate AI space, but doing it responsibly and with respecting Fair Housing guardrails. Big acknowledgment to:,,and",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/g-27XjVp"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 106,
        "person_name": "https://www.linkedin.com/in/michaelpcassidy",
        "text_description": "SpaceX successfully sends first text message from Starlink to a cell phone!\n\nJAN 10, 2024\nSPACEX SENDS FIRST TEXT MESSAGES VIA ITS NEWLY LAUNCHED DIRECT TO CELL SATELLITES\nOn Monday, January 8, the Starlink team successfully sent and received our first text messages using T-Mobile network spectrum through one of our new Direct to Cell satellites launched six days prior. Connecting cell phones to satellites has several major challenges to overcome. For example, in terrestrial networks cell towers are stationary, but in a satellite network they move at tens of thousands of miles per hour relative to users on Earth. This requires seamless handoffs between satellites and accommodations for factors like Doppler shift and timing delays that challenge phone to space communications. Cell phones are also incredibly difficult to connect to satellites hundreds of kilometers away given a mobile phone’s low antenna gain and transmit power. Starlink satellites with the Direct to Cell payload are equipped with innovative new custom silicon, phased array antennas, and advanced software algorithms that overcome these challenges and provide standard LTE service to cell phones on the ground. As the global leader in rocket and satellite launch and manufacturing, SpaceX is uniquely positioned to rapidly scale our Direct to Cell network and will rapidly launch a constellation of hundreds of satellites to enable text service in 2024 and voice, data, and Internet of Things (IoT) services in 2025.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 107,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gU6x-5zC",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/gU6x-5zC"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 108,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "This episode of 30 minutes is very insightful\nMike was leading Google Loon and built several successful companies including Appollo Fusion (Electric propulsion system for low orbit spacecraft )",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 109,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "“The rabbit hole framework tests how robust the LLM’s guardrails are in preventing it from “saying” toxic things that its rich generative component was able to come up with. It also takes the classic frog in boiling water approach, where it iteratively tests the filters by slowly nudging the generated content toward more and more toxicity. “",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 110,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently developed statistical methods that become important in LLM evaluation (to compute confidence intervals on evaluation on a small labeled set). Prediction-Powered Inference",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 111,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Preparedness framework from OpenAI\nhttps://lnkd.in/dm-JMhsw",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/dm-JMhsw"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 112,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "From Daniel Coleman",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 113,
        "person_name": "https://www.linkedin.com/in/orenetzioni",
        "text_description": "This is so cool! Way to go Luis Ceze",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 114,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Join the Zillow product team to work on our Generative AI Platform \nhttps://lnkd.in/gGcybPuw",
        "time": "2mo ",
        "url_links": [
            "https://lnkd.in/gGcybPuw"
        ],
        "url_texts": "\nThe request is blocked.\nThe request is blocked.\n"
    },
    {
        "id": 115,
        "person_name": "https://www.linkedin.com/in/bjarnestroustrup",
        "text_description": "A talk I gave just got posted:\n\nA talk in honor of Sorin Istrail: C++: Evolving a Useful Language at the Sorinfest at Brown University.With the exception of mine, the talks are about biology (especially genomics and computational biology), including John vonNeumann Distinguished Lectures.My talk is a lightning tour of key ideas behind C++ (14 minute plus Q&A).\n\nHappy New Year.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 116,
        "person_name": "https://www.linkedin.com/in/agorodilov",
        "text_description": "great article!",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 117,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "-",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 118,
        "person_name": "https://www.linkedin.com/in/omarsar",
        "text_description": "Nice work surveying 300+ papers and summarizing research developments to look at in the space of Generative AI. \n\nIt covers computational challenges, scalability, real-world implications, and the potential for Gen AI to drive progress in fields like healthcare, finance, and education.\n\n\n\n---\nI also provide technical summaries of the latest LLM research papers here:",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 119,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gaWdzKaB",
        "time": "2mo ",
        "url_links": [
            "https://lnkd.in/gaWdzKaB"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 120,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "How Not to Be Stupid About AI, With Yann LeCun\nBy Steven Levy",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 121,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Winter is coming in quantum computing ?\n\n\"Meta’s head of AI research Yann LeCun recently made headlines after pouring cold water on the prospect of quantum computers making a meaningful contribution in the near future. Speaking at a media event celebrating the 10-year anniversary of Meta’s Fundamental AI Research team he said the technology is “a fascinating scientific topic,” but that he was less convinced of “the possibility of actually fabricating quantum computers that are actually useful.” \"",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 122,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "from Apple (there are several very interesting published by Apple AI team recently, Apple certainly becomes a company leading in AI )\n\"Our method takes only a monocular video with a small number of (50-100) frames, and it automatically learns to disentangle the static scene and a fully animatable human avatar within 30 minutes\"\nHUGS is up to 100 times faster in training and rendering than previously published methods and outperforms  Vid2Avatar and NeuMan on 3D reconstruction quality.\n\n\nDeploying LLM on devices with limited memory ()\n\"Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks\"\nInference latency is improved 4-5 times vs naive implementation on Apple M1 Max CPU and 20-25 times on GPUs",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 123,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Evaluation of multi-turn RL in LLMs\nImportant for goal-oriented LLM interactions\nThere is very little work is done in academia in this direction (multi-turn) of evaluation of LLMs, despite it's very important for commercial applications of LLMs",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 124,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Good Local search / maps in car experience can be a big win for every driver.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 125,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "“In fact, the banking sector is expected to have one of the largest opportunities in generative AI, according to McKinsey & Company. Gen AI could add the equivalent of $2.6 trillion to $4.4 trillion annually in value across the 63 use cases the McKinsey Global Institute analyzed. While not the largest beneficiaries within banking, asset management could see $59 billion in value and wealth management could see $45 billion.”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 126,
        "person_name": "https://www.linkedin.com/in/jalammar",
        "text_description": "Awesome poster presentation by Niklas Muennighoff for the paper \"Scaling Data-Constrained Language Models\" at \nhashtag\n#NeurIPS2023 \n\nKudosColin Raffel",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 127,
        "person_name": "https://www.linkedin.com/in/goericbrown",
        "text_description": "Network: my team is hiring for a Sr. Engineer, come help us build Zillow's ad platform. DM me if interested, this is a remote role. \nhashtag\n#zillow",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 128,
        "person_name": "https://www.linkedin.com/in/bilogorskiy",
        "text_description": "President Zelenskiy arrives at the IMF and World Bank.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 129,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Many of us learned neural networks from Christopher Beshop's book \"Neural Networks for Pattern Recognition\" 20+ years ago when it was a part of any advanced AI curriculum (I read it in 2001-2003 when I was in Vienna and Manchester). It was a classical book by 2005, very well written, easy to understand but deep at the same time, mathematically formal, and at the same time with a lot of practical examples\nA new book on Deep Learning from the same author",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 130,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Scientists from ETH (Thomas Hofmann, Bobby He ) found a radical simplification of the transformer architecture without loss of accuracy and significant improvement of training and inference speeds",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 131,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I see Rust is becoming one of the leading AI languages",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 132,
        "person_name": "https://www.linkedin.com/in/emollick",
        "text_description": "There has been a lot of past work trying to use AI to help with medical decision-making, but they used other forms of AI, not LLMs. Now Google trained a LLM specifically for diagnoses.\n\nIn a randomized trial, AI correctly diagnosed 59% of hard cases. Doctors only got 33% right (Interestingly, doctors & AI working together did well, but not as good as AI did alone)",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 133,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gHeZWb5d",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gHeZWb5d"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 134,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Evaluation of models and understanding of models becomes a big and complicated science area on its own\nhttps://lnkd.in/gghB_9X8",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gghB_9X8"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 135,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Singapore",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 136,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gfkuj8sz",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gfkuj8sz"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 137,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/g_zD_FX6",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/g_zD_FX6"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 138,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "an insightful discussion\nProfs. Geoffrey Hinton, Yann LeCunn, Andrew Ng, Max Tegmark, also Nando de Freitas, and other prominent thinkers",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 139,
        "person_name": "https://www.linkedin.com/in/pramodith",
        "text_description": "A lot of new prompting techniques were proposed in the last month or so. Here’s a list of some of the ones that I came across:\n\n1. Chain of Note: Ask the LLM to generate notes of the relevant parts of the input text and then produce a response.\n\n2. Contrastive Chain of Thoughts: Using correct and wrong explanations in your few-shot examples.💭\n\n3. System 2 Attention: Ask the LLM to extract the relevant parts of the input text, ignoring any user-stated biases, irrelevant info, etc., in step 1. Step 2 uses the output of step 1 to answer the user’s query.🔍\n\n4. Thread of Thoughts: Useful in long contexts or RAG applications where multiple docs can be returned. The first stage asks the LLM to apply the chain of thought to moderately sized segments of the input, each segment is accompanied by an analysis generated by the LLM. The second stage uses the output of the first stage to produce an output.🧵\n\n5.Take a Deep Breath: Just add that phrase to your prompt 😅🌬️\n\n6. Emotional Prompts: Let LLMs know that you really need them to answer correctly😢\n\n7. Simulation Theory of Mind: Stage 1, take note of the perspective the LLM is supposed to assume, ask it what they know from that perspective. Stage 2 answer the question based on the perspective and the knowledge that comes with it.🤖\n\n8. Step Back Prompting: Ask the LLM to take a step back and generate some fundamental knowledge that can be used to answer the question and then provide the final answer.🔙\n\nSome common themes across these are that 2 stage prompts, where the first prompt refines the initially provided documents/text and the second prompt uses this refinement/additional info to produce the final answer.\n\nFound these techniques by following awesome creators like,,.\n\nIf you know of other prominent techniques that I’ve missed, drop a comment below, for everyone to learn! 💬",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 140,
        "person_name": "https://www.linkedin.com/in/satyanadella",
        "text_description": "Today, we’re sharing new data that shows the remarkable productivity gains Copilot is already driving for early users. \nhashtag\n#MSIgnite",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 141,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "When it comes to Deep Learning, I think nobody symbolizes the field better than Geoffrey Hinton, the father of Deep Learning. He even coined the term! Here are his biggest contributions to the field:\n\n- 1986 - He invents the Boltzmann machines\n- 1985 - He proposes a new learning algorithm for Boltzmann machines\n- 1986 - He is credited as one of the inventors of the Back-propagation algorithm\n- 1991 - He invents the Mixture of Experts\n- 2006 - He proposes an algorithm to train Deep Belief Nets. This is the article that led to the term \"Deep Learning\"\n- 2006 - He shows how to build Autoencoders with Neural Networks\n- 2008 - He invents t-SNE, a new technique for dimension reduction\n- 2009 - He presents an algorithm to train Deep Boltzmann machines\n- 2009 - Trains Restricted Boltzmann Machines and Deep Belief Networks with the CIFAR-10 dataset\n- 2010 - Shows the improved performance of Restricted Boltzmann Machines with ReLU\n- 2011 - Shows how to build a generative text model with Recurrent Neural Networks\n- 2012 - Invents RMSprop in a course lecture (!!!)\n- 2012 - Proposes the Feature Dropout technique to improve networks\n- 2012 - Suggests mini-batch gradient descent in a course lecture\n- 2012 - Deep Learning for speech recognition\n- 2014 - Revolutions computer vision capabilities with AlexNet (the most cited paper of his whole career)\n- 2014 - He proposes the Dropout technique to reduce overfitting\n- 2014 - the CIFAR 10 dataset is made available\n- 2015 - He invents the Distillation Network to reduce the size of models\n- 2016 - He invents the Layer Normation technique (used in every Transformer architecture\n- 2017- He proposes CapsNets, or Capsule networks aiming to overcome some limitations of CNNs, particularly in the area of understanding hierarchical relationships between objects and their parts within an image\n- 2022 - He presents a new alternative to the Back-propagation algorithm: the Forward-forward algorithm\n\nNow, the guy has 327 publications, so I couldn't capture everything here, but I believe this encapsulates his most impactful works. Considering the trend, it seems a lot more is going to come from him in the coming years!\n\n\n--\n👉 Let me help you become better at Machine Learning:\n--",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 142,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gAcMcJTp",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gAcMcJTp"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 143,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "LLM are strong fact verifiers, even if they hallucinate\nEven, surprisingly, stronger hallucination generators can be strong verifiers\n\"Extensive experiments further reveal that FLANT511B, the least factual generator in our study, even surprisingly outperforms GPT3.5 and ChatGPT for fact verification.\"",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 144,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gJdGuJYh",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gJdGuJYh"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 145,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I think it's an important development to apply LLM for solving particular hard tasks (chip design) and will lead to the development of LLMs solving other hard problems in other business, engineering, and science domains",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 146,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "If you ever forget :)",
        "time": "4yr ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 147,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/g9wirSne",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/g9wirSne"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 148,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/dK6CS2PD",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/dK6CS2PD"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 149,
        "person_name": "https://www.linkedin.com/company/zillow/",
        "text_description": "We’re proud to have Chief Economist Skylar Olsen join the new Center for Strategic and International Studies (CSIS) Geoeconomic Council of Advisers, where she’ll represent Zillow among other industry leaders in discussions surrounding the global economy.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 150,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/dH7kBrzC",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/dH7kBrzC"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 151,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gcYE4gWs",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/gcYE4gWs"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 152,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Explainable AI ( from Ricardo Baeza-Yates )",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 153,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gWRYYi76",
        "time": "4mo ",
        "url_links": [
            "https://lnkd.in/gWRYYi76"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 154,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gthD-tdr",
        "time": "4mo ",
        "url_links": [
            "https://lnkd.in/gthD-tdr"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 155,
        "person_name": "https://www.linkedin.com/in/bradleyaberning",
        "text_description": "Zillow just reported solid financial results that meaningfully outperformed the broader real estate industry for the fourth consecutive quarter. We’ve made great progress rolling out and expanding our product offerings to more markets, and we’re looking to continue this into the new year.\n\nThank you to Zillow employees, investors, customers and other supporters for being part of our journey to simplify and streamline the real estate transaction.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 156,
        "person_name": "https://www.linkedin.com/in/lilinwang",
        "text_description": "After tens of nights and weekends, I'm proudly to open source our github project, TigerLab! (Check it out here: https://lnkd.in/dvux7YFq) \n\n🌐 TigerLab: A Framework for Trustworthy LLM Development\n\nHere's what you can unlock with TigerLab's toolkits:\n🔍 TigerRAG: Information Retrieval\n🧠 TigerTune: Model Fine-Tuning\n🔄 TigerDA: Data Augmentation\n🛡 TigerArmor: AI Safety\n\nI was exploring the capabilities of LLM with my friends recently. Our exploration revealed a gap—a lack of a systematic approach for achieving high-quality, secure end-to-end LLM implementation. Driven by this insight, we poured our hearts into this project. Today, we proudly open sourced it to you.\n\nTo all the dedicated LLM researchers and developers out there, we invite you to explore, contribute, and innovate with TigerLab. Want a closer look? Schedule a call with us ()!\n\nYour insights and feedback are the keystones of our progress. Together, let's make AI more accessible and secure!",
        "time": "4mo ",
        "url_links": [
            "https://lnkd.in/dvux7YFq)"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 157,
        "person_name": "https://www.linkedin.com/in/zhanyong-wan",
        "text_description": "This concludes my series on \nhashtag\n#Google's \nhashtag\n#culturechange. The main purpose of me writing this series is to document history from my perspective - I believe that history is the sum of all who choose to write it.\n\nI know many great people working at Google, and I wish them best of luck in restoring Google's culture and glory. While each of us can draw our own conclusion, I wish that by reflecting on what happened in the past, we can all become a little bit wiser.",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 158,
        "person_name": "https://www.linkedin.com/in/zhanyong-wan",
        "text_description": "This concludes my series on \nhashtag\n#Google's \nhashtag\n#culturechange. The main purpose of me writing this series is to document history from my perspective - I believe that history is the sum of all who choose to write it.\n\nI know many great people working at Google, and I wish them best of luck in restoring Google's culture and glory. While each of us can draw our own conclusion, I wish that by reflecting on what happened in the past, we can all become a little bit wiser.",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 159,
        "person_name": "https://www.linkedin.com/in/naga-gurumoorthy-a0b21a2",
        "text_description": "I’m happy to share that I’m starting a new position as Corporate Vice President at Microsoft!",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 160,
        "person_name": "https://www.linkedin.com/in/peterfbell",
        "text_description": "I’m happy to share that I’m starting a new position as Chief Technology Officer at Geeks Who Lead! \n\nIn addition to supporting my research into the future of the engineering org,will also be supporting the creation of a completely new set of niche communities for senior engineering leaders to connect with and learn from their peers.\n\nWe're going to start off with communities for (series A+) Startup CTOs and for CTOs at Scale (80-100+ FTEs in prod/eng with established, mature systems) with regular monthly meetups - initially in NYC and online.\n\nIf you'd like to learn more, I'll be organizing an event on Wednesday evening in NYC where there are still a few seats for Directors, VPs and CTOs at companies with at least 20 FTEs in product/engineering - sign up here!\n\nIf you're busy Wednesday or don't qualify, please do sign up here to be kept up to date on our progress.\n\nReally excited to be back in the business of building learning communities for senior engineering leaders!\n\nAnd keep an eye out this month or next for the relaunch of the podcast including interviews with the CTOs atand many more!",
        "time": "19h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 161,
        "person_name": "https://www.linkedin.com/in/jason-roselander-267a86",
        "text_description": "In the nearly two decades I spent at Amazon and Google, I never got an email from the CEO that was actually related to detailed, technical matters. (I did play dodgeball with Bezos tho.) They were both great companies, and I think that's pretty much expected.\n\nDay 4 at NVIDIA, Jensen asked some pretty specific questions about someone's status report.\n\nThis should be exciting!",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 162,
        "person_name": "https://www.linkedin.com/in/themza",
        "text_description": "Update! I have a new role at AWS as VP for AI products, responsible for working with our customers & internal teams to drive success with AI in every industry.\n\nI've been at Amazon for nearly 14 years, and have been involved in machine learning and artificial intelligence almost the whole time (one of my first product launches was the first Amazon Machine Learning service!).\n\nI've come to view generative AI as more of a discovery than an invention - one which we are still learning how to harness, understand, and apply. There is so much momentum around AI today, and I consider it a great honor to get the chance to add to that momentum by safely exploring, advancing, and channeling this discovery with so many smart, driven, innovative customers, partners, and teams at Amazon.\n\nLet's go!",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 163,
        "person_name": "https://www.linkedin.com/in/jennifer-elias-845b1130",
        "text_description": "Google CEO Sundar Pichai sent a memo calling the company's AI image mistakes \"completely unacceptable\" and said it plans to launch new processes for AI launches.\n\nPichai called the issues “problematic” and said they “have offended our users and shown bias.\"\n\nPichai’s memo said the teams have been working around the clock to address the issues and that the company will instate a clear set of actions and structural changes, as well as “improved launch processes.”\n\nYou can find the full text of the memo here",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 164,
        "person_name": "https://www.linkedin.com/in/sridhar-ramaswamy",
        "text_description": "I’m honored and beyond excited as we start a new phase of the Snowflake journey together. Snowflake is a once-in-a-generation company and a truly special place. \n\nI love our customer-first obsession to deliver a tightly integrated and efficient platform. I am excited by the expansiveness of our vision around a true Data Cloud: where data and applications come together for our customers and our developer partners.\n\nAnd I can’t wait to show customers the massive business value we can deliver together with AI!",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 165,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Following the webinar on the evaluation of Large Language Models (LLMs), I have received numerous inquiries on various aspects of this topic. These include inquiries on statistical techniques aimed at reducing sample size, comparisons between graded and ungraded LLMs, the use of software libraries for evaluation purposes, and specific methodologies for assessing LLMs designed for executing tasks within the external environment, among other subjects.\nIn the industry, I find myself missing the culture of 'Summer Schools,' which are popular in academia. For instance, I particularly appreciated the European Summer School on Logic, Language, and Computation, among others. The comprehensive evaluation of LLMs is a substantial field that undoubtedly merits a detailed exploration. Ideally, this would encompass a series of webinars totaling at least five hours or, more effectively, a combination of five hours of theoretical instruction coupled with five hours of practical exercises. Such a program would provide participants with actionable knowledge that could be directly applied to the development of their own LLMs and LLM applications.",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 166,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Following the webinar on the evaluation of Large Language Models (LLMs), I have received numerous inquiries on various aspects of this topic. These include inquiries on statistical techniques aimed at reducing sample size, comparisons between graded and ungraded LLMs, the use of software libraries for evaluation purposes, and specific methodologies for assessing LLMs designed for executing tasks within the external environment, among other subjects.\nIn the industry, I find myself missing the culture of 'Summer Schools,' which are popular in academia. For instance, I particularly appreciated the European Summer School on Logic, Language, and Computation, among others. The comprehensive evaluation of LLMs is a substantial field that undoubtedly merits a detailed exploration. Ideally, this would encompass a series of webinars totaling at least five hours or, more effectively, a combination of five hours of theoretical instruction coupled with five hours of practical exercises. Such a program would provide participants with actionable knowledge that could be directly applied to the development of their own LLMs and LLM applications.",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 167,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 168,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 169,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 170,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 171,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 172,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 173,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "(for the today's webinar )",
        "time": "4d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 174,
        "person_name": "https://www.linkedin.com/in/kostya-shulgin",
        "text_description": "Two years ago, the Search Platform team at DoorDash started a journey to replace Elastcsearch with our in-house build search engine. Today, we are powering all search and recommendation surfaces at DoorDash. Anish Walawalkar,and I have penned an article about it.\n\nTo read more about it:",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 175,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 176,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 177,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 178,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "To elucidate further, my perspective on the critical importance of evaluation stems from observing the historical competition among giants like Google, Microsoft Search, and Yahoo. During this era, each entity claimed superiority based on their proprietary evaluation metrics. Despite these claims, a stark difference in the quality of search results emerged, leading to a clear victor both in the United States and globally, attributed to unparalleled search quality. This victory underscores the role of robust evaluation systems.\nOrganizations employed talented scientists and engineers to design systems optimized against these evaluation metrics, creating what can be termed as 'evaluation-driven systems.' The essence of these systems lies in their iterative improvement process, where each update undergoes rigorous evaluation, and only enhancements that positively impact these metrics are implemented. Consequently, the long-term success of these systems is heavily contingent upon the effectiveness of their evaluation methodologies.\nGoogle's triumph can be partly attributed to its evaluation framework, which more accurately mirrored the quality of search results from a human perspective compared to its rivals. This was true for both offline and online evaluations. Addressing the challenge of devising a fair and effective evaluation, especially offline, remains a formidable task. Notably, traditional metrics like NDCG from the 1990s fall short in accurately assessing search engine performance today.\nThe narrative extends into the realm of Large Language Models (LLMs), which are predominantly consumer-facing and aim to tackle complex queries through interactive engagement. The evaluation of such systems is equally challenging yet pivotal. It shapes the development trajectory of these models and ultimately determines their success. Therefore, the task of evaluating LLMs holds paramount importance, demanding attention from both industry professionals and academic researchers. This conviction stems from the understanding that rigorous evaluation not only drives technological advancement but also ensures the alignment of these advancements with human needs and expectations.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 179,
        "person_name": "https://www.linkedin.com/company/it-university-of-copenhagen/",
        "text_description": "Efficient queries = faster, cheaper, and more energy friendly systems! 💻☘️\n\nIn a new research project, Associate Professor at ITU,, will investigate how ranking algorithms influence query efficiency, and develop new models for query optimisation.\n\n\"Ultimately, our vision is for the project to pioneer a new research paradigm,\" says Zoi Kaoudi.\n\nLearn more about the project in this article. 👇\n\nThe project is funded by. 🙏",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 180,
        "person_name": "https://www.linkedin.com/in/zhanyong-wan",
        "text_description": "This concludes my series on \nhashtag\n#Google's \nhashtag\n#culturechange. The main purpose of me writing this series is to document history from my perspective - I believe that history is the sum of all who choose to write it.\n\nI know many great people working at Google, and I wish them best of luck in restoring Google's culture and glory. While each of us can draw our own conclusion, I wish that by reflecting on what happened in the past, we can all become a little bit wiser.",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 181,
        "person_name": "https://www.linkedin.com/in/jainankit",
        "text_description": "🌟 On this Rare Disease Day, I'm inspired to share a story that's very close to my heart. It's about resilience, hope, and the incredible journey of my daughter, a rare disease warrior. 🌟\n\nJust a few years ago, our lives were turned upside down when my Reyna was diagnosed with congenital hyperinsulinism. Her blood sugars dropped critically low, three doctors didn't quite know what was going on and we felt hopeless. Once we started understanding her condition (thank you,and so many others for helping us open doors, get answer), we took her to CHOP, enrolled her in a clinical trial, performed a 21% pancreatectomy which finally led to her being cured of HI.\n\nThrough all of this, Reyna showed an unwavering spirit that taught us the true meaning of strength. I still remember seeing her the day after her surgery as she woke up and wondering how she was smiling at the sight of us.\n\nHer journey wasn't easy, but it has been profoundly inspiring. After an uneventful few years, we may have another rare healthcare journey upon us but we are entering it with the hope that we can go through it with the same strength with such we went through the first one.\n\nToday, I want to honor my daughter's resilience and shine a light on the rare disease community. These warriors face unimaginable challenges every day, yet they confront them with courage that is nothing short of awe-inspiring.\n\nLet's use Rare Disease Day to raise awareness, foster understanding, and extend our support to those living with rare diseases and their families. Every story of struggle, every step towards progress, and every victory, no matter how small, deserves to be celebrated.\n\nTo everyone in the rare disease community, you are not alone. Your strength inspires the world, and together, we can bring about change. Here's to hope, healing, and the incredible journey ahead. 💖",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 182,
        "person_name": "https://www.linkedin.com/in/zhanyong-wan",
        "text_description": "This concludes my series on \nhashtag\n#Google's \nhashtag\n#culturechange. The main purpose of me writing this series is to document history from my perspective - I believe that history is the sum of all who choose to write it.\n\nI know many great people working at Google, and I wish them best of luck in restoring Google's culture and glory. While each of us can draw our own conclusion, I wish that by reflecting on what happened in the past, we can all become a little bit wiser.",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 183,
        "person_name": "https://www.linkedin.com/in/naga-gurumoorthy-a0b21a2",
        "text_description": "I’m happy to share that I’m starting a new position as Corporate Vice President at Microsoft!",
        "time": "13h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 184,
        "person_name": "https://www.linkedin.com/company/bytebytego/",
        "text_description": "Top 5 Kafka use cases\n\nKafka was originally built for massive log processing. It retains messages until expiration and lets consumers pull messages at their own pace.\n\nLet’s review the popular Kafka use cases.\n\n- Log processing and analysis\n- Data streaming in recommendations\n- System monitoring and alerting\n- CDC (Change data capture)\n- System migration\n\nOver to you: Do you have any other Kafka use cases to share?\n\n--\nSubscribe to our weekly newsletter to get a Free System Design PDF (158 pages):\n\n\n.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 185,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "I gave a talk and held a Q&A session at the MBZ University of AI in Abu Dhabi a couple of week ago.\nI was kindly hosted by MBZUAI President Eric Xing.\nVideo:\nSlides:",
        "time": "19h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 186,
        "person_name": "https://www.linkedin.com/in/gorelkin",
        "text_description": "Existing methods for controlling language models, such as RLHF and Constitutional AI, involve determining which LLM behaviors are desirable and training them into a language model. However, in many cases, 𝐢𝐭 𝐢𝐬 𝐝𝐞𝐬𝐢𝐫𝐚𝐛𝐥𝐞 𝐟𝐨𝐫 𝐋𝐋𝐌𝐬 𝐭𝐨 𝐛𝐞 𝐜𝐨𝐧𝐭𝐫𝐨𝐥𝐥𝐚𝐛𝐥𝐞 𝐚𝐭 𝐢𝐧𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐭𝐢𝐦𝐞, so that they can be used in multiple contexts with diverse needs. 𝐓𝐡𝐞 𝐚𝐮𝐭𝐡𝐨𝐫𝐬 𝐚𝐩𝐩𝐥𝐲 𝐚 𝐧𝐨𝐯𝐞𝐥 𝐬𝐢𝐦𝐩𝐥𝐢𝐟𝐢𝐜𝐚𝐭𝐢𝐨𝐧 𝐨𝐟 𝐂𝐨𝐧𝐬𝐭𝐢𝐭𝐮𝐭𝐢𝐨𝐧𝐚𝐥 𝐀𝐈, 𝐃𝐢𝐫𝐞𝐜𝐭 𝐏𝐫𝐢𝐧𝐜𝐢𝐩𝐥𝐞 𝐅𝐞𝐞𝐝𝐛𝐚𝐜𝐤, 𝐰𝐡𝐢𝐜𝐡 𝐬𝐤𝐢𝐩𝐬 𝐭𝐡𝐞 𝐫𝐚𝐧𝐤𝐢𝐧𝐠 𝐨𝐟 𝐫𝐞𝐬𝐩𝐨𝐧𝐬𝐞𝐬 𝐚𝐧𝐝 𝐮𝐬𝐞𝐬 𝐃𝐏𝐎 𝐝𝐢𝐫𝐞𝐜𝐭𝐥𝐲 𝐨𝐧 𝐜𝐫𝐢𝐭𝐢𝐪𝐮𝐞𝐬 𝐚𝐧𝐝 𝐫𝐞𝐯𝐢𝐬𝐢𝐨𝐧𝐬.\n\nSOURCE:",
        "time": "20h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 187,
        "person_name": "https://www.linkedin.com/in/greg-coquillo",
        "text_description": "Our goal is to elevate Seattle’s AI Community on the entrepreneurial map. That’s why we enjoy bringing tech leaders together to forge new friendships as they embark on the journey of disruptive technologies that improve people’s lives.\n\nWhere large investment funds may not be able help researchers, founders and practitioners push forward, a strong network can often pick up the pieces.\n\nAlways happy to partner withto bring passionate AI folks together to find career mentors, co-founders, investors or simply make new friends.\n\nLast’s night’s food-provided gathering wouldn’t have been possible without’sof course!\n\nThanks to all who came out. See you in March.\n\n\n\n\n\nClick\nand follow for more content\n\nSet the 🔔 notification on my page, don’t miss a post!",
        "time": "21h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 188,
        "person_name": "https://www.linkedin.com/in/jason-roselander-267a86",
        "text_description": "In the nearly two decades I spent at Amazon and Google, I never got an email from the CEO that was actually related to detailed, technical matters. (I did play dodgeball with Bezos tho.) They were both great companies, and I think that's pretty much expected.\n\nDay 4 at NVIDIA, Jensen asked some pretty specific questions about someone's status report.\n\nThis should be exciting!",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 189,
        "person_name": "https://www.linkedin.com/in/adam-levine-464614133",
        "text_description": "I’m happy to share that I’m starting a new position as Sr. Mid Market Account Executive at GitHub!",
        "time": "23h ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 190,
        "person_name": "https://www.linkedin.com/in/anthony-alcaraz-b80763155",
        "text_description": "“Weaving Neural Semantics” — LLMs to Strengthen Symbolic Knowledge Fabrics 🧠 \n\n\nThe exponential growth of digital information has created enormous challenges in organizing, distilling, and extracting value from available data.\n\nKnowledge graphs — structured representations of real-world entities and semantic relationships between them — offer tantalizing potential to meet these data management challenges.\n\nThey allow querying interconnected conceptual knowledge, discovering hidden insights, and enabling a new generation of intelligent applications.\nHowever, constructing enterprise-scale high-quality knowledge graphs through manual means is infeasible. Subject matter experts must meticulously identify key entities and define ontologies specifying permissible relationships.\n\nLabeling training data for machine learning systems requires substantial human effort.\n\nThese manual bottlenecks create a massive impediment to delivering impactful graph-powered solutions.\n\nRecent breakthroughs in foundation models for natural language processing provide timely hope.\n\nTheir ability to understand nuanced language, reason about semantic connections, and generate synthesized content unlocks automation opportunities for knowledge graph construction that ease previous constraints.\n\nSpecific stubborn challenges in data management and integration that foundation models can substantially alleviate include:\n\nEntity resolution across datasets, critical for data integration, remains manual-intensive work. Lack of real-world grounding limits statistical approaches. Foundation models have strong potential to match entities based on semantic meaning.\n\nData discovery across large enterprise corpora requires understanding user information needs and connecting queries to relevant subsets. Language models show promise for natural language interfaces to aid search.\n\nCleaning heterogeneous data requires complex wrangling logic customized to unique quirks of each dataset. Foundation models can synthesize data transformation scripts from input-output examples.\n\nSchema mapping is an intricate process to align elements, attributes, and structures across datasets. Improved semantic parsing capabilities allow models to automate finding correspondences.\n\nQuery synthesis remains difficult for non-technical users without SQL expertise. Foundation models enable accurately translating natural language questions into executable query text.\n\nFoundation model progress displays encouraging evidence that language-centric techniques can substantially resolve these stubborn challenges which have slowed data unification, analysis, and knowledge discovery.\n\nThis article explores leading techniques leveraging their breakthrough capabilities for automating the construction of structured knowledge graphs.\n\nFind the code mentioned in a notebook.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 191,
        "person_name": "https://www.linkedin.com/in/rmadanes",
        "text_description": "Looking forward to co-chairing an \nhashtag\n#AI conference in NYC this Monday March 4, where we are bringing together a number of startup executives, CxOs, and EY leaders in our \nhashtag\n#EYGenAIStrategyWorkshop. Should be a great discussion. I’m co-chairing this event withand, leaders in our GenAI consulting and FS Data and AI consulting practices.\n\nWe will be joined by a great set of executives on our panels including:\n*from LangChain\n*from MosaicML/Databricks\n*from LSEG\n*from Cerebras\n*from Cognaize\n*from\nDataminr\n*from Rasa\n*from\n\nAnd a number of other executives from AI companies, financial services, and industry. It’s great to delve into the tech and discuss the maturity of the AI stack, the economics of deploying to production, and a number of other key topics relevant to executives leaning into AI.\n\nAnd we have a great group of EY leaders to speak and moderate panels including:\n*, global vice-chair transformation\n*, global AI consulting leader\n*, AI and automation partner\n*, AI and quantitative modeling scientist\n* And also my co-chairsand, who lead our GenAI practice and our FS/Data and AI practices\n\nLet me know of your interest in attending our next GenAI executive workshop!\n\nThank you goes tofor helping us put together the event! Andtoo, for assembling the workshop.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 192,
        "person_name": "https://www.linkedin.com/in/jakemannix",
        "text_description": "People have been saying \"OMG, if you go to binary (or ternary) LLMs, GPUs won't be necessary, because matrix multiplication -> addition\".\n\n(c.f.)\n\nbut... multiplying a vector by a binary matrix is still highly parallelizable, right?\n\nThe operation of floating point matrix multiplication breaks down to gajillions of dot-products done in parallel. Dot product with floating point ops is a classic map+reduce, where \"map\" is \"*\", and \"reduce\" is \"+\". Moving to binary (ternary) values on one side of the dot product just means \"*\" becomes identity or set-to-zero (or flip-sign). Reduce is still \"+\".\n\nSomeone who knows GPU hardware better than me: why are GPUs any worse at doing this than at doing map=\"*\", reduce=\"+\"? I get that it'll be generally _easier_ for any hardware to do it (flops are expensive), but why would GPUs not still be the best workhorse to do massively parallel addition?",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 193,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”Macron’s words represented deterrence in action and were in line with broader French thinking on security. Ambiguity is a key part of effective deterrence. Other countries might have done well to mirror the French approach, rather than rushing to dismiss the option.”",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 194,
        "person_name": "https://www.linkedin.com/in/alexxubyte",
        "text_description": "What does ACID mean?\n.\n.\nThe diagram below explains what ACID means in the context of a database transaction.\n\n🔹 Atomicity\nThe writes in a transaction are executed all at once and cannot be broken into smaller parts. If there are faults when executing the transaction, the writes in the transaction are rolled back.\n\nSo atomicity means “all or nothing”.\n\n🔹 Consistency\nUnlike “consistency” in CAP theorem, which means every read receives the most recent write or an error, here consistency means preserving database invariants. Any data written by a transaction must be valid according to all defined rules and maintain the database in a good state.\n\n🔹 Isolation\nWhen there are concurrent writes from two different transactions, the two transactions are isolated from each other. The most strict isolation is “serializability”, where each transaction acts like it is the only transaction running in the database. However, this is hard to implement in reality, so we often adopt loser isolation level.\n\n🔹 Durability\nData is persisted after a transaction is committed even in a system failure. In a distributed system, this means the data is replicated to some other nodes.\n\n–\nSubscribe to our weekly newsletter to get a Free System Design PDF (158 pages):\n\n\n.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 195,
        "person_name": "https://www.linkedin.com/in/alykhan24",
        "text_description": "Exciting news! Cohere is expanding with a new office right in the heart of Manhattan. This is your chance to join a dynamic team at the forefront of artificial intelligence innovation, and to make a real impact in the industry, all from one of the most vibrant cities in the world! Check out open positions at.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 196,
        "person_name": "https://www.linkedin.com/in/pankaj-rajan",
        "text_description": "We will be doing the inaugural launch of our popular \"Orange Wip\" in Bangalore next month. If you are an aspiring entrepreneur or curious about early-stage startup building, do come to learn more about super{set} and othercompanies,,, and more. Meet and talk to,,,,,, and our founding engineers to understand what it takes to build a company!",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 197,
        "person_name": "https://www.linkedin.com/in/adam-cheer-06854022",
        "text_description": "2 great quotes on why purpose-built vector databases matter and why Pinecone is well suited to handle your Enterprise, production workloads:\n\n\"Just as a librarian can quickly retrieve a book based on its category, a vector database can swiftly fetch the relevant vectors. Nijmeijer says that this is essential for LLMs as they constantly process vast amounts of textual data, converting them into vectors for various tasks like understanding context, generating responses, or finding patterns. Without a vector database, managing and retrieving these vectors would be as cumbersome as finding a specific book in the world’s largest library, drastically slowing down the AI's performance.\"\n\n\"It’s also worth considering the fact that every company that is a database company may now be quite happy to style itself as a vector database company, so look at where vendors have come from and at what point they have evolved.\"",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 198,
        "person_name": "https://www.linkedin.com/in/clementdelangue",
        "text_description": "We just crossed 100,000 organizations on HF!\n\nSome of my favorites:\n- The MLX community for on-device AI:\n- Theorg with over 150+ datasets:\n- Theorg to show big financial institutions can use the hub:\n- TheNLP org:\n- Social post explorers for posters on HF:\n- Theorg with 600+ models including Gemma, T5, BERT,...:\n- Thesociety and ethics team org:\n- Theorg for AYA:\n- Theorg as one of the first enterprise hub organization\n- Theorg with 200+ models for\n\nThis is a tiny sample. What are you favorites?",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 199,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "An intriguing piece of research from the teams at Meta highlights the development of sub-billion parameter models. These models, possessing only a few hundred million parameters, demonstrate high accuracy across various NLP tasks. While the focus of the article is on the applicability for mobile devices, the significance of subbillion P, high-performing models cannot be overstated, especially in scenarios where low latency and high throughput are crucial for real-time online serving, streaming large volumes of documents, and other similar use cases.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 200,
        "person_name": "https://www.linkedin.com/in/eli-brosh-058989",
        "text_description": "Excited to introduce our latest research, a novel method for automatic prompt optimization using synthetic data (https://lnkd.in/dPgATixD). \n\nI often wished for a tool like this while building production AI systems 🙂\n\nOur method leverages Large Language Models (LLMs) to 𝐢𝐧𝐭𝐞𝐫𝐚𝐜𝐭𝐢𝐯𝐞𝐥𝐲 𝐞𝐧𝐡𝐚𝐧𝐜𝐞 and perfect prompts according to user intent. It’s designed for real-world use cases, and addresses the challenges of extensive manual prompt engineering, prompt sensitivity, and visibility into performance.\n\nWe're also proud to release Auto Prompt (), an open-source system that offers several powerful features beyond prompt engineering:\n\n💵 𝐏𝐫𝐨𝐦𝐩𝐭 𝐝𝐢𝐬𝐭𝐢𝐥𝐥𝐚𝐭𝐢𝐨𝐧. Transfer prompts between black-box models effortlessly.\n🏋️ 𝐏𝐫𝐨𝐦𝐩𝐭 𝐬𝐪𝐮𝐚𝐬𝐡𝐢𝐧𝐠. Combine multiple prompts into one for cost efficiency.\n🧩 𝐒𝐲𝐧𝐭𝐡𝐞𝐭𝐢𝐜 𝐝𝐚𝐭𝐚 𝐠𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧. Create small, yet effective synthetic benchmarks for rigorous evaluation.\n🚀 𝐌𝐨𝐝𝐮𝐥𝐚𝐫𝐢𝐭𝐲. Easily integrates with popular tools like,, and\n\nCheck out the research paper and try the system on GitHub! We're excited to hear your thoughts and discuss the future of prompt optimization together!",
        "time": "3w ",
        "url_links": [
            "https://lnkd.in/dPgATixD)."
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 201,
        "person_name": "https://www.linkedin.com/in/kennethzhu88",
        "text_description": "I’m happy to share that I’m starting a new position as Executive Director, Data Science Lead! Appreciate the guidance and support from all my seniors, mentors and colleagues!",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 202,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "More multi-lingual models\n15B model from NVidia Nemotron 4 (8B models from Nemotron have been available for some time, hope to see Nemotron4 available too), \nA very strong performance among the models of similar size\nI see multi-lingual development as very important for the industry because it will enable new products in many companies worldwide - traditionally it was hard to develop NLP intensive applications in non-EN",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 203,
        "person_name": "https://www.linkedin.com/in/anthony-alcaraz-b80763155",
        "text_description": "How Large Language Models Can Thrive Within An AI Ecosystem ☘ \n\n\nThe rise of large language models (LLMs) has enthralled technologists and businesses alike. Their ability to generate remarkably human-like text and engage in dialogue by simply digesting vast troves of data seems to promise an AI-powered future just around the corner.\n\nHowever, behind the hype lies a more nuanced reality about the strengths and limitations of LLMs — and the need for a collaborative AI approach centered around them.\n\nWhile LLMs demonstrate impressive language mastery, understanding and responding to multifaceted human communication requires grounding it within wider real-world contexts.\n\nOn their own, LLMs lack the complete picture of how concepts relate or map across textual descriptions. Their knowledge remains confined within sequences of words and predictive patterns deduced from surface-level observations of texts rather than a structured understanding of underlying ideas and abstract relationships.\n\nThis is where LLMs need to be viewed as a crucial yet imperfect building block that performs exceptionally well on specific linguistic tasks but needs tight integration with complementary AI techniques to truly achieve its potential.\n\nUsed judiciously in conjunction with other specialized approaches, LLMs can play a starring role within an ensemble that collectively inches closer towards the lofty goal of replicating generalized human intelligence within machines.\nThe challenges of developing real-world AI solutions highlight the need for orchestrating diverse data types, technologies and skill sets into a cohesive symphony.\n\nStructured knowledge representation techniques for systematically organizing and reasoning about concepts and entities can provide LLMs the scaffolding needed to ground language in context.\n\nLightweight machine learning models tailored for niche tasks through supervised learning can efficiently handle the sub-processes that don’t require costly LLMs.\n\nBlending LLMs within this collage of AI fabric tailored to the problem benefits both — smoothing out the rough edges for LLMs to shine while allowing other complementary approaches to optimize the solution.\n\nRather than a one-stop solution for every AI need, LLMs are better positioned as a versatile component within a varied toolkit. Making them part of an integrated ensemble continues progress towards replicating the complex, multi-faceted cognition underlying the human mind.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 204,
        "person_name": "https://www.linkedin.com/in/parulpandeyindia",
        "text_description": "We at H2O.ai are proud to announce \nhashtag\n#H2ODanube1.8B, a cutting-edge open-source natural language model. This release marks a significant milestone in making sophisticated language models accessible on mobile devices\nHere’s what makes Danube stand out:\n\n• Accessibility: Danube enables mobile developers to integrate advanced language models into apps and devices, functioning smoothly offline.\n\n• Open Source Commitment: We’re proud to offer Danube-1.8B under an open-source Apache 2.0 license, reflecting our belief in open-source innovation.\n\n• Advanced AI for All: With its efficiency and cost-effectiveness, Danube democratizes cutting-edge AI, aligning with our vision of responsible AI development.\n\n• Versatility and Performance: Fine-tuned for various tasks using a trillion-token dataset, Danube boasts impressive benchmarks.\n\n• Ready for Developers: Available on\nwith forthcoming tools for easy adoption and fine-tuning.\n\n• Beyond Text: A chat-tuned version of Danube broadens its scope to conversational AI applications.\n\nI’m incredibly proud of my colleagues at H2O, who, under the leadership of our CEOare continually pushing boundaries. Together, we are making significant strides towards the GenAI revolution, with Danube marking the beginning of what promises to be a surge in offline AI applications.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 205,
        "person_name": "https://www.linkedin.com/in/parulpandeyindia",
        "text_description": "We at H2O.ai are proud to announce \nhashtag\n#H2ODanube1.8B, a cutting-edge open-source natural language model. This release marks a significant milestone in making sophisticated language models accessible on mobile devices\nHere’s what makes Danube stand out:\n\n• Accessibility: Danube enables mobile developers to integrate advanced language models into apps and devices, functioning smoothly offline.\n\n• Open Source Commitment: We’re proud to offer Danube-1.8B under an open-source Apache 2.0 license, reflecting our belief in open-source innovation.\n\n• Advanced AI for All: With its efficiency and cost-effectiveness, Danube democratizes cutting-edge AI, aligning with our vision of responsible AI development.\n\n• Versatility and Performance: Fine-tuned for various tasks using a trillion-token dataset, Danube boasts impressive benchmarks.\n\n• Ready for Developers: Available on\nwith forthcoming tools for easy adoption and fine-tuning.\n\n• Beyond Text: A chat-tuned version of Danube broadens its scope to conversational AI applications.\n\nI’m incredibly proud of my colleagues at H2O, who, under the leadership of our CEOare continually pushing boundaries. Together, we are making significant strides towards the GenAI revolution, with Danube marking the beginning of what promises to be a surge in offline AI applications.",
        "time": "1d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 206,
        "person_name": "https://www.linkedin.com/in/sarahxguo",
        "text_description": "Congratulations to Sridhar Ramaswamy and outgoing CEO Frank Slootman after an amazing run.\n\nThere is no better person to run $SNOW now -- Sridhar is a technical pioneer, deep understanding of data infrastructure/AI, and proven ambitious, disciplined leadership in $100B+ revenue business.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 207,
        "person_name": "https://www.linkedin.com/in/kramasamy",
        "text_description": "We made 3-4x improvements in the performance of stateful pipelines in Apache Spark Structured Streaming by identifying how the choice of shuffle partitions impacts performance. For more details, read the blog by Anish Shrigondekar-",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 208,
        "person_name": "https://www.linkedin.com/in/purvagpatel",
        "text_description": "🚀✨ My first venture into AI art with LoRA, DreamBooth, and Stable Fusion, customizing my AI models: https://lnkd.in/gstigGgK 🎨💫\n\nAstonished by the quality and details in these images. The biggest takeaway is to dream big.",
        "time": "2d ",
        "url_links": [
            "https://lnkd.in/gstigGgK"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 209,
        "person_name": "https://www.linkedin.com/in/annaberenberg",
        "text_description": "Everyone is talking about LLM models. But it is a great time to be an infrastructure engineer - just look at the complexity of the LLM serving, training and eval stacks and these are simplified stacks. There is so much to build !",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 210,
        "person_name": "https://www.linkedin.com/in/sridhar-ramaswamy",
        "text_description": "I’m honored and beyond excited as we start a new phase of the Snowflake journey together. Snowflake is a once-in-a-generation company and a truly special place. \n\nI love our customer-first obsession to deliver a tightly integrated and efficient platform. I am excited by the expansiveness of our vision around a true Data Cloud: where data and applications come together for our customers and our developer partners.\n\nAnd I can’t wait to show customers the massive business value we can deliver together with AI!",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 211,
        "person_name": "https://www.linkedin.com/in/richardsocher",
        "text_description": "Woah.. We beat ChatGPT 4 and pplx in an independently run user study of 120 queries. \nMain user question: Which output do you prefer?\n\nOnce you know which mode to useis the best AI assistant for chat and search.\n\nMore details and examples soon!\n\nNote that it looks like pplx does really poorly here but it's not that bad - just rarely as good as the best answer from us or ChatGPT Plus.\n\nI actually assumed we would not do as well and would have to improve our models more after the first study since we don't have as many users as ChatGPT..\nWith retention >90%, it is now almost exclusively a marketing and awareness question.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 212,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "it's quite exciting. The Information reports that OpenAI is shifting its AI focus to software that operates tasks (the article is from Feb 7) - the Information is a very reliable source for such information. \nThere are various dimensions to consider when defining 'agents' and the range of actions they are capable of performing. It is becoming increasingly apparent that equipping Large Language Models (LLMs) with the ability to learn actions beyond mere language tasks represents a significant advancement. For instance, the development of ToolFormer by MetaAI, followed by subsequent research in LLMs that involve 'tools' (such as APIs and functions), illustrates this shift. Historically, most systems have been designed with a focus on information retrieval rather than action-oriented tasks (or other text -> text tasks). This is largely due to the complexities involved in developing user-friendly interfaces that facilitate control over actions. Consider mapping applications, which currently specialize in search and recommendation functions. However, real-world tasks often require both discovery and search capabilities aimed at achieving broader objectives. For example, planning a trip from Vancouver, CA, to Portland might involve a detailed search for specific locations. Presently, this process is manual and cumbersome. A task-driven action interface that enables discovery, collection, planning, and organization would greatly enhance user convenience and directly address their needs. The challenge has been that designing intuitive controls for action-oriented tasks is difficult, which is why even when big tech companies introduce enhancements that add significant utility and value for some users, these features are not widely adopted due to their complexity.\nNow, with advancements in LLM technology, there is potential to develop easy and intuitive interfaces that can execute a series of complex logic actions based on simple human interactions. This opens up possibilities not only for creating new applications but also for significantly improving traditional search/discovery platforms used in web search, travel and vacation planning, e-commerce, and retail, among others. Potential approaches might include the ToolFormer method (learning API calls) or alternative strategies (such as learning to write Domain-Specific Language (DSL) specifications based on natural language interactions).\nIt is exciting to note OpenAI's focus on this strategy, given its potential to dramatically impact various industries through the creation of new applications and significant enhancements to user utility, value, and ease of use.",
        "time": "2d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 213,
        "person_name": "https://www.linkedin.com/in/ronnyk",
        "text_description": "Build vs. Buy for \nhashtag\n#ABTesting with vendors answering tough questions!\n\nFor my Accelerating Innovation with A/B Testing class (), we had a Build vs. Buy session. I asked three of the larger A/B testing vendors ABTasty, Optimizely, and Split to answer 6 questions on 10 slides and present in class, which they did.\nOther vendors were offered to submit 10 slides: Statsig, VWO, and Webtrends-Optimize did.\n\nQuestions and the vendor decks at\n\nUpdates\n2/9/2022 - Addedto deck\n7/24/2022 - Addedto deck\n9/1/2022:summarized the several build vs. buy decks at\n9/20/2022 - Addedto deck\n11/2/2022 - Added\n\n:\n:\n:\n:\n:\n[added]:\n:\n:",
        "time": "2yr ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 214,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 215,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 216,
        "person_name": "https://www.linkedin.com/in/jiangzh",
        "text_description": "Delighted to share our latest work, \"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs\", accepted at NSDI 2024. Preprint is avaliable here: https://lnkd.in/g-DqfcHv\n\nIn this paper, we present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. MegaScale achieves 55.2% MFU when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to prior methods, which reduces the training time for 300B data to 1.75 days. The paper outlines our efforts in addressing the complexities of scaling up training processes and contributes insights to the ongoing development of large-scale LLM systems. We invite you to read our work and explore how we're pushing the boundaries of AI training scalability!\n\nI'm incredibly grateful for the collaboration with a remarkable team: Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu.\n\nAdditionally, we're still hiring! If you're interested in joining us on this exciting journey and helping shape the future of AI, please reach out toor myself.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g-DqfcHv"
        ],
        "url_texts": "Help | Advanced Search\n\nquick links\nHelp | Advanced Search\nquick links\nLoginHelp PagesAbout\nLogin\nHelp Pages\nAbout\nComputer Science > Machine Learning\nTitle:MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs\nSubmission history\nAccess Paper:\nDownload a PDF of the paper titled MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs, by Ziheng Jiang and 31 other authorsDownload PDFHTML (experimental)TeX SourceOther Formats\nDownload PDF\nHTML (experimental)\nTeX Source\nOther Formats\nReferences & Citations\nNASA ADSGoogle ScholarSemantic Scholar\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nBibTeX formatted citation\nBookmark\nBibliographic and Citation Tools\nCode, Data and Media Associated with this Article\nDemos\nRecommenders and Search Tools\nAuthorVenueInstitutionTopic\nAuthor\nVenue\nInstitution\nTopic\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\nAboutHelp\nAbout\nHelp\ncontact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe\ncontact arXivClick here to contact arXiv\n Contact\nsubscribe to arXiv mailingsClick here to subscribe\n Subscribe\nCopyrightPrivacy Policy\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance arXiv Operational Status                     Get status notifications via                    email                    or slack\nWeb Accessibility Assistance\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n"
    },
    {
        "id": 217,
        "person_name": "https://www.linkedin.com/in/benjaminhan",
        "text_description": "KnowledgeableLMs \nhashtag\n#workshop @ \nhashtag\n#ACL2024 is open for submissions: deadline May 20.\n\n“We welcome long (8 page) and short (4 page) paper submissions on all topics related to knowledgable LMs, including:\n\nAnalysis of knowledge within LMs: how much they know and where that knowledge is from. Enhancing LMs with existing knowledge sources (knowledge graphs, domain-specific databases, manuals, and rules, etc, either during training or inference). Analyzing and improving RAG (retrieval-augmented generation) systems Updating and editing knowledge in LMs. Knowledge extraction and generation using LMs Evaluation of knowledge utilization (faithfulness, truthfulness) by LMs. Identification and mitigation of LM hallucinations, factual error correction.”",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 218,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 219,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 220,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Do open foundation models increase the marginal risk of intentional misuse with negative impact on society? (compared to closed models and traditional technologies)\nDo they provide increased benefits? [short answer: yes]\nHave previous studies shown an increase in marginal risk? [short answer: no]\n\nA study by 25 authors from various institutions in academia and industry (including Joelle Pineau from Meta).",
        "time": "3d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 221,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Webinar ; Evaluating LLMs for Production Systems : Methods and Practices \nVideo: https://lnkd.in/g5jZPAqh\nQuestions: https://lnkd.in/gmrMWdxy\nSlidedeck presentation:\n(I'll be answering after a break later today)\n\nIn my recent webinar, I delved into the critical realm of evaluating Large Language Models (LLMs) and their applications. The central thesis of my presentation was the pivotal role that rigorous evaluation plays in the success and advancement of any technological system. My aim was not only to underscore the significance of evaluation but also to provide a comprehensive guide on orchestrating effective assessment strategies for LLMs and their varied applications.\nThe discussion began with a foundational overview of what constitutes evaluation in the context of LLMs, moving towards a structured approach on organizing this process to maximize outcomes. I explored the multifaceted nature of evaluation, highlighting key criteria essential for a successful evaluation process. These criteria serve as benchmarks that guide the development and refinement of LLMs, ensuring they meet the desired standards of performance and utility.\nA significant portion of the webinar was dedicated to examining specific aspects of LLM evaluation. This included the assessment of embeddings, in-context learning (ICL), code-generating LLMs, and LLMs designed for action-taking across various domains. Each of these areas presents unique challenges and requirements for evaluation, necessitating tailored approaches to accurately gauge their effectiveness and efficiency.\nMoreover, I provided an overview of open-source tools available for LLM evaluation, offering participants valuable resources to facilitate their own assessment efforts. These tools play a crucial role in streamlining the evaluation process, enabling developers and researchers to conduct thorough and consistent analyses of LLM capabilities.\nIn addition to evaluating the functional aspects of LLMs, I also touched upon the operational properties critical for their deployment in production environments. This includes assessing software performance and ensuring that LLMs can operate seamlessly and reliably within real-world applications.\nOverall, the webinar aimed to equip attendees with a deep understanding of the importance of evaluation in the lifecycle of LLMs and the practical knowledge needed to implement effective evaluation processes. Through this comprehensive exploration, my goal was to contribute to the ongoing advancement and optimization of LLM technologies, paving the way for more robust, efficient, and impactful applications in the future.",
        "time": "3d ",
        "url_links": [
            "https://lnkd.in/g5jZPAqh",
            "https://lnkd.in/gmrMWdxy"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n\n\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 222,
        "person_name": "https://www.linkedin.com/in/armand-ruiz",
        "text_description": "At IBM, we curated 6.48TB of data to train our LLM Granite.13B. This was reduced to 2.07 TB after pre-processing, a 68% decrease. This step was crucial to ensure that we provide a high-quality, unbiased, ethical, and legal dataset for training our models for enterprise use cases. This is how:\n\nData Sources used for training:\n\n1) 𝗮𝗿𝗫𝗶𝘃: Over 1.8 million scientific paper pre-prints posted to arXiv.\n2) 𝗖𝗼𝗺𝗺𝗼𝗻 𝗖𝗿𝗮𝘄𝗹: Open repository of web crawl data.\n3) 𝗗𝗲𝗲𝗽𝗠𝗶𝗻𝗱 𝗠𝗮𝘁𝗵𝗲𝗺𝗮𝘁𝗶𝗰𝘀: Mathematical question and answer pairs data.\n4) 𝗙𝗿𝗲𝗲 𝗟𝗮𝘄: Public-domain legal opinions from US federal and state courts.\n5) 𝗚𝗶𝘁𝗛𝘂𝗯 𝗖𝗹𝗲𝗮𝗻: Code data from CodeParrot covering a variety of coding languages.\n6) 𝗛𝗮𝗰𝗸𝗲𝗿 𝗡𝗲𝘄𝘀: News on computer science and entrepreneurship, taken between 2007-2018.\n7) 𝗢𝗽𝗲𝗻𝗪𝗲𝗯 𝗧𝗲𝘅𝘁: Open-source version of OpenAI’s Web Text corpus containing web pages through 2019.\n8) 𝗣𝗿𝗼𝗷𝗲𝗰𝘁 𝗚𝘂𝘁𝗲𝗻𝗯𝗲𝗿𝗴 (𝗣𝗚-𝟭𝟵): A repository of free e-books with focus on older works for which U.S. copyright has expired.\n9) 𝗣𝘂𝗯𝗺𝗲𝗱 𝗖𝗲𝗻𝘁𝗿𝗮𝗹: Biomedical and life sciences papers.\n10) 𝗦𝗘𝗖 𝗙𝗶𝗹𝗶𝗻𝗴𝘀: 10-K/Q filings from the US Securities and Exchange Commission (SEC) for the years 1934-2022.\n11) 𝗦𝘁𝗮𝗰𝗸 𝗘𝘅𝗰𝗵𝗮𝗻𝗴𝗲: Anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers.\n12) 𝗨𝗦𝗣𝗧𝗢: US patents granted from 1975 to May 2023, excluding design patents.\n13) 𝗪𝗲𝗯𝗵𝗼𝘀𝗲: Unstructured web content converted into machine-readable data feeds acquired by IBM.\n14) 𝗪𝗶𝗸𝗶𝗺𝗲𝗱𝗶𝗮: Eight English Wikimedia projects (enwiki, enwikibooks, enwikinews, enwikiquote, enwikisource, enwikiversity, enwikivoyage, enwiktionary). containing extracted plain text from pages and articles.\n\nOnce the data has been cleared and downloaded, it is prepared for model training through a series of steps collectively known as the pre-processing pipeline. These steps include the following:\n\n1) Text extraction\n2) De-duplication\n3) Language identification\n4) Sentence splitting\n5) Hate, abuse, and profanity annotation\n6) Document quality annotation\n7) URL block-listing annotation\n8) Filtering\n9) Tokenization\n\nSome pre-processing steps adhere to an annotation/filtering pattern, where documents or sentences are first annotated and then filtered during the filtering task based on defined thresholds.\n\nThis is how we build trustworthy LLMs for your Business. Kudos to the team from IBM Research that keeps innovating and building great models for our customers.\n\n____\n\nIf you like this content, please repost it ♻️ and follow me,, for more similar posts.",
        "time": "4d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 223,
        "person_name": "https://www.linkedin.com/in/armand-ruiz",
        "text_description": "At IBM, we curated 6.48TB of data to train our LLM Granite.13B. This was reduced to 2.07 TB after pre-processing, a 68% decrease. This step was crucial to ensure that we provide a high-quality, unbiased, ethical, and legal dataset for training our models for enterprise use cases. This is how:\n\nData Sources used for training:\n\n1) 𝗮𝗿𝗫𝗶𝘃: Over 1.8 million scientific paper pre-prints posted to arXiv.\n2) 𝗖𝗼𝗺𝗺𝗼𝗻 𝗖𝗿𝗮𝘄𝗹: Open repository of web crawl data.\n3) 𝗗𝗲𝗲𝗽𝗠𝗶𝗻𝗱 𝗠𝗮𝘁𝗵𝗲𝗺𝗮𝘁𝗶𝗰𝘀: Mathematical question and answer pairs data.\n4) 𝗙𝗿𝗲𝗲 𝗟𝗮𝘄: Public-domain legal opinions from US federal and state courts.\n5) 𝗚𝗶𝘁𝗛𝘂𝗯 𝗖𝗹𝗲𝗮𝗻: Code data from CodeParrot covering a variety of coding languages.\n6) 𝗛𝗮𝗰𝗸𝗲𝗿 𝗡𝗲𝘄𝘀: News on computer science and entrepreneurship, taken between 2007-2018.\n7) 𝗢𝗽𝗲𝗻𝗪𝗲𝗯 𝗧𝗲𝘅𝘁: Open-source version of OpenAI’s Web Text corpus containing web pages through 2019.\n8) 𝗣𝗿𝗼𝗷𝗲𝗰𝘁 𝗚𝘂𝘁𝗲𝗻𝗯𝗲𝗿𝗴 (𝗣𝗚-𝟭𝟵): A repository of free e-books with focus on older works for which U.S. copyright has expired.\n9) 𝗣𝘂𝗯𝗺𝗲𝗱 𝗖𝗲𝗻𝘁𝗿𝗮𝗹: Biomedical and life sciences papers.\n10) 𝗦𝗘𝗖 𝗙𝗶𝗹𝗶𝗻𝗴𝘀: 10-K/Q filings from the US Securities and Exchange Commission (SEC) for the years 1934-2022.\n11) 𝗦𝘁𝗮𝗰𝗸 𝗘𝘅𝗰𝗵𝗮𝗻𝗴𝗲: Anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers.\n12) 𝗨𝗦𝗣𝗧𝗢: US patents granted from 1975 to May 2023, excluding design patents.\n13) 𝗪𝗲𝗯𝗵𝗼𝘀𝗲: Unstructured web content converted into machine-readable data feeds acquired by IBM.\n14) 𝗪𝗶𝗸𝗶𝗺𝗲𝗱𝗶𝗮: Eight English Wikimedia projects (enwiki, enwikibooks, enwikinews, enwikiquote, enwikisource, enwikiversity, enwikivoyage, enwiktionary). containing extracted plain text from pages and articles.\n\nOnce the data has been cleared and downloaded, it is prepared for model training through a series of steps collectively known as the pre-processing pipeline. These steps include the following:\n\n1) Text extraction\n2) De-duplication\n3) Language identification\n4) Sentence splitting\n5) Hate, abuse, and profanity annotation\n6) Document quality annotation\n7) URL block-listing annotation\n8) Filtering\n9) Tokenization\n\nSome pre-processing steps adhere to an annotation/filtering pattern, where documents or sentences are first annotated and then filtered during the filtering task based on defined thresholds.\n\nThis is how we build trustworthy LLMs for your Business. Kudos to the team from IBM Research that keeps innovating and building great models for our customers.\n\n____\n\nIf you like this content, please repost it ♻️ and follow me,, for more similar posts.",
        "time": "4d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 224,
        "person_name": "https://www.linkedin.com/in/arvind-jain-5935161",
        "text_description": "Five years ago, we set out to build Glean with the simple goal of helping people find the answers they need to do their best work. Since then, we’ve expanded our work to help CIOs quickly and securely deploy generative AI in the enterprise, and today, we are so excited to share that we’ve raised over $200M at a $2.2B valuation led byand, to help us further achieve our vision for the future of work.\n\nGenerative AI took the world by storm last year, and I’ve spoken to countless CIOs who are eager and energized to bring this powerful technology into their organizations.\n\nBut every CIO also knows they must deploy generative AI in a way that’s safe and secure enough for the enterprise.\n\nGlean’s state-of-the-art search and RAG technology retrieves the most relevant information for LLMs to generate personalized answers grounded in each enterprise’s unique knowledge graph. All answers generated are secure, private, permissions-aware, and fully referenceable back to source documentation in the enterprise.\n\nThis uniquely positions Glean to deliver a solution that meets the challenging requirements of a truly enterprise-ready AI assistant, and it’s an honor to be chosen by some of the world’s leading enterprises to bring this transformational technology to their workforces.\n\nI can’t say enough how grateful I am for everyone who has supported us at— our customers, partners, investors, community, and team.\n\nBecause of your support, we’re able to work as hard as we do to achieve our ultimate goal: be the enterprise AI platform with a ready-to-use AI work assistant, as well as the tools to build custom generative AI experiences grounded in company knowledge.\n\nWe’ve got a lot further to explore – this latest round of funding will enable us to further propel our work in developing the most secure, comprehensive, and intuitive AI assistant of the future.\n\nThank you to all of our supporters:at,at,at,at, Capital One Ventures,at Citi Ventures,andat,andatVentures,at,andat,at,at,at.\n\nThe best is yet to come. Read more about this exciting milestone:",
        "time": "4d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 225,
        "person_name": "https://www.linkedin.com/company/rensselaer-polytechnic-institute/",
        "text_description": "Professor Deborah McGuinness has been elected a fellow of the ACM, Association for Computing Machinery, for her “contributions to knowledge technologies including ontologies and knowledge graphs.” She is one of 68 fellows selected by her peers to receive the honor for transformative contributions to computing science and technology.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 226,
        "person_name": "https://www.linkedin.com/in/khaled-elghamry-phd-49821a33",
        "text_description": "In the 37th Annual Symposium on Arabic Linguistics, Long Island University, NYC, presenting:\n\"AROSA: A Web-Mined Arabic Corpus for Opinion Mining and Sentiment Analysis Using Subjective Predicates\"",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 227,
        "person_name": "https://www.linkedin.com/in/conorgrennan",
        "text_description": "Not saying that Jensen Huang stole my idea. But 4 days after I publish The Future Belongs to English Majors, his statement was *eerily* similar. \n\nBut he's right. The folks who will crush generative AI are the ones who understand that the value they bring is in their domain expertise, and augmenting that with help of generative AI.\n\nThe Future Belongs to English Majors:\n\nAI-Proof Your Career in Four Steps:\n\nWant to be at the cutting edge of generative AI? Just sign up for the free AI Mindset newsletter, and get our free Generative AI Quick Start Guide:",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 228,
        "person_name": "https://www.linkedin.com/in/bendee983",
        "text_description": "While other tech companies have been touting their genAI products through large PR efforts, Apple has been silently releasing papers, models, and programming libraries.\nA closer look at some of these releases shows how Apple is leveraging its full-stack control to establish its advantage in on-device genAI.\nWho knows, maybe this year will prove that you don't need to be a hyperscaler to be the king of genAI.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 229,
        "person_name": "https://www.linkedin.com/in/supreeth-n",
        "text_description": "I am excited to share that we have two papers accepted to \nhashtag\n#CVPR2024. \n\nIn our first work, we address the problem of implausible human hands in AI-generated images, such as those obtained from Stable Diffusion. We introduce a two-stage diffusion process that leverages topological and geometric human priors to generate realistic-looking hands.\n\nIn our second work, we address the task of identifying, segmenting, and tracking hand-held objects in videos. We propose a transformer-based method to tackle this problem.\n\nJoint work with our collaborators, Xiang Chen,,, Huy Anh Nguyen, Lihan Huang, and my advisor.\n\nMore details to follow!",
        "time": "4d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 230,
        "person_name": "https://www.linkedin.com/company/quanta-magazine/",
        "text_description": "This week in the Fundamentals Newsletter, our senior math writer Jordana Cepelewicz describes the mathematical utility of studying how shapes “tile” the plane.",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 231,
        "person_name": "https://www.linkedin.com/in/noah-a-preston",
        "text_description": "Wall Street’s biggest one-day gain on record\n\nNVIDIA drove up its market value by $277 billion. Now at $2 trillion in market value for the first time. The insatiable demand for its chips has made the Silicon Valley firm a pioneer of the GenAI boom.\n\nThey provide the technology to run AI at 40,000+ companies such as,,,,,,, and.\n\nNVIDIA DGX™ AI supercomputer allowedChatGPT to reach 100 million users in two months.\n\nNVIDIA Hopper™ GPU Transformer Engine is driving breakthroughs in LLMs that are fundamentally changing the world as we know it.\n\nNVIDIA DGX Cloud partners with,,, andCloud Infrastructure to create access to AI supercomputing via web browser.\n\nWinning awards like \"Best Place to Work in 2023\" andwinning \"World's Best Performing CEO\" suggests they have strong company culture.\n\nWill Nvidia continue its explosive growth through 2024 and beyond? Has hype led to their stock being overvalued?\n\nExcited to find out.",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 232,
        "person_name": "https://www.linkedin.com/in/rishiljacob",
        "text_description": "This was an amazing video indeed !",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 233,
        "person_name": "https://www.linkedin.com/in/maria-khalusova-a958aa14",
        "text_description": "I'm very excited about the launch of the Open Source AI Cookbook. This is a collection of notebooks for AI builders powered, as the name heavily implies, with Open Source - models, libraries, datasets, tools, etc.\n\nThe cookbook aims to reflect the incredible diversity of Open Source in AI. Unlike closed-source solutions/models, open-source thrives through collaboration, so if you’re building AI with open source libraries, models and tools, bring your “recipes” to the Cookbook -!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 234,
        "person_name": "https://www.linkedin.com/in/raphaelmansuy",
        "text_description": "Fine-Tuning Large AI Models Just Got More Efficient: Introducing LoRA+ ...\n\nFinetuning large pre-trained AI models like GPT-3 for specific tasks remains computationally expensive. But a new technique called LoRA+ from UC Berkeley promises more efficient and faster fine-tuning without compromising performance.\n\n👉 The Limitations of Current Approaches\n\nExisting methods like LoRA constrain model updates to low-rank matrices during fine-tuning. But as model width grows exponentially, these methods fail to efficiently learn new features. The root issue? Both adapter matrices in LoRA get updated using the same learning rate.\n\n👉 Introducing LoRA+: Tuning the Learning Rates\n\nThe Berkeley researchers resolved this by using different learning rates for the two adapter matrices, with one rate significantly higher. This allows efficient feature learning even with exponentially wide models.\n\nLoRA+ builds on top of LoRA with this simple but impactful innovation. And it delivers tangible improvements:\n\n- 1-2% better performance on various NLP tasks\n- 2X faster fine-tuning time for the same computational cost\n- Applicable across models like GPT-2, RoBERTa, LLama\n\n👉 Real-World Implications\n\nBy enhancing fine-tuning efficiency even for models with billions of parameters, LoRA+ can:\n\n- Democratize access to large language models for smaller teams\n- Accelerate experimentation cycles for innovating with AI\n- Make state-of-the-art AI capabilities more affordable\n\n👉 Try Out LoRA+ Today\n\nThe researchers have open-sourced LoRA+ to foster community collaboration. I invite AI practitioners to try it out on your projects and move one step closer to accessible and scalable AI.\n\nWhat fine-tuning bottlenecks do you face today with large models? Comment below or message me - I'm happy to discuss ideas on how LoRA+ could help your team.",
        "time": "5d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 235,
        "person_name": "https://www.linkedin.com/in/jakubzavrel",
        "text_description": "As big names in AI are already thinking about trillions of dollars of compute, let's remember that our current approach to AI is ridiculously inefficient and the human brain consumes orders of magnitude less energy for the same results.\n\nAs Groq recently showed for inference, there is still a lot of low hanging fruit on computational optimization. The whole power balance in the AI ecosystem could change with one research paper.\n\nNovel approaches like Tensor3D, Tenplex, RTP, G10, and DistPar aim to optimize parallelism in deep learning, reduce communication overhead, improve memory efficiency, and enhance scalability on multi-GPU clusters, showing promising results in reducing training time, enabling dynamic parallelization, minimizing memory consumption, and improving throughput in distributed neural network computing.\n.\n\nCheck out the Groq paper and follow recent related work on Zeta Alpha:",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 236,
        "person_name": "https://www.linkedin.com/in/francescopochetti",
        "text_description": "Watched Jeremy Howard's latest and greatest lectures on how to write 🚄💨CUDA kernels from scratch starting from pure 🐢 Python.\nI wrote a very visual post on the first of them. \nRGB-2-grayscale and matmul at GPU speed.\nEnjoy!",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 237,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 238,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 239,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My opinion : A significant factor in the widespread adoption of large language models (LLMs) in the industry will not merely be the increased accuracy of these models, but rather the development and operation tools that facilitate easy integration of LLMs into various development and production environments.",
        "time": "6d ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 240,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Going through US immigration with Global Entry is a breeze: just stand in front of the machine, look at the camera, and it's done.\nThank you, ConvNet-based face authentication!\n\nHard to predict that our 2005 CVPR paper that revived Siamese nets could have this kind of impact (if indirect)",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 241,
        "person_name": "https://www.linkedin.com/in/ericalunalee",
        "text_description": "Join us on Mar 7 in Culver City for an event on \nhashtag\n#GenerativeAI! Hosted by my nonprofit WomenOfAI.org with Scopely featuring our keynote speaker Faith 'Aya' U. of ImmerseMe & a special intro by Tricia Bertero of Scopely!\n\nTalk won’t be recorded or live streamed so try to join in person if local. This low cost ticket has 100% proceeds going to our nonprofit.\n\nSign Up:",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 242,
        "person_name": "https://www.linkedin.com/in/ashvardanian",
        "text_description": "SimSIMD crosses 200,000 Python downloads and is now compatible with more Python environments than \nhashtag\n#NumPy and \nhashtag\n#SciPy! It's not only up to 200x faster, but also the most portable way to compute Vector Similarity in modern\napplications 🎉\n\n\n\nYou can't properly run SciPy in AWS Lambdas or Amazon Linux OS onGraviton chips. Same forOS or any other CentOS-based operating system onCPUs...\n\nThis includes some of the largest supercomputing clusters under construction right now. The reason being, that NumPy and SciPy don't cover the full spectrum of builds. SciPy ships 25 packages. SimSIMD ships 105. Check yourself and integrate into your applications 🤗\n\nSciPy:\nSimSIMD:\n\nPS: If you are in SF and love working on such stuff, consider joining our hackathon this Saturday:",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 243,
        "person_name": "https://www.linkedin.com/company/megagon-labs/",
        "text_description": "Invited Speaker Joon Sung Park, is a computer science PhD student in the Human-Computer Interaction and Natural Language Processing groups at Stanford University. His work introduces the concept of, and the techniques for creating generative agents -- computational agents that simulate human behavior. His work has won best paper awards at UIST and CHI, as well as multiple best paper nominations and other paper awards at CHI, CSCW, and ASSETS, and has been reported in venues such as The Times, The Guardian, NBC, The New York Times, Wired, Science, and Nature. Joon is recognized with the Microsoft Research Ph.D. Fellowship (2022), Terry Winograd Fellowship (2021), and Siebel Scholar Award (2019).\n\nThank youfor enlightening us with your work!",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 244,
        "person_name": "https://www.linkedin.com/in/philipp-schmid-a6a2bb196",
        "text_description": "Yesterday, Google released Gemma, an open version based on Gemini! 🤗 Did you know you can try and experiment with it for free using the Hugging Face API and the OpenAI SDK? 🤯 Yes! With the new Message API of Text Generation Inference, you can change 2 parameters and switch from closed to open\n\n```python\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\nclient = OpenAI(\nbase_url=\"\",\napi_key=\"hf_xxx\") # Replace with your token\n\nchat_completion = client.chat.completions.create(\nmodel=\"google/gemma-7b-it\",\nmessages=[\n{\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n],\nstream=True,\nmax_tokens=500\n)\n\n# iterate and print stream\nfor message in chat_completion:\nprint(message.choices[0].delta.content, end=\"\")\n```\n\n\nGemma Blog:\nMessage API Blog:\n\nIf you hit rate limits, you can create a dedicated endpoint on Inference Endpoint with 1-click of a button or use Google Cloud. 🤗",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 245,
        "person_name": "https://www.linkedin.com/company/voestalpine-hpm-digital-solutions/",
        "text_description": "DIGITAL SOLUTIONS is an early adopter in the field of Artificial Intelligence (AI)! Shout-out to ai-landscape.at for highlighting. We are keen on exploring new possibilities to enhance our business and drive \nhashtag\n. At, we are investigating the potential of Generative AI and Large Language Models (LLMs) to create substantial value for our customers' operations. 💼 Some possible applications include summarization and extraction, content generation, Q&A assistants, and more. 📈\n\nWhile exploring the vast opportunities that AI can offer to our businesses, it's crucial to have a solid understanding of what AI is and what it can do:\nGenerative AI is a rapidly evolving field with the potential to transform the way we produce and engage with content. 🚀 Capable of autonomously generating a diverse range of content, such as text, images, music, and code, by learning from extensive datasets, Generative AI exhibits human-like creativity. 🤖 Nevertheless, its outputs often require human oversight for accuracy and appropriateness in various applications. 🔍\n\nIs your company at the forefront of the technological revolution with Generative AI? What ideas are you exploring with this technology? 🌟 Do you think the content you just read was created using AI? Let us know in the comments below!\n\nWhat is GenAI?:\nVideo - GenAI explained in 2min:",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 246,
        "person_name": "https://www.linkedin.com/company/it-university-of-copenhagen/",
        "text_description": "Efficient queries = faster, cheaper, and more energy friendly systems! 💻☘️\n\nIn a new research project, Associate Professor at ITU,, will investigate how ranking algorithms influence query efficiency, and develop new models for query optimisation.\n\n\"Ultimately, our vision is for the project to pioneer a new research paradigm,\" says Zoi Kaoudi.\n\nLearn more about the project in this article. 👇\n\nThe project is funded by. 🙏",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 247,
        "person_name": "https://www.linkedin.com/in/joe-kelleher-04aa899",
        "text_description": "👇 💡 👇 💡 👇",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 248,
        "person_name": "https://www.linkedin.com/in/junta-nakai",
        "text_description": "“One of the reasons we’ve been so successful is because \nhashtag\n#AI is a CEO-level priority at every single company in the world,” Nakai said. \n\nI did an interview withon whyis becoming a defacto technology in Financial Services.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 249,
        "person_name": "https://www.linkedin.com/in/tylermoynihan",
        "text_description": "We should all be standing behind this amazing woman, Yulia Navalnaya, who is taking her murdered husband’s place as the mantle bearer for opposition to Putin.\n\nAlexei Navalny knew the danger he faced, after being falsely charged, prevented from running for office, and shamefully poisoned.\n\nHe returned to Russia anyway and was immediately imprisoned and ultimately murdered. This is the definition of tyranny.\n\nPutin does the same thing to political opponents as he is doing to Ukraine. He muders innocent people to quench his thirst for unbridled power. It sounds like a horror movie. Except its real.\n\nI’m not saying anything new here. But we now have the opportunity to support this woman who is now herself facing incredible personal danger in order to carry on her husbands calling to bring real freedom to Russia.\n\nThat’s something worth supporting.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 250,
        "person_name": "https://www.linkedin.com/in/loubna-ben-allal-238690152",
        "text_description": "Today we’re releasing 🌌 Cosmopedia: The largest open synthetic dataset of textbooks, blogposts and stories generated by Mixtral with a total of 25B tokens and 30M files 🚀\n\n\nA little backstory to this \"cosmic\" journey: Two weeks ago I started experimenting with some cool web clustering fromand synthetic data prompts byand. I was incredibly impressed by the quality and diversity of the generations when using Mixtral. Then 600 H100 GPUs became free on the HF cluster for a night!\n\n💡 Given we had llm-swarm library bywhich scales efficiently for data generation,and I scraped some data and launched the pipeline at full capacity, resulting in 25 billion tokens from textbooks, blog posts, and stories with GPT-3.5 quality, all under Apache2.0 license.\nThis makes Cosmopedia the largest synthetic dataset available 🚀\n\nWe also trained a Phi-like model on it, cosmo-1b, to test the quality of the dataset:\nIt's comparable to other 1B models on many evals :)\n\nWe're sharing it all with you: the dataset, prompts, and end-to-end pipeline\n\nThis is version 0.1 of the dataset, with significant room for improvement: additional generation styles, languages, better coverage of scientific topics and even better models! Super excited about what the community will build on top of it. Enjoy!✨",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 251,
        "person_name": "https://www.linkedin.com/company/oxforduni/",
        "text_description": "Yesterday, Oxford University hosted Prof Geoffrey Hinton, the ‘Godfather of AI’, to deliver its annual Romanes Lecture at the Sheldonian Theatre.\n\nWatch in full ⬇️",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 252,
        "person_name": "https://www.linkedin.com/in/chiphuyen",
        "text_description": "I never dreamed of this happening, but I walked into a public library in Tokyo and found a translated copy of my book!!\n\nMany people asked me why I chose to work with a publisher (O’Reilly) instead of self-publishing. Because of the opportunity to see my book in different countries, like this.\n\nLike most people, I don’t write books for financial reasons. I write because I love writing, and it makes me really happy to see my writing reaching people in a way I never thought possible.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 253,
        "person_name": "https://www.linkedin.com/in/pieromolino",
        "text_description": "Today we are releasing \nhashtag\n#LoRA Land: 25 fine-tuned \nhashtag\n#mistral-7b \nhashtag\n#llm that outperform \nhashtag\n#gpt4 on task-specific applications.\n\nAll 25 fine-tuned models\n📈 Outperform GPT-4, GPT-3.5-turbo, and mistral-7b-instruct for specific tasks\n⚡️ Are cost-effectively served from a single\nthrough\n\n💰 Were trained for less than $8 each on average\n\nThis is pretty important imho. It shows how smaller, task-specific models are the best option for deploying LLM applications in a cost effective manner after you are done building a prototype with GPT-4 and you want to keep costs in check and keep control of your models.\n\nYou can prompt all of the fine-tuned models today and compare their results to mistral-7b-instruct in real time in our LoRA Land UI.\n\nAnd you can also download all of them through!\n\nLinks in the comments.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 254,
        "person_name": "https://www.linkedin.com/in/ashvardanian",
        "text_description": "Do you have C++ engineers in your company? Chances are, their code can become noticeably faster by simply swapping the default string implementation. \n\nWhat's wrong with strings in C++, and how do you fix them?\n\nArticle:\nRepo:\n\nIn the article, I go through throughput/latency issues and potential safety issues originating from ambiguous APIs. On the one hand, the C++ standard library string generally relies on LibC for heavy lifting, which can be 4x slower than a hand-written kernel. Conversely, some function interfaces, like \"replace,\" have 14 different overloads with different argument orders, making it impossible to guess the right one. There are also some cool features we can borrow from\nand\n! So let's fix\nstrings together!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 255,
        "person_name": "https://www.linkedin.com/in/deanwampler",
        "text_description": "Join me tomorrow now in Chicago!",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 256,
        "person_name": "https://www.linkedin.com/in/ruchi798",
        "text_description": "🎉 Checked off a milestone from my bucket list by contributing as a technical reviewer for the book 'Generative AI with LangChain’.\n\nAs someone who's navigated the world of Data Science, transitioning from a Kaggle Grandmaster to a Lead Data Scientist, I've seen firsthand how transformative AI can be.\n\nIf you're curious about Generative AI but don't know where to start, I've crafted something for you, drawing from my own experiences.\n\n⬇️ Steal my 10-step roadmap for your guidance:\n\n1> Understand the Basics:\nLearn how Generative AI is changing the way we work with text, images, and videos, focusing on the basics like neural networks and deep learning.\n\n2> Explore Important Models:\nGet to know about Large Language Models (LLMs), Transformer architecture, and cool models like Stable Diffusion, and their big impact.\n\n3> Try It Yourself:\nApply concepts through small projects or online platforms.\n\n4> Improve Your AI:\nLearn how to make your models smarter and more accurate, using special training methods and LangChain.\n\n5> Explore Ethics and Privacy:\nConsider how your AI work affects ethics and privacy, and make sure you're using AI in a good way.\n\n6> Put AI to Work:\nUse LangChain to add AI to real systems, making them smarter and more useful, like checking facts or improving how AI understands instructions.\n\n7> Deploy at Scale:\nLearn how to make your AI solutions accessible to a wider audience.\n\n8> Stay Updated:\nAI evolves rapidly. Keep learning.\n\n9> Solve Real Issues:\nUse AI to help with coding, make data science better, and discover new things, solving actual problems.\n\n10> Share and Collaborate:\nJoin AI communities, share what you've made, and collaborate to do even more amazing things with AI.\n\nBut don’t stop here!'s book is a detailed guide on how to turn your ideas into reality using LLMs and more, and it fits perfectly with this journey.\n\n💬 Have you experimented with any Generative AI projects or ideas that you're excited about? Share your work in the comments below!",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 257,
        "person_name": "https://www.linkedin.com/in/ernest-franklin-spicer",
        "text_description": "Some great material for those in the LLM world.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 258,
        "person_name": "https://www.linkedin.com/in/shubhampachori",
        "text_description": "Meet Mamba-Chat, the cutting-edge language model, from Haven, that uses a novel architecture which greatly improves the model’s ability to handle large-scale and complex datasets with unprecedented efficiency. \n\nTry this NVIDIA-optimized\ndirectly from a browser or through API.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 259,
        "person_name": "https://www.linkedin.com/in/sibaraki",
        "text_description": "My Chat with the outstanding Yannis Ioannidis (Ph.D.): notable \nhashtag\n#Researcher; \nhashtag\n#President ACM, Association for Computing Machinery — world’s largest \nhashtag\n#computing organization and No.1 in \nhashtag\n#computingscience; \n\nDepartment of\nand\n;\n\nFacultyand\nCenter;\n\nPast coordinator, legal entity head\n;\n\nSoftware\nof the\nProject;\n\nGreek delegate;\n\nGlobal\nHub of the\nNetwork.\n\n\n*Video interview:\n*Yannis's interview profile with\n:\n\n*Interview highlights:\n-\n,\nstorytelling, bridging\nand\nlanguages. 0:00\n\n- Storytelling,\n, and their impact. 6:21\n\n- Using\nin various\n. 11:55\n\n-\ncreativity,\n, and\npotential. 19:21\n\n- AI advancements and specialized computing. 25:58\n----hybrid systems;\n;\n;\n;\n;\n\n\n- ACM presidency and future of computing. 34:15\n----discusses ACM presidency,\n, and\nresearch.\n----conceived ACM 4.0 to address changes needed for the organization's next 25 years.\n----discusses ACM's efforts to apply\n.\n\n- ACM's role in achieving UN\nand\nfor\nmitigation. 41:40\n----seeks ACM's help in uniting industry and members to tackle SDGs through computing-related solutions.\n----feels optimistic about ACM's role in climate change mitigation after noticing increased focus on digital technologies at a recent conference.\n---shares experiences from global visits, talks on technical research and ACM's role in serving diverse needs worldwide.\n\n- ACM's future and the role of technology in society. 49:36\n\n- Yannis Ioannidis seeks second term as ACM President to continue 4.0 initiatives.\n----encourages bold thinking and idea sharing to drive ACM's impact on computing professionals and the world.\n----the importance of being both a dreamer and a doer, and pursuing imagination to action for the benefit of humanity and ecosystems.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 260,
        "person_name": "https://www.linkedin.com/in/yannis-ioannidis-b718a78",
        "text_description": "Many thanks to Stephen Ibaraki, who recently took the time to sit down with me and discuss ACM, Association for Computing Machinery's vision for the future of computing. I enjoyed talking about the many ways that the latest advances in\ntechnologies, such as\nor\n, will continue to impact\n, making it critical for relevant\nand\nto always be in focus. Watch our conversation in Stephen's post below!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 261,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "An interesting example of hallucinations of Sora. \nCan you spot them ?",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 262,
        "person_name": "https://www.linkedin.com/in/rkellehernvidia",
        "text_description": "If you’re passionate about AI in the life sciences and are interested in helping an ecosystem grow on NVIDIA platforms, this is an incredibly opportunity to learn from the best.",
        "time": "1w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 263,
        "person_name": "https://www.linkedin.com/in/philipp-schmid-a6a2bb196",
        "text_description": "Can we use an LLM for embedding retrieval and text generation at the same time? New research from Contextual AI explores this in generative representational instruction tuning (GRIT), achieving state-of-the-art performance in both generative and embedding tasks. 🤯 The paper also open source GritLM7-B & GritLM-8x7B, which can generate embeddings and text! 🔥\n\n𝗜𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻\n1️⃣ Select pre-trained LLM, e.g. Mistral or Mixtral and adapt the model to use different attention mechanism\n2️⃣ Create a mixed dataset with instruction (Tülu) and retrieval tasks (E5)\n3️⃣ Trains LLM on the dataset using contrastive loss for embedding samples and Language Modelling Loss for instruction samples. Separation is achieved by different “BOS” tokens, e.g. <|embed|>.\n\n𝗜𝗻𝘀𝗶𝗴𝗵𝘁𝘀\n🏆 GritLM 7B scores #1 on MTEB with 66.8 and keeps a MMLU score of 57.6\n🧠 Uses bidirectional attention (BERT) for embedding and causal attention (GPT) for generation\n🤝 Matches the performance of embedding-only models\n🐌 Embedding creation should be slower compared to S-BERT models\n🔍 RAG can be sped up by caching the query or key-value states of the attention. Only works with single document retrieval (top 1).\n🥇 Can be used for reranking too\n🤗 Models on\n\nArxiv:\nGithub:\nModels:\n\nHuge Congrats to,and the wholeteam on this release and research!🤗",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 264,
        "person_name": "https://www.linkedin.com/in/emollick",
        "text_description": "It may not be as visible on a broad platform like LinkedIn, but a lot of exciting research has been published on using generative AI to solve real-life issues in key subfields in medicine, marketing, law, and other topics.\n\nHere, for example, is a review of published papers in 2023 on using ChatGPT in radiology.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 265,
        "person_name": "https://www.linkedin.com/in/patricia-arocena-phd",
        "text_description": "Looking forward to discussing the impact of AI in the Financial Services industry at the Association for the Advancement of Artificial Intelligence (AAAI) bridge event on Feb 20.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 266,
        "person_name": "https://www.linkedin.com/in/georgegu06",
        "text_description": "I’m happy to share that I’m starting a new position as AI Architect at ModMed!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 267,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "\"Will AI lead us to our end?\"\nA conversation with Nate Lanxon (Bloomberg) at the World Government Summit earlier this week.\n\nThe answer is \"no!\"\nBut we need to do it right.\nIn particular, if regulations make it difficult for open source AI platforms to exist, it will lead to a world in which all of our information diet will be controlled by a small number of closed and proprietary systems.\nThat would be very dangerous for democracy and cultural diversity.\n\nWe need free and diverse AI assistants for the same reason we need a free and diverse press.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 268,
        "person_name": "https://www.linkedin.com/in/swapnasaritsahu",
        "text_description": "We are Expanding \nhashtag\n#search team . If you are one who wants to build the next generation search, apply below positions. We play with \nhashtag\n#NLP, \nhashtag\n#transformers and \nhashtag\n#LLMs everyday. Trying to bring the next generation Search technology to\n. We have multiple positions in\nand\n.\n\nYou should be one who are good at these technologies with relevant experience. you are someone either love to\nor design\nfor\n\n\nFor Director position in India :\n\n\nFor Director position in Minneapolis :\n\n\nFor L6 ( lead Data Scientist positions ) India :\n\n\nFor L5 ( Sr Data Scientist positions) India :\n\n\nDo apply above rather than sending me the resume. Please make sure you have extensively worked in Search, NLP and Information domain.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 269,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 270,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 271,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 272,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 273,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "\"OpenAI has been developing a web search service in challenge to Google\"\nI believe openAI can do it.\nin 2024, one can develop a search engine index for < 10B pages (with the crawling infrastructure to have it reasonably fresh, even handle very frequent updates for 100M pages), integrate with RAG (or similar ) infrastructure, and tune retrieval/ranking for reasonably good search quality with a relatively small number of people and time. OpenAI certainly can do it. It will be hard to compete with Google or Bing head to head in the big web search, but if openAI focuses on a certain narrow (but very important for billions of people) area \"vertical\" they may invent a product where they can outperform competitors (for example, local/travel, e-commerce/shopping, news/events)",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 274,
        "person_name": "https://www.linkedin.com/in/michaelpcassidy",
        "text_description": "Seraphim.vc data shows VC investments in space were actually pretty strong in 2023 - much better than VC investments in other industries: \"While general venture capital investment saw a 35% drop in investment between 2022 and 2023, SpaceTech substantially outperformed. Despite a slow start to 2023, the year culminated with a total annual investment of $6.8bn, in line with 2022 investment levels.\"",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 275,
        "person_name": "https://www.linkedin.com/in/wsuverkropp",
        "text_description": "Oh look....that's 'my' product in this video! \n\nStreetview cameras have come a very long way, from experimental platforms to highly integrated, compact cameras! Look for the little 'owl' camera - that's what I've been working on with my wonderful team.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 276,
        "person_name": "https://www.linkedin.com/in/irinapetrakovaotto",
        "text_description": "Last week was interesting for me. I had a few unprompted conversations about \"real\" reasons that we don't see women in STEMs It reminded me that we have a long way to go....\nhashtag",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 277,
        "person_name": "https://www.linkedin.com/in/anima-anandkumar",
        "text_description": "Super excited to share our new efficient training routine of Neural Operators via mixed precision, recently accepted at \nhashtag\n#ICLR2024! We show more than 50% gain in throughput with almost no loss in accuracy. \nNeural operators are\nmethods for solving\n. Unlike traditional numerical solvers, which require high precision, we show that neural operators can have reduced precision without loss in accuracy.\nWhile automatic mixed precision (AMP) methods have been developed for standard neural networks in deep learning, we cannot directly apply them to neural operators due to a few reasons: (1) AMP only works on real numbers, while neural operators can be complex-valued, (2) Since neural operators work on PDEs that may be turbulent and hard to solve, we need to guarantee stability even as we reduce precision.\nIn our paper, we prove theoretically that neural operators can work under reduced precision as well as show those benefits in practice.\nFor the theory: we use the intuition that neural operators already incur discretization error and we can reduce precision as long as it does not exceed this error. For bounded L-Lipschitz functions and Fourier neural operator (FNO), we show that the precision error of float16 is always bounded by the discretization error of Fourier transforms for mesh dimensions up to a million. How does this conclusion hold up in practice? Our experimental setup includes 5 PDE datasets spanning 2D/3D, planar/spherical, regular/irregular geometries. After making minor architectural changes, we show that lower precision works well practice.\nPaper:",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 278,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Yesterday evening, the TIME100 Impact Award dinner took place at the Museum of the Future in Dubai.\nA very elegant event in which I received the award along with InstaDeep CEO Karim Beguir, artist Sougwen Chung, and AI ethicist Kay Firth-Butterfield []\n\nThe award was presented to me by TIME CEO Jess Sibley who said \"Yann stands out the most because he has never been afraid to speak his mind and go against the grain.”\n\nYou can watch the video of my acceptance speech in the link below: understanding intelligence and amplifying human intelligence with machine intelligence are intrinsically good pursuits. But we need AI platforms to be open to ensure that our future AI assistants reflect the diversity of human languages, cultures, value systems, and centers of interest.\n\nNote: it's called TIME100 because TIME is 100 years old, not because there are 100 awardees (there are 4 this year).",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 279,
        "person_name": "https://www.linkedin.com/in/thom-wolf",
        "text_description": "Crazy what you can do with a $200 robot arm nowadays. Will low-cost and open-source robotics have its \"Imagenet moment\" in 2024?",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 280,
        "person_name": "https://www.linkedin.com/in/sonalsgupta",
        "text_description": "Next Chapter: Google Gemini, building the future of AI! 🚀 🚀 \n\nI'm excited to announce that I have joined the incredible Google Gemini team! While this marks a new beginning, I wouldn't be here without the amazing experiences and growth I had during the past six years at Meta.\n\nThose years were filled with personal and professional leaps. We brought groundbreaking ideas to life, including Make-A-Video, training the first product text-to-image generation model at Meta, releasing Text-to-Stickers, and pushing the boundaries of conversational AI in both semantic parsing and question answering. Each project challenged me, stretched my skills, and ignited my passion for blending research with impactful applications.\n\nI'm incredibly grateful to my exceptional collaborators at Meta for making this journey unforgettable. Your talent, dedication, and shared laughter made it all the more worthwhile. Thank you for everything!\n\nNow, I'm thrilled to join Gemini, where the combination of cutting-edge research and product development deeply resonates with me, along with the team's \"move fast together\" ethos. It’s also the chance to reunite with my smart-yet-funny grad school friends,and, and work together withagain!\n\nCan’t wait to share what we are building. Stay tuned for updates!",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 281,
        "person_name": "https://www.linkedin.com/in/susiekaradsheh",
        "text_description": "For those of you who leverage OpenAI, check out the below guidance. Absolutely impactful.",
        "time": "2w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 282,
        "person_name": "https://www.linkedin.com/company/llamaindex/",
        "text_description": "Introducing a Short Course Series on Advanced RAG Orchestration 🪄🤖\n\nAs an AI engineer, it can be daunting to dive into how to build high-quality, advanced RAG yourself - there’s literally hundreds of options at every stage of the pipeline.\n\nEasily stitch together custom modules into DAGs over your data, with observability baked in (here we showPhoenix 🔬).\n\nCheck out our first course in the series on query pipelines. We show you how to compose basic workflows like prompt chaining, output parsing and streaming to advanced RAG with query rewriting, retrieval, and more.\n\nYouTube:",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 283,
        "person_name": "https://www.linkedin.com/in/anindya-misra-430a435",
        "text_description": "hashtag\n#onecrm",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 284,
        "person_name": "https://www.linkedin.com/in/renaud-charrin-20a65011b",
        "text_description": "Exciting news! Moving to the Florida Keys Islands has been exciting!\nOur team at KEY WEST COMMUNITY SAILING CENTER INC is expanding and I'm thrilled to announce that I'm joining as a Member Board of Directors. Looking forward to contributing to this amazing organization and making a difference in our community.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 285,
        "person_name": "https://www.linkedin.com/in/karthickciyer",
        "text_description": "NVIDIA CEO Jensen Huang has been elected as a member of the National Academy of Engineering — one of the highest academic distinctions that can be awarded to engineers.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 286,
        "person_name": "https://www.linkedin.com/in/farahabdallah",
        "text_description": "Fascinating research on applying machine learning to the problem of compressing image data with multiple dimensions, such as color, contrast, etc. to reduce the number of pixels of an image to be transmitted via a retinal prosthesis, and help with vision! Could a similar approach be applied to reduce the computational cost of large models processing videos?",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 287,
        "person_name": "https://www.linkedin.com/in/jkibbey",
        "text_description": "Are you an experienced program/project manager who wants to work with a dynamic, fast-moving team of compliance professionals? We are hiring for a Senior Compliance Program Manager to assist our teams continue to innovate compliance concepts in our regulated and non-regulated lines of business. This is a great opportunity to work on the cutting edge of compliance! If you are dedicated to advancing compliance through innovation, please be in contact.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 288,
        "person_name": "https://www.linkedin.com/in/rvshankar",
        "text_description": "This visually stunning piece showcases technological advancements, highlighting the lineage of innovations that have transformed the way we interpret and utilize text data from 1950 to 2024.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 289,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Thank you @TIME for selecting me as one of three recipients of the \nhashtag\n#TIME100 Impact Award.\nI am being recognized, not because I'm a \"polarizing figure\" (hopefully) but because of my advocacy for open source AI platforms.\nI see it as a moral necessity: “In the future, our entire information diet is going to be mediated by [AI] systems, they will constitute basically the repository of all human knowledge. And you cannot have this kind of dependency on a proprietary, closed system.”\n\nThe other two recipients are artist Sougwen Chung and fellow Frenchman and ML/AI nerd Karim Beguir, CEO of InstaDeep.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 290,
        "person_name": "https://www.linkedin.com/in/piotr-%C5%BCelasko-937b33102",
        "text_description": "Very excited to finally share the news!\n\nToday we released a new state-of-the-art NVIDIA AI NeMo model Canary 1B.\nCanary is an ASR and speech translation model for English, French, German, and Spanish that tops the HF Open ASR Leaderboard. It outperforms both Whisper and Seamless, despite being trained on an order of magnitude less data!\n\n📄 Blog\n🎙️ Demo\n📀 Model\n\nIt's also the first released Nvidia NeMo model with its training powered by Lhotse. I'm planning to write separately about how NeMo and Lhotse can be used together to handle large and diverse data, maximizing efficiency and performance.\n\nFinally, kudos to the whole team for great execution. Working together on this project is a pure pleasure!",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 291,
        "person_name": "https://www.linkedin.com/company/unwrapai/",
        "text_description": "We appreciate the write up, Pacific Coast Business Times! As Jorge mentions, Santa Barbara's tech scene has been growing like crazy lately with companies like Sonos, Inc, Procore Technologies, AppFolio, Apeel, etc. and we're happy to be based here.\n\nAlso, we're hiring! Check out open positions on our website, or email your resume to.",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 292,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 293,
        "person_name": "https://www.linkedin.com/in/ping-li-a4624389",
        "text_description": "Machine Learning Enhanced Sparse Vector Search  with Privacy Protection\n\n02/07/2024 (today), 4pm Pacific/California time. \n\nZoom:(pls click for the the real link) , Password: 923229\n\nHosted by Instacart ()\n\nI sincerely thankand Dr.for the invitation,\n\nI am extremely excited to give the first public presentation after quite a long time. It is quite rare in my career that I finished the talk slides half a day earlier than the scheduled hour, and hence I would like to share them here with the community.\n\nThe current free-trial product () includes:\n(1) machine learning\n(2) vector search (extended to be fast neural research) , and\n(3) RAG-based prompt generator (presented as a chatbot for convenience).\n\nTo save operational costs, we currently only offer cloud solution with single CPU. Nevertheless, the speed seems to be fast, as told by many who have tried the product.\n\nThere is any question including requests for specific input/output data formats, please feel free to contact us. We can also remove the restrictions (such as the upper limit on the number of vectors) for certain users/collaborators.\n\nThank you and happy Lunar New Year!",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 294,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 295,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 296,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 297,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Recently I gave a talk about LLM Evaluation at the research institution in Zurich, Switzerland\nThe goal is very pragmatic, I do not want to review all methods or benchmarks,\nbut review methods and benchmarks that are useful as examples or templates to build evaluation suites in practical industrial cases\nI plan to rework it to make it more comprehensive and readable on its own, the current deck is intended to support my talk (I'll record it and share the recording)\nI'll be grateful for any comments und feedback",
        "time": "3w ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 298,
        "person_name": "https://www.linkedin.com/in/dtunkelang",
        "text_description": "OpenAI, meet open AI.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 299,
        "person_name": "https://www.linkedin.com/in/jamesmanyika",
        "text_description": "Throughout history, technology has helped transform how learning happens, from the slide rule and slate to the calculator and personal computer. I’ve long been excited about the potential for AI to assist teachers and students everywhere — especially the potential to help personalize learning and to harness the world’s knowledge and bring it to students everywhere.\n\nFor well over a decade I’ve been inspired byand the pioneering work that he and his team athave been doing pursuing their bold mission to provide a world-class education for anyone, anywhere – a mission I’ve supported.\n\nIn this conversation Sal and I discuss how he and his team are exploring the use of large language models as a Socratic learning partner — not to provide answers, but to assist students to think critically through prompts and questions on complex subjects, and help teachers too. We also discussed other opportunities and challenges associated with AI and learning - and the importance of harnessing the potential of AI and of getting it right.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 300,
        "person_name": "https://www.linkedin.com/in/mitodru",
        "text_description": "After months of hardwork, research, and brainstorming, I would like to introduce “Paramanu”, India’s first generative foundation language models pretrained from scratch and instruction-tuned on a single NVIDIA A100 40G for 10 low resources Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Sanskrit, Tamil, and Telugu)\n\nMade and Trained in 🇮🇳 with 🇩🇪 engineering ❤️ Innovation from Kolkata, India 🇮🇳\n\nTL;DR: High quality generative language models are possible without high amount of compute power and humongous number of parameters!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 301,
        "person_name": "https://www.linkedin.com/company/ddsa-the-danish-data-science-academy/",
        "text_description": "🤗 Announcing our third keynote for D3A 1.0 - Professor Barbara Plank 🤗\n\nDespite the recent success of Natural Language Processing\n, driven by advances in large language models\non enormous amounts of data, there are many challenges ahead to make human-facing NLP a reality - that is, more trustworthy and inclusive technology ✅\n\nWhile language varies along many dimensions, the conventional machine learning paradigm relies on learning from single ground truths, disregarding the nuanced subjectivity that is an integral part of human language variation.\n\nJoin us for Professor Barbara Plank's keynote and learn about some of the challenges and potential solutions - all related to the fascinating diversity of language 💬💬💬\n\nBarbara Plank is Chair for AI and Computational Linguistics at Ludwig-Maximilians-Universität München, Co-director of the Center for Information and Language Processing (CIS), Head of the Munich AI and Natural Language Processing (MaiNLP) research lab, and part-time Professor at.\n\n👉 Watch the full D3A 1.0 program at\n\n\n\n\nThomas Riisgaard Hansen\nSerge Belongie\nSofie Inari Castella\n\nPhoto:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 302,
        "person_name": "https://www.linkedin.com/company/nlplanet/",
        "text_description": "Open-source LLMs are now sufficiently good to be used as reasoning engines in agents 😮\n\n🧠 Open-source LLMs, such as Mixtral, are now demonstrating their abilities as core reasoning engines in agent workflows. Mixtral even exceeds GPT-3.5 benchmarks and holds potential for further optimization through fine-tuning.\n\n🤖 ReAct agents merge reasoning with proactive task execution using LLMs. These agents can utilize various tools to engage in reflective action cycles.\n——————————————————\nWant to stay at the forefront of Generative AI developments? Follow NLPlanet for daily insights into the most relevant news, guides, and research! 🚀",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 303,
        "person_name": "https://www.linkedin.com/in/orenetzioni",
        "text_description": "Our scientific advisors at truemedia.org are Renee DiResta, Darrell M. West, Jevin West, and Siwei Lyu. We will launch our deepfake fighting prototype in the next month--please join the waiting list.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 304,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "From my experience from big companies to startups, making corpora and converting it into proper training and evaluation sets for your model training tasks is one of the most challenging tasks in NLP, especially when one wants to address a large domain and needs very diverse corpora from web pages to articles, books, social media, other communications and other to address all nuances of the language but with quality bars to remove duplicates, low-quality content, toxic content, private data etc\nAllenAI released Dolma, a corpus that they used to train OLMo, but also a detailed description of how they collected the data",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 305,
        "person_name": "https://www.linkedin.com/in/barbara-plank-5045a4b",
        "text_description": "It was an amazing event, thanks for the invitation!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 306,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "The video of the Lytle Lecture I gave at University of Washington last week is available. \n\nTitle: \"Objective Driven AI: Towards Machines that can Learn, Reason, and Plan\"\n\n- Lytle Lecture Page:\n- Slides:\n- Video:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 307,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "https://lnkd.in/gNdpxw4i",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/gNdpxw4i"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 308,
        "person_name": "https://www.linkedin.com/in/bernhardhientzsch",
        "text_description": "On the occasion of the announcement of the retirement of Agus Sudjianto from Wells Fargo later this year, I want to express my strong appreciation to Agus and to Vijay Nair for their strong support for my team developing advanced models and methods for capital markets, championing their use, and encouraging us to pursue research and development in very advanced methodologies (including deep learning for FBSDE and other applications). Thanks to you, work did not always feel like work.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 309,
        "person_name": "https://www.linkedin.com/in/sarazanzottera",
        "text_description": "More tutorials are coming for \nhashtag\n#Haystack 2.0!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 310,
        "person_name": "https://www.linkedin.com/in/mikhail-burtsev-85a47b9",
        "text_description": "For those who missed the latest AI Finance workshop at London Institute for Mathematical Sciences . You can find link to slides in the short post from organisers.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 311,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Besides the biography, a lot of interesting details about mathematics and physics education in Ukrainian high schools, and after-school physics 'circles' , olympiads, (my life too)",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 312,
        "person_name": "https://www.linkedin.com/company/quanta-magazine/",
        "text_description": "As a student growing up in Ukraine, Maryna Viazovska (right) fell in love with competitive math Olympiads. In high school, she fell just short of being able to represent Ukraine at the International Math Olympiad. In 2022, she won a Fields Medal.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 313,
        "person_name": "https://www.linkedin.com/company/oxforduni/",
        "text_description": "'I want AI to be accessible to all.'\n\nLionel Tarassenko is Professor of Engineering Science in the University's Department of Engineering Science. He is also the first President of Oxford's newest college,.\n\nLearn how his decades-long career in AI and machine learning is now shaping the next generation of researchers. ⬇️\n\n\n|",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 314,
        "person_name": "https://www.linkedin.com/in/alexander-toshev-9270726",
        "text_description": "If you are excited about cutting edge research in Multimodal Foundational Models and Embodied Agents in particular, and ML in general, we have openings for Research Engineers. Feel free to apply at the link and DM.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 315,
        "person_name": "https://www.linkedin.com/in/dtunkelang",
        "text_description": "Microsoft owns Github, but it's Google that has partnered with Huggingface, aka the Github of AI. Google is making this move to counter Microsoft's alliance with OpenAI. Will all the AI startups take sides, or will any remain independent?",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 316,
        "person_name": "https://www.linkedin.com/in/theodorosgalanos",
        "text_description": "This past year has honestly been incredible.\n\nI feel very fortunate to be part of the Generative AI journey we've embarked on, and I am incredible excited with the things we've accomplished and the solutions that we will be soon deploying to the world.\n\nMy goal has always been to make an impact, change the way we do things, the questions we ask and the tools we use to get to the right answers. Now I get to bring ideas to life with AI.\n\nAnd yes, absolutely. It is a tool, not a toy. Arguably the most important tools we've ever invented.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 317,
        "person_name": "https://www.linkedin.com/in/vamshiambati",
        "text_description": "Super excited to launch our newest offering - LLMcloud.ai, a fast inference, and fine-tuning cloud for open-source LLMs and Generative AI Apps.\n\nOur dedicated team of engineers at, worked tirelessly to bring you top-performing open-source models, toolchains for comparing with commercial LLMs, one-click LLM deployments, LoRA fine-tuning workflows, and application stacks for building your own Chatbots, RAG search, Co-Pilots, and Web Agents.\n\nHere are some highlights (and more in the release blog):\n\n1. LLM Catalog of 25+ curated top-performing open-source models (Llama, Llava, Mistral, Mixtral) and commercial models (OpenAI, Gemini, Cohere)\n\n2. Model Inference speeds of ~22.5ms for 7B parameter LLMs, on par with industry bests like Anyscale (Try out today for free!)\n\n3. LoRA fine-tuning workflows for open LLMs with advanced evaluation and benchmarking to compare with commercial providers (in enterprise version)\n\n4. Application stacks for building your own Chatbots, RAG search, Co-Pilots, and Web Agents (try -)\n\nSign up now for free atand experience the power of\n+\n+\n\n\nTalk to us for the enterprise installation and setting up your own private GenAI cloud!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 318,
        "person_name": "https://www.linkedin.com/in/ashwinram",
        "text_description": "If you're betting your business on a single model from a single company, you might want to rethink your strategy. No one model is perfect for all use cases. I'm excited about this partnership between Hugging Faceandas it will accelerate the adoption of\nto create significant business impact with the best model(s) hosted on's enterprise-grade\nplatform.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 319,
        "person_name": "https://www.linkedin.com/in/devansh-devansh-516004168",
        "text_description": "To prepare for my upcoming look into natural gradients, I thought I'd go back to the idea that introduced me to orthogonality, the true power of richer decision boundaries, and more. Let's talk about the amazing CoshNet and how it uses Complex Valued Functions to improve Convolutional Neural Networks\n\nCNNs have been the go-to for Computer Vision forever. Their strength relies on the iterative feature extraction process. The lower layers of this process extract low-resolution features (edges, color changes etc) but stacking enough layers gives you access to some very high-resolution feature maps.\n\nHowever, this comes with a problem. Feature Extraction in CNNs is very expensive. Considering that the lower layers are just extracting simple features, it should be possible to pull out the information by using a fixed function. This is the idea behind CoshNet, which uses a fixed complex function for lower-level feature extraction. CoshNet shows stronger generalization, better performance, lower costs, more stability, and strong adversarial robustness compared to traditional CNNs.\n\nShoutout toand the rest of the team for this masterpiece.\n\nRead my analysis of CoshNet over here-",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 320,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Results of Google and MIT on tuning and using LLMs to multi\nModal data where other data includes time series ( sensor data and other ). Performs well on five tasks: mental health, activity tracking, metabolism, sleep, and cardiology.\nResults are practically interesting because this type of multi modality is very common in many other domains and areas of business. Such tuning of llm will have very practical consequences in applying LLM well beyond textual tasks to solve real problems",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 321,
        "person_name": "https://www.linkedin.com/in/armand-ruiz",
        "text_description": "In my organization at IBM, we currently have hundreds of generative AI pilots underway. Here are some of the proven use cases we've observed, along with their initial benefits: \n\n𝟭. 𝗥𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹-𝗔𝘂𝗴𝗺𝗲𝗻𝘁𝗲𝗱 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻 (𝗥𝗔𝗚)\n\nUp to 95% accuracy automating answers\n\n𝟮. 𝗦𝘂𝗺𝗺𝗮𝗿𝗶𝘇𝗮𝘁𝗶𝗼𝗻\n\nUp to 40% productivity gains in front and back-office functions\n\n𝟯. 𝗖𝗼𝗻𝘁𝗲𝗻𝘁 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻\n\nUp to 40% cost savings in content creation\n\n𝟰. 𝗡𝗮𝗺𝗲𝗱 𝗲𝗻𝘁𝗶𝘁𝘆 𝗿𝗲𝗰𝗼𝗴𝗻𝗶𝘁𝗶𝗼𝗻\n\nUp to 90% reduction of text reading and analysis work\n\n𝟱. 𝗜𝗻𝘀𝗶𝗴𝗵𝘁 𝗘𝘅𝘁𝗿𝗮𝗰𝘁𝗶𝗼𝗻\n\nUp to 80% faster in processing data\n\n𝟲. 𝗖𝗹𝗮𝘀𝘀𝗶𝗳𝗶𝗰𝗮𝘁𝗶𝗼𝗻\n\nUp to 30% cycle time reduction in customer service support\n\nStart small, validate initial concepts, and then strategically scale up.\n\nMy team can help, DM me if you would like to explore the capabilities of IBM watsonx",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 322,
        "person_name": "https://www.linkedin.com/company/llamaindex/",
        "text_description": "📖 Complete Architecture for Building Enterprise RAG 🛠️\n\nThis architecture by 🔭 Galileo provides a reference on both the algorithmic and system-level components needed to build production RAG 💫. There’s a lot in here 👇:\n\nIt covers everything from the following broad categories:\n✅ Handling user input: user auth 🔐, input guardrails, query understanding\n✅ Core advanced chunking/retrieval techniques: the bread and butter of\n✅ How to store embeddings/documents/chat history/user feedback, and vector db tradeoffs\n✅ How to choose a proper LLM inference solution\n✅ Handling output: output guardrails, collecting user feedback, observability\n✅ Other system-level improvements: Caching, multi-tenancy\n\nCheck it out as your weekend reading!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 323,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”To succeed, Ukraine and the West must align expectations and articulate a clear vision for the next 18 months: what we are building toward, how, and what the theory of success is moving forward. Without a long-term strategy, it will be difficult to achieve unity of effort and manage scarce resources…Much depends on sustained Western support and choices made now.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 324,
        "person_name": "https://www.linkedin.com/in/xin-luna-dong-6820953b",
        "text_description": "I felt extremely humbled to be named an ACM fellow and earlier an IEEE fellow. As someone who has grown up in industry, an award from a \"primarily academically focused\" organization like ACM is a truly special honor. I'm incredibly grateful for the numerous inspirations I have gained from the technical challenges unique at industry.\n\nThere is a long list of people I wish I could list here, to whom I owe my sincere thanks for their advice, guidance, support, encouragement, trust, inspiration, collaborations, and friendship!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 325,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "regarding grassroots efforts in our industry and open-source\nI think EleutherAI Harness is a very big success story by this year \nA large majority of research papers I have read use EleutherAI harness for evaluation\nAlmost all big dashboards such as HuggingFace (Open LLM Leaderboard) use it for the evaluation of LLM. There are other frameworks that had certain advantages (MosaicML Composer, efficient, multi-GPU scaling, FSDP until recent times, but Eleuther added them recently etc )\nWell-written code, easy to add new models and functionality, compatible with major inference frameworks, accelerators, support of adapters etc",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 326,
        "person_name": "https://www.linkedin.com/in/alexander-ratner-038ba239",
        "text_description": "Snorkel AI Enterprise LLM summit happening in T-minus ~20 min!\n\nTopics include:\n- The critical role of data development in real production GenAI\n- Enterprise POVs from\n- Details on our new programmatic alignment work - as seen on top of the AlpacaEval 2.0 leaderboard this week!\n- And much more...",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 327,
        "person_name": "https://www.linkedin.com/in/aishwarya-srinivasan",
        "text_description": "Did you hear that Sam Altman is reaching out to TSMC and Middle eastern investors for building his own AI chips? \n\nWith increased interest of companies to build their AI chips, I anticipate many job opportunities coming up for AI Hardware engineers 🧑‍💻 👩‍💻",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 328,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”It isn’t helpful to pretend that there is an obvious, near-term solution to these problems. Focusing U.S. military power and strategic attention overwhelmingly on Asia, as some analysts advocate, would take a toll on American global leadership in any circumstances. At a time when the Middle East and Europe are already in such profound turmoil, it could be tantamount to superpower suicide.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 329,
        "person_name": "https://www.linkedin.com/company/aiatmeta/",
        "text_description": "Together with the state of Indiana, today we announced that we will be building a new AI-focused data center in Jeffersonville, IN — one of the first built on the next-generation design that we announced in 2023.\n\nSee the full announcement ➡\n\nWe selected Jeffersonville for a number of reasons, including strong access to infrastructure and renewable energy, a strong pool of talent and a great partnership with the local community that has helped us propel this project forward.\n\nOnce operational, this data center will help us to train and deploy larger and more sophisticated AI models at scale, helping us deliver new forms of utility and entertainment for people and businesses across our apps and devices.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 330,
        "person_name": "https://www.linkedin.com/in/aditya-rane",
        "text_description": "Exciting news to start the long weekend. Two of my favourite platforms are coming together in perfect synergy. Hugging Face a thriving community of Gen AI developers with Scale and Security of Google Cloud is going to take Gen AI development to new heights !!!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 331,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "I've made that point before:\n- LLM: 1E13 tokens x 0.75 word/token x 2 bytes/token = 1E13 bytes.\n- 4 year old child: 16k wake hours x 3600 s/hour x 1E6 optical nerve fibers x 2 eyes x 10 bytes/s = 1E15 bytes.\n\nIn 4 years, a child has seen 50 times more data than the biggest LLMs.\n\n1E13 tokens is pretty much all the quality text publicly available on the Internet. It would take 170k years for a human to read (8 h/day, 250 word/minute).\n\nText is simply too low bandwidth and too scarce a modality to learn how the world works.\n\nVideo is more redundant, but redundancy is precisely what you need for Self-Supervised Learning to work well.\n\nIncidentally, 16k hours of video is about 30 minutes of YouTube uploads.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 332,
        "person_name": "https://www.linkedin.com/in/armand-ruiz",
        "text_description": "All Developers are Now AI Developers \n \nWorldwide, there are 30 million developers, 300,000 ML engineers, and only 30,000 ML researchers.\n\nFor those innovating at the very forefront of ML, the references estimate there may only be 50 researchers in the world who know how to build a GPT-4 or Claude 2-level system.\n\nThe good news is that in the face of these talent shortages, tasks that used to require years of fundamental research and sophisticated ML expertise can now be accomplished in days or weeks by mainstream developers building on top of powerful pre-trained language models.\n\nThe future belongs to those who can effectively apply existing AI, not just pioneer new AI.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 333,
        "person_name": "https://www.linkedin.com/in/ping-li-a4624389",
        "text_description": "I’m happy to share that I’m starting a new position as Co-Founder and CEO at www.vecml.com !\n\nThank Drfor making a tutorial video\n\nVecML () is committed to developing the cutting-edge infrastructures for building the next generation of LLM/AIGC applications, for accelerating accuracy, performance & cost-effectiveness with privacy protection of AIGC Infra.\n\nCurrently, we are fast iterating our products. As the initial offering, we provide a free-trial SaaS service for the following:\n\n(1) neural search engine with privacy, including the standard vector search, search with sparse vectors, search with constraints (business filters), etc. Additional features such as search with neural net similarities and search with GPUs are also available upon request.\n\n(2) machine learning (ML) platform. Our ML platform is fast, accurate, and easy-to-use. Both web UI and python interface are provided. To save cost, we place a limit on the training time. Please contact us if your company would like to have models with even higher accuracy.\n\n(3) Prompt generator, demoed as a chatbot. Users can upload private documents (such as PDF files), which are processed by our system to help generate the new prompt when a question arrive. The technologies behind the prompt generator include our neural search engine and ML platform.\n\nUsers can also fairly easily build an LLM-empowered recommender system using our tools.\n\nThis is just the beginning of our journey. We will frequently update our products to better serve the needs for developing the next generation of LLM/AIGC applications.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 334,
        "person_name": "https://www.linkedin.com/in/behshad-behzadi-6978864",
        "text_description": "The future of Video Generation is simply mind-blowing. Meet Lumiere! \n\nRead paper: https://lnkd.in/drcpq4sf",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/drcpq4sf"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 335,
        "person_name": "https://www.linkedin.com/in/rvshankar",
        "text_description": "If you're looking to level up your PyTorch game, this is the series to binge-watch to get started. The \"PyTorch Deep Learning Series\" by Luke Ditria is an insightful journey through various aspects of PyTorch and deep learning. Starting with the introduction, each section delves into fundamental topics like neural network linear classification, activation functions, and multilayer perceptron regression. The series progresses to advanced techniques, covering mastering convolutions, exploring deep ResNet basics, and enhancing CNN classifier performance. Luke also explores unsupervised learning strategies, autoencoders, and dives into object detection basics and semantic segmentation. The series concludes with a focus on generative adversarial networks (GANs). Whether you're a beginner or looking to deepen your understanding, this series provides valuable insights and practical examples.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 336,
        "person_name": "https://www.linkedin.com/in/vikram-chatterji",
        "text_description": "Excellent post by Pratik Bhavsar on how to build high quality generative AI apps using RAG! \n\nThere can be a lot of gotchas and landmines to look out for when using RAG with your data. If you are building apps using powerful vector databases such as,,,or others, highly recommend reading this.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 337,
        "person_name": "https://www.linkedin.com/in/clementdelangue",
        "text_description": "Google + Hugging Face + Open-Source AI = 🔥🔥🔥",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 338,
        "person_name": "https://www.linkedin.com/in/rvshankar",
        "text_description": "Deploying Llama2 models can be expensive but AWS has specific hardware in the Inferential family that supports cost effective deployment. The blog post discusses the deployment of Llama 2 on Amazon EC2 Inf2 instances using AWS Inferentia2 for both training and inference. It provides detailed steps on creating, compiling, and deploying the Llama-2 model using the latest AWS Neuron SDK release, achieving high performance at low cost.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 339,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I came across a paper that discusses how instruction tuning can improve Language Model (LLMs) performance for search tasks. The authors explore how this technique can enhance query and document understanding and matching. The results are obtained on different LLMs, and it looks like the technique is generic and LLM-independent. I expect that the impact of this paper both on LLM and search/IR community will be very big\n\nThe paper provides a lot of useful information on instruction design, volume, and other important parameters to make such tuning practical. The authors also share their data, making it easier for companies building search/ QA / engines to create their own datasets.\n\nSearch/IR/QA is expected to be the biggest application of LLMs both in monetary impact and the number of users for a very long time. I believe this paper is an important development with a big impact, and I highly recommend giving it a read. Check it out here:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 340,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "A must read!\n\n”Despite its faddishness, or perhaps because of it, the multi-domain operations concept is now guiding the transformation and modernization of Western armed forces and of their peers. Yet, there are real concerns about whether multi-domain operations will mature into a fully functional warfighting concept or whether it will go by the wayside like effect-based operations in the past.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 341,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I came across a paper that discusses how instruction tuning can improve Language Model (LLMs) performance for search tasks. The authors explore how this technique can enhance query and document understanding and matching. The results are obtained on different LLMs, and it looks like the technique is generic and LLM-independent. I expect that the impact of this paper both on LLM and search/IR community will be very big\n\nThe paper provides a lot of useful information on instruction design, volume, and other important parameters to make such tuning practical. The authors also share their data, making it easier for companies building search/ QA / engines to create their own datasets.\n\nSearch/IR/QA is expected to be the biggest application of LLMs both in monetary impact and the number of users for a very long time. I believe this paper is an important development with a big impact, and I highly recommend giving it a read. Check it out here:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 342,
        "person_name": "https://www.linkedin.com/in/sebastianraschka",
        "text_description": "LoRA remains my top choice among the various effective finetuning methods for LLMs. I've just developed and shared a \"LoRA From Scratch\" implementation here: https://lnkd.in/d-cXaGxB. \n\nThis is a hands-on approach to building LoRA from the ground up, which is, in my opinion, a great way to learn.\n\nAdditionally, there's aStudio available for those interested in experimenting with the code in this article. This also includes scripts for optimizing various LoRA hyperparameters.",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/d-cXaGxB."
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 343,
        "person_name": "https://www.linkedin.com/in/andrew-iain-jardine",
        "text_description": "Did Meta just drop a \nhashtag\n#llama 3 teaser? 🦙🦙🦙.........New self-play research shows \nhashtag\n#Llama2 can teach itself using DPO to beat \nhashtag\n#GPT4 🤯\n\nJust as Zuck teases the upcoming release of Llama 3, this colab withcomes out. This team isn't the same one behind Llama, but just imagine if they apply this to Llama 3 🤤. No doubt if Meta doesn't, the\ncommunity will soon after release.\n\n𝐖𝐡𝐚𝐭 𝐭𝐡𝐞𝐲 𝐝𝐢𝐝:\n1️⃣ Start with Llama 70B tuned on OpenAssistant data\n2️⃣ Use model to generate prompt + multiple responses to same prompt\n3️⃣ Same model used as judge to rank own responses\n4️⃣ Use DPO (Direct Preference Optimization) + rankings to re-train model\n5️⃣ Repeat steps 2-4 for 3 iterations\n\n𝐅𝐢𝐧𝐝𝐢𝐧𝐠𝐬:\n✨ Beats GPT4 0613,\nand Gemini Pro on AlpacaEval 2.0\n✨ Model achieves 20.44% win rate without using GPT4 outputs for training\n✨ Model simultaneously improves in instruction following and reward modelling\n✨ LLM judge Pairwise accuracy increases from 65.1% to 81.7% in 3 iterations\n\nPaper 👉",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 344,
        "person_name": "https://www.linkedin.com/in/behshad-behzadi-6978864",
        "text_description": "Meet AlphaGeometry - An Olympiad-level AI system for solving hardest geometry problems. Hats off to the Google Deepmind team!\n\nAs a highschool student, I was an active participant of Mathematics and Informatics olympiads. The problem statements in these competitions are some of the hardest creative thinking problems I have ever seen in my life (certainly harder than whatever I did afterwards in my studies up to PhD).\n\nAlphaGeometry significantly uplevels the previous state of the art (10) and practically reaches (25) at the level of smartest mathematicians in the world in solving geometry problems. This is truly mind-blowing! Congratulationsteam.\n\nAlphaGeometry combines the predictive power of a neural language model with a rule-based deduction engine which work in tandem to find solutions. It demonstrates AI's growing ability to reason logically, and to disvoer and verify new knowledge.\n\nRead the paper (Nuture):\nRead the blog post:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 345,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "A great interview of Nick Clegg and me in El Païs about AI.\nWe discuss the limits of current technology, the necessity of building human-level AI assistants, and the regulation debates.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 346,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Evolution\nFrom min to nano :)\nminGPT \n\n->\nnanoGPT\n\ninteresting to follow such projects of rewriting large chunks of code\nin new directions",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 347,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Evolution\nFrom min to nano :)\nminGPT \n\n->\nnanoGPT\n\ninteresting to follow such projects of rewriting large chunks of code\nin new directions",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 348,
        "person_name": "https://www.linkedin.com/in/ahmad-al-dahle-63a963a0",
        "text_description": "Today Mark shared that we are beginning to train Llama 3, and are moving toward a vision to achieve artificial general intelligence (AGI). We’re doing this backed up by an incredible amount of AI compute to power these efforts, ensuring that we have the resources that we need to train and deploy these models now, and in the future. While Llama 3 is early in training, the team and I are really excited about its potential\n\nOur vision for general intelligence means we're working on the hardest and most interesting challenges to push on the frontier of AI including multi-modality, reasoning, tools, planning and many other capabilities that's going to deliver utility that’s closer to what we get from people. We want to do this as openly as possible, to help advance the industry alongside us.\n\nI’m also thrilled that we are bringing FAIR and GenAI closer together as we work toward our long-term vision of building AI systems with general intelligence.\n\nIf you are an AI researcher and want to do work that will redefine the boundaries of what’s possible in artificial intelligence, please reach out!\n\nLink to MZ IG -",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 349,
        "person_name": "https://www.linkedin.com/in/irinapetrakovaotto",
        "text_description": "The Era of Copilots is here! If you interested in understanding how to build, extend and integrate with Microsoft copilots and build new products, this is for you! \nhashtag\n#microsoftcopilot \nhashtag\n#microsoftAI \nhashtag\n#genai \nhashtag\n#isv \nhashtag\n#productinnovation",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 350,
        "person_name": "https://www.linkedin.com/in/sandeep-singh-a4172554",
        "text_description": "I’m happy to share that I’m starting a new position as Principal Applied Scientist, Generative AI at Oracle Cloud!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 351,
        "person_name": "https://www.linkedin.com/in/ryanlpeterman",
        "text_description": "10 vetted engineering blog posts from Meta's top experts that will help you become a better engineer: \n\nPython can be fast:\n1. How Lazy Imports and Cinder accelerate machine learning at Meta by\n2. Introducing Immortal Objects for Python by\n3. Meta contributes new features to Python 3.12 by\n\nProgramming languages:\n1. From zero to 10 million lines of Kotlin\n2. Programming languages endorsed for server-side use at Meta by\n\nScaling backend services:\n1. Open-sourcing a 10x reduction in Apache Cassandra tail latency by\n2. Sharding & IDs at Instagram\n\nMedia technologies:\n1. Bringing HDR Video to Reels by,,\n2. How Meta brought AV1 to Reels by,,,,\n3. Why xHE-AAC is being embraced at Meta by",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 352,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "FAIR's mission is to develop the science and technology for human-level AI assistants: machines that understand the world, can perceive, remember, reason, plan, and act.\n\nIn the not-to-distant future, all of our interactions with the digital world will be mediated by AI assistants through our smart glasses and other devices.\nWe need these systems to have human-level intelligence if they are to understand the world, people, and tools, so as to help us in our daily lives.\n\nTo accelerate progress, FAIR is now a sister organization of GenAI, the AI product division.\nSimultaneously, Meta is building a massive computing infrastructure to support AI research, development, and production.\n\nOf course, we are committed to open research and open source AI platforms (yes, Llama-3 is coming!)\nFAIR is quite simply the best place in the world to advance AI research to the next level, and Meta is the best place in the world to get it in the hands of billions of people.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 353,
        "person_name": "https://www.linkedin.com/in/michele-montebovi",
        "text_description": "🌟 Fine-Tuning of the Mamba Model for Question Answering \nhashtag\n#AI \nhashtag\n#TechUpdate \nhashtag\n#MachineLearning\nI am excited to introduce a newly published model: the \"Question Answering Generative Model,\" a fine-tuned version of the renowned Mamba model, equipped with 370 million parameters.\n🔍 Overview: This advanced model excels in answering complex questions, uniquely capable of understanding when an answer is not present in the given context.\n🛠️ Architecture: It's based on the Mamba architecture.\n💡 Key Features:\nAdvanced Parameterization: Effectively balances capability and efficiency.\nContextual Understanding: Identifies when answers are not available in the context.\n🚀 Try it out and let me know your experience!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 354,
        "person_name": "https://www.linkedin.com/in/elizabeth-reid-56356724",
        "text_description": "Today at \nhashtag\n#SamsungUnpacked, we announced some exciting news for Google Search. We’re taking another step in the journey to help people search more naturally than ever before with Circle to Search. We’re also thrilled to bring gen AI responses to multisearch, helping you ask more complex questions about what you see.\n\nSo proud of our teams getting us closer and closer to helping people search any way, anywhere. And, we've only just scratched the surface of what's possible.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 355,
        "person_name": "https://www.linkedin.com/in/yonatan-geifman-8a449a3",
        "text_description": "I’m excited to share that Deci AI is collaborating with Qualcomm to streamline deployment of generative AI and LLMs on Qualcomm hardware. With the release of DeciCoder-6B and DeciDiffusion 2.0, both models are engineered for accurate and efficient performance at scale, on the Qualcomm Cloud AI 100, we’re making AI more accessible and cost-effective, and economically viable for a wider range of applications.\n\n𝐃𝐞𝐜𝐢𝐂𝐨𝐝𝐞𝐫-6𝐁 is a multi-language code LLM that supports 8 languages, outperforms CodeGen 2.5 7B, CodeLlama 7B, and StarCoder 7B, with 3 points advantage in Python over StarCoderBase 15.5B. It also achieves 19x higher throughput compared to CodeGen 2.5 7B.\n\n𝐃𝐞𝐜𝐢𝐃𝐢𝐟𝐟𝐮𝐬𝐢𝐨𝐧 2.0 is text-to-image diffusion model that outperforms Stable Diffusion 1.5, enabling 2.6x faster, cheaper image generation with image quality on par with Stable Diffusion v2.1. It generates images in under 1 second for a fraction of the cost.\n\nGenerated by Deci’s AutoNAC, a NAS-based engine that is hardware-aware, both models are optimized to leverage the full computational power of Qualcomm’s Cloud AI 100.\n\nWe can’t wait for everyone – from enterprises to the community – to push the boundaries of what's possible in AI efficiency and performance together with us at Deci, and our partner, Qualcomm.\n\n𝐃𝐨𝐰𝐧𝐥𝐨𝐚𝐝 𝐚𝐧𝐝 𝐥𝐢𝐤𝐞 𝐃𝐞𝐜𝐢𝐂𝐨𝐝𝐞𝐫 6𝐁 𝐚𝐧𝐝 𝐃𝐞𝐜𝐢𝐃𝐢𝐟𝐟𝐮𝐬𝐢𝐨𝐧 2.0 𝐨𝐧 𝐇𝐮𝐠𝐠𝐢𝐧𝐠 𝐅𝐚𝐜𝐞 >\n\n🤗 DeciCoder-6B Model Card:\n🤗 DeciDiffusion 2.0 Model Card:\n📘Technical Blog :\n📔DeciCoder-6B Notebook:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 356,
        "person_name": "https://www.linkedin.com/in/orenetzioni",
        "text_description": "An inspiring note from a hard-working and very talented individual!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 357,
        "person_name": "https://www.linkedin.com/in/pascalbiese",
        "text_description": "A new API: 4x cheaper than OpenAI AND better performance 👾 \n\nTogether AI just released their Together Embeddings endpoint, here are some of the highlights:\n\n✅ 8 leading embedding models – including models that outperform OpenAI’s ada-002 and Cohere’s Embed-v3 in MTEB and LoCo Benchmarks\n✅ State-of-the-art long context M2-Retrieval models up to 32k context length\n✅ Up to 4x cheaper than other popular platforms\n✅ Integrations with MongoDB, LangChain, and LlamaIndex for RAG\n✅ A fully OpenAI compatible API to make migrating easy\n\nIf you're still using OpenAI-ada-002 for your embeddings, you should consider stopping. There have been a lot of better and cheaper options for a while now - and with this new API, you get all of the recent advancements at a fair price.\n\nFor more information, check out their full blog post:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 358,
        "person_name": "https://www.linkedin.com/company/llamaindex/",
        "text_description": "RankGPT (by Sun et al.) is one of the first papers we’ve seen that uses out-of-the-box LLMs (gpt-3.5, gpt-4) to effectively rank documents 💫 - beating monoBERT + Cohere rerank. \n\nThis means it’s perfect as a second stage pass, to filter through documents selected through first-stage vector retrieval.\n\nThanks to, we’ve implemented it as a coremodule. This also allows you to plug in any LLM, not just! We have an example withbelow:\n\nNotebook:\n\nIn the case where the retrieved context exceeds the LLM context window, the authors implement a sliding window strategy to progressively rerank a window of chunks.\n\nCheck out the original paper here:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 359,
        "person_name": "https://www.linkedin.com/company/nlplanet/",
        "text_description": "When to use RAG and when to use finetuning for LLMs? 🤔\n\n👀 Developers optimize Large Language Models for various applications using Retrieval-Augmented Generation or finetuning, each with distinct benefits for specific tasks.\n\n1️⃣ RAG combines a retriever with an LLM to enhance responses using real-time data or external databases, ensuring responses are evidence-based and more accurate.\n\n2️⃣ Finetuning adjusts a model's weights with targeted training to improve its contextual, stylistic, or domain response, essential for cases needing specific behavioral or language adaptation.\n\n💁 Before choosing an approach, assess the use-case needs. If your app relies on updated external info, RAG may be suitable. For customizing model's tone or jargon for a niche, consider finetuning.\n——————————————————\nWant to stay at the forefront of Generative AI developments? Follow NLPlanet for daily insights into the most relevant news, guides, and research! 🚀",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 360,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "\"Today’s Royal Navy is largely a product of the 1998 Strategic Defence Review, which foresaw a fleet geared towards worldwide force projection. This was a logical decision given the threats facing British interests after the Cold War. But subsequent governments, distracted by counterinsurgency campaigns and eager to cut defence spending, reduced the number of escorts, especially destroyers and frigates, leaving the navy considerably smaller than the 1998 review envisaged. And since then, geopolitics has worsened.\"",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 361,
        "person_name": "https://www.linkedin.com/in/cobusgreyling",
        "text_description": "Meta Taxonomy Of Large Language Model Correction & Refinement\n\nA number of LLM techniques and classes of implementation are emerging, a recent study created a meta taxonomy of approaches and techniques on how to improve and correct LLM output.\n\nThis study surfaces a very insightful and comprehensive taxonomy for improving LLM output.\n\nThe taxonomy includes wide ranging elements, which can be used in isolation, or an approach can be multifaceted.\n\nThere is no right or wrong approach to refining and correcting LLM output.\n\nMuch depend on organisations access to people, technology and process...some key considerations are:\n\n1. The availability of annotated data and people to perform annotation.\n2. The extent to which a data productivity suite is available to accelerate data discovery and annotation.\n3. The level to which different LLMs are available to orchestrate the process of critique and refinement.\n4. Access to people, technology and processes for refinement approaches like in-context learning (RAG), RLHF, Supervision, etc.\n5. If correction will be performed during or after inference.\n6. The number of accessible LLMs, token usage considerations, inference latency, etc.\n\nLinks to the full article in the comments...",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 362,
        "person_name": "https://www.linkedin.com/in/sahar-mor",
        "text_description": "I recently came back from an LLM developers event and was mind-blown by how many builders still don’t cache LLMs’ responses, leading to unnecessary increased costs and latency.\n\nCaching, in the context of language models, is storing prompts and their corresponding responses in a database for future use. By caching responses to previously posed questions, LLM-powered apps can deliver faster and cheaper responses, bypassing the need for another LLM API call.\n\nGPTCache is a great start and only requires a few lines of code.\n\nIt works for an exact match, i.e., using the same prompt twice, or for a similar match, i.e., two prompts with the same meaning. For example, “Who was the first US president?” and “Tell me who was the first US president?” are not exact matches but would yield the same answer, therefore saving an API call.\n\nThe invaluable benefits of caching:\n(1) Faster and cheaper inference in production, with some queries achieving close-to-zero latency thanks to a cached response\n(2) Faster and cheaper development cycles, as you don’t incur costs or wait for the response when working with the same prompt repeatedly\n(3) Having all prompts stored in a database simplifies the process of fine-tuning a language model once you choose to do so, as you can use the stored prompt-response pairs\n\nGPTCache also provides metrics such as the cache hit ratio, latency, and recall to gauge how well your cache performs and improve it.\n\nWhat caching method do you use?\n\nGitHub repo\n\nMore techniques to reduce latency and LLM API cost, including fine-tuning, prompt engineering, and more, in this week’s AI Tidbits Deep Dive",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 363,
        "person_name": "https://www.linkedin.com/in/patrickcopeland",
        "text_description": "Don’t become a “declawed cat\" at work!\n\nA \"declawed cat\" here symbolizes a person who has lost their sharpness, agility, and passion to adapt or innovate. I’ve seen this many times at Microsoft, Google, and Amazon when people around me got too comfortable in a familiar setting, leading to a decline in their professional growth and enthusiasm.\n\nEspecially in larger companies, there are individuals who embody the \"declawed.” They are individuals who have been in the same role for an extended period without seeking new challenges or opportunities to learn. They tend to be valued by the company because they have institutional knowledge, but their days are marked by routine, devoid of the excitement that comes with new projects or learning opportunities. They may stick around because the environment is familiar, and they fear the uncertainty that comes with change. However, this safety comes at a HUGE personal cost: they lose their edge, stop acquiring new skills, and gradually become less valuable in a fast-evolving market. They also become overly imprinted with a coprorate culture that may not generalize to the market place.\n\nTo stay vibrant and effective in your career, it's crucial to keep challenging yourself. This could mean taking on new and complex projects, learning new skills, or even changing roles or companies to step out of your comfort zone. Maintain your sharpness, much like a cat that keeps its claws honed. Embracing continuous learning and welcoming change not only enhances your skill set but also keeps you mentally and emotionally engaged.\n\nComfort is the enemy of growth. The moment you feel too comfortable, question whether you're still growing or if you've started to become complacent. The goal is to remain a vital, contributing member of your team or organization, constantly bringing fresh ideas and perspectives. In a world that's always changing, the ability to adapt and learn is your greatest asset. Don't become a declawed cat; stay sharp — it’s a harder, but more rewarding path in the long run.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 364,
        "person_name": "https://www.linkedin.com/in/farahabdallah",
        "text_description": "MLK Day holds a special place in my heart, marking a favorite holiday in the country that became my home and my children's. We asked them, 'What would MLK do if he were here today?' My 4-year-old thinks all kids should go to the same school, and the 8-year-old wants more peace, especially for those who need it. Despite progress, MLK's timeless wisdom for love, peace, and the rejection of discrimination continues to resonate in our world.\n\"I refuse to accept the view that mankind is so tragically bound to the starless midnight of racism and war that the bright daybreak of peace and brotherhood can never become a reality... I believe that unarmed truth and unconditional love will have the final word.\"",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 365,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”By now, the reality should start sinking in that if we don’t change our policy, Ukraine will lose. And a Russian victory would deal such a blow to Western credibility, it would dwarf the consequences of the failure in Afghanistan.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 366,
        "person_name": "https://www.linkedin.com/in/satyanadella",
        "text_description": "With Copilot Pro, we're helping even more people supercharge their creativity and productivity by unlocking Copilot in Microsoft 365 apps, providing access to the very latest models—and more.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 367,
        "person_name": "https://www.linkedin.com/in/jordirib1",
        "text_description": "Our team is releasing Copilot Pro today which is a new subscription service for people who want priority access to the latest models. It includes Open AI's GPT-4 Turbo and Dalle3 with dual guidance and landscape mode. My favorite part is that Copilot Pro doesn't have the 30-turn limit, enables creating Copilot GPTs, and is twice as fast as regular Copilot. Once you've signed up, you get the Pro experience in all Copilot entry points such as Bing, Edge, Windows, and mobile apps. You can also access it in Microsoft 365 consumer apps like Word, Excel, Outlook, PowerPoint and OneNote. Below is an example of how to generate Dalle3 landscape images. More details on the release here",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 368,
        "person_name": "https://www.linkedin.com/in/maxime-labonne",
        "text_description": "🎉 NousResearch just released Nous-Hermes-2-Mixtral-8x7B\n\n🏆 Potentially the best open-source LLM: high-quality merges incoming!\n🥇 First fine-tuned version of Mixtral 8x7B that outperforms the official Mixtral Instruct\n📅 Trained on >1 million samples generated by GPT-4 + other open-source datasets\n\nThey released an SFT and a DPO versions of this model + GGUF versions.\n\nImpressive work, true MoEs like Mixtral are tricky to fine-tune. This is going to be a very popular model.\n\n🤗 SFT:\n🤗 DPO:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 369,
        "person_name": "https://www.linkedin.com/in/katharinaprobst",
        "text_description": "I'm excited to share that I'm joining the Google for Education team! I'll be leading engineering and data science teams.\n\nEducation is something that is very close to my heart. Outside of work, I support efforts such asand, and I couldn't be more excited to help move the needle on education across the world with Google.\n\nI'll work on Google Classroom, integrations with Chromebooks & Workspace, ensuring Google's educational products uphold the highest privacy & compliance standards, and new AI-powered products to make teachers more productive and help students learn. Sundar recently wrote an oped in India Today (see below) that talks about the impact Google's reading tutor can have, and this is one of the projects I'll have the privilege to work on!\n\nI look forward to working with an amazing team. Thank you,,,, and the rest of the team who have already spent lots of time with me ahead of this move! I've had a chance to meet many folks on the team and can't wait to work with them.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 370,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 371,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 372,
        "person_name": "https://www.linkedin.com/in/sahar-mor",
        "text_description": "12 battle-tested and research-backed strategies to slash LLM costs and speed up responses.\n\n2024 is shaping up to be a pivotal year for language models, transitioning from cool MVPs to production-ready products. In this second piece of my series dedicated to LLM builders and researchers, I'm diving into the practicalities of reducing cost and latency in LLM applications\n\nHere is an overview of six out of the twelve strategies I explore in this piece:\n\n(1) Semantic Caching - it’s surprising how many developers still don’t cache LLM responses. Simple yet effective, it cuts down costs and speeds up responses. GPTCache () is a perfect tool to start with - easy to implement and packed with actionable metrics.\n\n(2) Compressing prompts - with longer prompts becoming a norm, Microsoft's LLMLingua is a game-changer, compressing prompts by up to 20x with negligible performance loss. This means faster, cheaper API calls.\n\n(3) Summarizing conversations - multi-turn applications tend to accumulate tokens quickly as the conversation progresses. Using tools like’s ConversationSummaryMemory can slash tokens and latency.\n\n(4) Improving context retrieval for RAG-powered apps - refining the retrieval mechanism and ensuring only relevant context is being retrieved and sent to the LLM is key for sharper, cost-effective results.\n\n(5) Routing to cheaper models - GPT-4 is 8x more expensive than Claude 2.1, which is 10x the price of Claude Instant. Using inferior models for tasks that don't require high-end models can lead to substantial savings. Open-source packages suchmake such a transition seamless.\n\n(6) Accelerating inference - packages like vLLM and Hugging Face's TGI speed up open-source language models by up to 24x, supporting models such as Llama 2, Mixtral-8x7B, and Phi 2 out of the box\n\nMore techniques, including fine-tuning, prompt engineering, and more, in this week’s AI Tidbits Deep Dive",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 373,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 374,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 375,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 376,
        "person_name": "https://www.linkedin.com/company/llamaindex/",
        "text_description": "Parallelizing RAG Ingestion for 3-15x Speedups ⚡️⚡️\n\nThanks to Val Andrei Fajardo, LlamaIndex now allows you to scale up your RAG pipeline to ingest hundreds/thousands of documents with ease.\n\nThis includes both data loading (SimpleDirectoryReader) and data ingestion/transformations. See guides below 👇\n\n1️⃣ Loading:\n2️⃣ Ingestion:\n\nOne of our community membersfound that this sped up his document ingestion workflow by ~15x 🔥",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 377,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "My nephew (actually Großneffe ) \n is an undergraduate mathematics student (here in the USA)\nCan you recommend articles or books about deep learning, and generative AI written for mathematicians,\n(especially with interests in differential geometry)\nexample\n(this one is too long :) )\nSomething intended to make a mathematician interested in AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 378,
        "person_name": "https://www.linkedin.com/in/thom-wolf",
        "text_description": "Surprisingly unpopular opinion in AI today: Google (web-search) won't be \"simply replaced\" by AI chat. Web search is in large part a door to the ever growing and mutating world of human and human activities: new/old businesses, reviews, recipes, communities, memes, news discussions, travel sites...\nWhat do you think?",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 379,
        "person_name": "https://www.linkedin.com/in/alexander-ratner-038ba239",
        "text_description": "Prediction: 2024 is the year that enterprises move from internal LLM POCs/demos (mostly: GPT-4) to real production use cases (mostly: smaller fine-tuned + distilled models). It's a game of musical chairs in enterprise AI & the song is about to end- time to deliver real ROI!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 380,
        "person_name": "https://www.linkedin.com/in/niels-rogge-a3b7a3127",
        "text_description": "CLIP by OpenAI was truly a breakthrough, as it opened up many advances in image-text understanding, such as conditioning generative models like \nhashtag\n#StableDiffusion or \nhashtag\n#dalle on text, enabling text-based image segmentation and object detection, 3D understanding, and more. Nowadays, most multimodal large language models (like CogVLM or Llava) use a CLIP vision encoder as it's known to produce very good image features.\n\nResearchers atnow have improved CLIP by simply replacing the loss function used to train the model (softmax) with a simpler sigmoid. This way, the model can be trained by considering each image-text pair independently rather than requiring a global view of all pairs within a batch. They dubbed the model SigLIP (CLIP with Sigmoid loss). This allows to train with larger batch sizes and obtain a higher performance at smaller ones. The model gets state-of-the-art performance on zero-shot image classification and image-text retrieval tasks.\n\nThe model can now be used with a few lines of code in theTransformers library! Curious to see what people will build with this.\n\nDocs:\n\nCheckpoints:\n\nDemo notebook:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 381,
        "person_name": "https://www.linkedin.com/in/normanwinarsky",
        "text_description": "I’m happy to share that I’m starting a new position as Strategic Advisor and Angel Investor at Elemental Agronomy! \nThe 21st-century farming revolution will be powered by small autonomous robots and AI transformers. Elemental Agronomy will lead this revolution.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 382,
        "person_name": "https://www.linkedin.com/in/sahar-mor",
        "text_description": "UCSD and Google just presented a new method to improve language models on table understanding tasks, such as table-based Q&A and fact verification. \n\nChain-of-Table is a technique where complex tables are simplified into smaller, more manageable segments. This approach allows the LLM to understand and analyze each part of the table in depth. Smaller, more digestible table segments lead to more accurate generations, similar to how breaking down complex prompts has proven to yield better performance.\n\nIt is considered a “chain” because it creates a sequential linkage between different parts of a table where each link (or segment of the table) connects to the next, allowing for a step-by-step analysis.\n\nChain-of-Table achieves state-of-the-art performance in table understanding tasks and demonstrated strong results across various LLMs, including proprietary and open-source options\n\nThis achievement, along with other recent ones, such as JPMorgan's DocLLM, marks a significant leap in document understanding.\n\nPaper\n\n—\nJoin thousands of world-class researchers and engineers from Google, Stanford, OpenAI, and Meta staying ahead on AI",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 383,
        "person_name": "https://www.linkedin.com/in/ivanka-kasynets",
        "text_description": "AI technology influences not only social media but also brings value to defense potential.\nAndrei Lopatenko 🇺🇦 shared such an important and inspiring article.",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 384,
        "person_name": "https://www.linkedin.com/in/thom-wolf",
        "text_description": "what is or could be the Wikipedia of AI is a question I keep coming to",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 385,
        "person_name": "https://www.linkedin.com/company/llamaindex/",
        "text_description": "Learn how to build a full-stack RAG application that goes beyond the basics with LlamaIndex and @AzureCosmosDB!\n\n@seldo's latest talk to the Azure Cosmos DB User Group will take you from zero to sixty:\n👉🏻 What is RAG and how does it work under the hood?\n👉🏻 How to build a full-stack RAG application\n👉🏻 How to extend your app for precision and scale with advanced query techniques\n\nCheck out the video:\n\n\nOr head to the OSS repo including full instructions on how to deploy the app:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 386,
        "person_name": "https://www.linkedin.com/in/zainkahn",
        "text_description": "This is the wildest gadget I've seen in ages\n\nIt's a mobile devices that doesn't have any apps\n\nIt's powered with AI and you can do things like order a taxi or edit a document by speaking to it directly\n\nIt's called the R1 by Rabbit and here's why it's so cool:\n\n• In addition to a touchscreen, it also has a scrolling wheel and a moving camera\n• The camera can do some crazy things like edit documents and generate recipes if you point it at things\n• Most fascinatingly, a “teach rabbit“ functionality will let you train the device to do tasks on their behalf, like creating and editing new documents and sending them to people\n\nAlso cool that it's only $199 and doesn't need a subscription. Would you get this? I'm seriously considering it.\n________\n\nIf you liked this, join Superhuman - my newsletter with 500,000+ readers that teaches you how to leverage AI to boost your productivity:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 387,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”Putting ’more players on the field’ requires getting new construction ships delivered on time; moving ships out of maintenance periods on time; taking proper care of the fleet so the ships reach their full planned service life; and adding in unmanned systems and other disruptive technologies to maximize the manned ships and planes’ capabilities.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 388,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”The U.S. Navy’s surface fleet finds itself in a position not unlike the 1930s, when it needed new focus and energy to prepare for a potential war, the chief of naval operations said Tuesday.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 389,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”Gazprom’s declining profits provide a strong indicator for Moscow’s problems. The Russian energy giant is not in a position to build more pipelines or even more LNG processing facilities anytime soon on its own. It lost $10 billion during the fiscal year ending on June 30, 2023, and was forced to use almost two-thirds of its cash reserves to continue operating. Some analysts predict the company will lose another $10 billion in 2024 despite the Kremlin’s upbeat messaging.”",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 390,
        "person_name": "https://www.linkedin.com/in/alainchance",
        "text_description": "Researchers have demonstrated the potential of Generative artificial intelligence applications in chemistry such as Coscientist and ChemCrow. Their collaboration with policymakers is essential to prevent misuse of this technology.\n\nAI chemist performs complex experiments based on plain text prompts. On its first try, the technology could successfully replicate work that won a Nobel prize. By Prachi Patel, Chemical and Engineering News (), December 26, 2023,\n\nDaniil A. Boiko, Robert MacKnight, Ben Kline & Gabe Gomes. Autonomous chemical research with large language models. Nature 624, 570–578 (2023).\n\nGabe Gomes, Assistant Professor, Chemical Engineering, Chemistry, at,.\n\nChemCrow is an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design.\n\nChemCrow = GPT-4 with powers for CHEMISTRY, AI Matej,\n\n‘Employed responsibly, ChemCrow not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.’\n\nAndres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, ChemCrow: Augmenting large-language models with chemistry tools, arXiv:2304.05376 [physics.chem-ph],\n\nGenerative artificial intelligence, by Wikipedia,",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 391,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "A super interesting paper from Microsoft. \nEmbedding generation is one of the most important for industrial application LLM tasks \nAuthors found a method to tune LLM for very high quality text embedding using only synthetic data and minimal data pipeline for the process ( very large data pipeline is another limiting step due to complexity, processing time , errors )",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 392,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”Throwing Ukraine under the bus — again — will not save the United States and its allies. Instead, it will tip the world into a sinkhole of global war on multiple fronts.”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 393,
        "person_name": "https://www.linkedin.com/in/michaelpcassidy",
        "text_description": "\"30 Years in 30 Minutes\" interviewed me :)",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 394,
        "person_name": "https://www.linkedin.com/company/nlplanet/",
        "text_description": "Merging LLMs leads to top Open LLM Leaderboard rankings 🔀\n\n👀 Model merging is a technique that combines two or more LLMs into a single model. It's a relatively new and experimental method to create new models for cheap (no GPU required).\n\n🤖 This tutorial shows how to use the mergekit library. More specifically, it will review four merge methods (SLERP, TIES, DARE, and Passthrough) and provide examples of configurations.\n\nSource:\n——————————————————\nWant to stay at the forefront of Generative AI developments? Follow NLPlanet for daily insights into the most relevant news, guides, and research! 🚀",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 395,
        "person_name": "https://www.linkedin.com/in/yann-lecun",
        "text_description": "Modems use an adaptive linear classifiers to turn analog signals into bit strings. It's called an \"adaptive equalizer\"\nIt is trained in two phases. The first one is supervised, the second one is self-supervised:\n1. In the initial phase, when the modem first connects to another modem, the transmitting modem sends a known pseudo-random bit training sequence. The receiving model trains its equalizer in supervised mode to turn the mushy analog signal into clean bits. It's basically a linear binary classifier trained with MSE loss. The random sequence is what produces this horrible screeching noise we hear when the modems first connect.\n2. the running phase where the equalizer adaptively \"tracks\" the changing characteristics of the transmission line using a self-supervised (or clustering) method in which a thresholded binary version of the weight sum is used as the equalizer's desired output. Basically, the equalizer trains itself to make its own output binary. If the properties of the line change slowly, this method can automatically adapt to the changes.\nThis adaptive equalizer algorithm was invented by Robert Lucky at Bell Labs in 1965. It enabled digital communication over phone lines above 2400 bauds to 9600 bauds.\nBob was the Executive Director of Bell Labs' division \"113\" in which I was hired in 1988 (he was 3 levels above me). He passed away last year.\n\nWikipedia:\nInterview of Bob:\nAdaptive equalizers:\nOriginal 1965 paper:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 396,
        "person_name": "https://www.linkedin.com/in/orenetzioni",
        "text_description": "This is so cool! Way to go Luis Ceze",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 397,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I’ve noticed that my posts that got more than 50000+ views and especially more than 100000 views on LinkedIn are posts that are about optimization of LLMs ( training , inference , supporting data pipelines , smaller models etc all type of optimization). I believe optimization as an enabler of much wider set of applications ( due to cost , latency , hardware requirements and other limits current models impose ) will be one of the top topics on 2024",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 398,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "I’ve noticed that my posts that got more than 50000+ views and especially more than 100000 views on LinkedIn are posts that are about optimization of LLMs ( training , inference , supporting data pipelines , smaller models etc all type of optimization). I believe optimization as an enabler of much wider set of applications ( due to cost , latency , hardware requirements and other limits current models impose ) will be one of the top topics on 2024",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 399,
        "person_name": "https://www.linkedin.com/in/adammgrant",
        "text_description": "My recommended new books to launch 2024. For a sneak preview: https://lnkd.in/edzrpA5t",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/edzrpA5t"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 400,
        "person_name": "https://www.linkedin.com/in/michaelpcassidy",
        "text_description": "\"30 Years in 30 Minutes\" interviewed me :)",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 401,
        "person_name": "https://www.linkedin.com/in/softwaredoug",
        "text_description": "Hey check out what the Search Relevance team at Reddit, Inc. has been up to!",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 402,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "A super interesting paper from Microsoft. \nEmbedding generation is one of the most important for industrial application LLM tasks \nAuthors found a method to tune LLM for very high quality text embedding using only synthetic data and minimal data pipeline for the process ( very large data pipeline is another limiting step due to complexity, processing time , errors )",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 403,
        "person_name": "https://www.linkedin.com/in/reachsumit",
        "text_description": "I just published Vol. 33 of \"Top Information Retrieval Papers of the Week\" on Substack.\n\nMy Substack newsletter features the 7-10 most notable research papers on information retrieval (including recommender systems, search & ranking, etc.) from each week, with a brief summary, and links to the paper/codebase.\n\nThis week’s newsletter highlights the following research work:\n📚 A Non-Contrastive Learning Approach for Aligned and Compact Representations in Recommendation, from Visa Research\n📚 Leveraging LLMs to Generate Synthetic Data for Achieving State-of-the-Art Text Embeddings, from Microsoft\n📚 Improving the User Experience of SKU Lookup in CRMs via Trie Suggestions and Vector Embeddings, from Microsoft\n📚 An RL-based Method for Revenue Optimization in Multi-Stage Recommenders with Limited Computation Resources, from Meituan\n📚 User Purchase Prediction and Allocation in FinTech Mutual Fund Promotions, from Tencent\n📚 Investigating the Limits of Sparse Attention for Efficient Cross-Encoders, from FSU\n📚 Mitigating False Negatives in Dense Passage Retrieval via Confidence Regularization, from NJU\n📚 Leveraging Multiple Pre-trained Recommenders via Joint Knowledge Distillation, from RUC/Tencent\n📚 A Survey on Causal Inference for Recommender Systems, from Tsinghua University\n📚 An I/O-Efficient Framework for Vector Similarity Search on Data Segments, from Zilliz",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 404,
        "person_name": "https://www.linkedin.com/in/sebastianraschka",
        "text_description": "Thanks, everyone, for all the support and positive words for my \"Build a Large Language Model (from Scratch)\" book! The next chapter on \"coding self-attention, multi-head attention, and causal self-attention from scratch\" is being finalized and will be in the MEAP in a few weeks!\n\nFor a sneak peek, you can find the code (along with short notes here):\n\nIn a nutshell, it will cover and discuss:\n\n- The problem with modeling long sequences\n- Capturing data dependencies with attention mechanisms\n- Attending to different parts of the input with self-attention\n- Implementing self-attention with trainable weights\n- Hiding future words with causal self-attention\n- Extending single-head attention to multi-head attention",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 405,
        "person_name": "https://www.linkedin.com/in/sunwalter",
        "text_description": "Generative AI is changing the way we all work. At the \nhashtag\n#wef in San Francisco in November, we had some great discussions on this topic and the economic impact to businesses and governments, including us at SAP.\n\nTogether withandand with collaboration from the\n, we are happy to share a writeup of our thoughts as published on thesite:",
        "time": "1mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 406,
        "person_name": "https://www.linkedin.com/in/deepakparamanand",
        "text_description": "My predictions for 2024\n\n1. LMMs (Large Multimodal Models) are going to take off! These models allow to combine text, speech, audio and video into one seamless experience. GPT4v and the likes will see more features come through and the open source world will come up with great alternatives. There are no great courses on this topic, stay tuned as I request some smart people to spare their time to this. In the meanwhile feast on this gem bywho explains the state of LMMs so well\n\n2. LLMs will continue to get better as they are still in the exploit phase where throwing more infrastructure is still showing benefits even controlling for data and algorithm.\n\n3. As the openly available data to train LLMs dries up, LLMs are going to need data from Enterprises. Yes, those large slow moving monliths that have been traditionally condemned for being slow are going to turn heroes for taking a measured approach to Gen AI. Expect either foundtational models to be pre trained on Enterprise data or fine tuned using Enterprise data.\n\n4. Consequently, be ready for data governance, data security functions to move from a traditionally cost function to revenue generation function.\n\n5. More and more of the non GenAI will be replaced by GenAI. Andrew Ng gave a one line code demonstration of how chatgpt can predict the sentiment of a text in seconds which would have taken months for a data scientist to realise. See here:\n\n6. Small Language Models like those shipped by Google called Google Nano are going to be adopted more and more to delight us. Expect smaller models to be available as apps to download on both mobiles and desktops.\n\n7. GenAI technologies have to answer the question 'what about paying content creators for using their data to build your GenAI'. I thought that the NY Times case was going to go entirely in their favor, but I've read that it may not be that easy after all.\n\n8. GenAI will have to also answer up to disinformation and deep fakes! Yes, the companies that create the AI that creates content cant detect if the content was indeed AI generated! How is that for irony? Deep fake detection is going to be a multibillion dollar industry.\n\n9. GenAI + PM will be a thing. I use GenAI myself in my PM role. I'm also helping an educational institution include GenAI to create and curate content along this topic.\n\nMy hope for 2024\n\nis that AI is used to solve more urgent problems such as climate change and judicial reform.and the team andare doing stellar work in reforming the justice system in India. I have collaborated with them in the past and will continue to do so in the future.\nI'm taking an active role in both these areas.\n\nJust FYI, amid the doom and gloom of slowdown and layoffs, the climate change industry is hiring in droves!",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 407,
        "person_name": "https://www.linkedin.com/in/lyndon-brown",
        "text_description": "As I spend time with 100s of tech leaders, it's clear that the pressure that current application monitoring strategies put on teams and budgets is becoming unsustainable. \n\n2024 is the year that \"Engineering leaders will take a fresh look at their application monitoring strategies, opting for more powerful solutions, as team velocity slows and observability costs balloon.\" 🔮\n\nVery cool to have my take included in's 2024 predictions! 👏\n\n🚨 \"The increase in AI-generated code, 3rd-party dependencies, and transient containerized workloads will make it more difficult for teams to reason about and troubleshoot application issues. At the same time, cloud cost concerns will reduce the ability of teams to keep throwing resources at performance problems....\"\n\nBe sure to check out the full article to hear all the predictions.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 408,
        "person_name": "https://www.linkedin.com/in/xamatriain",
        "text_description": "You have probably heard recently that Direct Preference Optimization (DPO) is taking over RLHF as the preferred method to align LLMs to human preferences (https://lnkd.in/gUT577VY). Well, that is \"old news\" now. The newest/coolest thing now is Kahneman Tversky optimization (KTO) (and yes, that is the Thinking fast and slow Kahneman!). Great work and research by, Dan Jurafsky and others aton Human Centered Loss Functions (HALOs):",
        "time": "1mo ",
        "url_links": [
            "https://lnkd.in/gUT577VY)."
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 409,
        "person_name": "https://www.linkedin.com/in/adammgrant",
        "text_description": "When leaders fail to explain why they rejected suggestions, we stop making them.\n\nGood explanations do more than give reasons. They express respect.\n\nWe don't have to get our way to appreciate having a say. Showing care makes us feel valued—and motivates us to speak up again.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 410,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "How does Uber predict ride ETAs? ETAs are used to compute fares, so it is critical to be quite accurate. Interestingly enough, they used to have an XGBoost model predicting ETA, but they built a Transformer model that outperformed it.\n\nThe ETA is provided to the user and is used to compute the ride's fares. The process goes as follows:\n\n- The user requests an ETA from the mobile application, and it is sent to the Uber service\n- The Uber service calls a routing engine to provide a rough estimate on the ETA.\n- Uber uses a Deep Learning model that refines the routing engine estimate. The routing engine computes an estimate of Y, and the Deep Learning model computes the residual R of Y such that Y + R is a better estimate. So, the target of the model is learning is\n\nR = Ground truth ETA - Y\n\n- The model's predictions are refined by considering the ride type (delivery trips vs ride trips, long vs short trips, pickup vs drop-off trips).\n- The prediction is served to the user in a few milliseconds.\n\nThink of the routing engine as Uber's version of Google Maps. Why can't the routing engine be enough? The routing engine just computes the time to get from point A to point B, but an Uber ride is a bit more than that. We need to take into account the different drivers available nearby for the ride. We need to account for the difficulty of parking at the pickup location or destination. Also, the routing engine is most likely not fully real-time, and the machine learning model will be able to use real-time information about the traffic.\n\nA routing engine works as follows:\n- The different segments of roads and intersections work as a graph. The intersections are the nodes, and the road segments are the edges connecting them.\n- Based on the current traffic information and the historical data, we produce a time estimate to go from one node to another neighboring node. Each road segment receives an edge weight that corresponds to those time estimates.\n- We can now use a graph algorithm to find the path that minimizes the total time to go from the origin to the destination.\n\nThe model consists of a Linear Transformer block followed by a feed-forward network. The feed-forward network is used as a calibration layer to help generalize all the different types of rides. Bringing the ride-type features closer to the target allows the model to give more attention to those features.\n\nThe continuous variables are bucketed into categorical variables, and each variable is associated with an embedding. The geospatial features are hashed at multiple resolutions into multiple embeddings with a small hash size for memory efficiency.\n\nHere is the paper about DeeprETA:.\n\n\n--\n👉 Learn more Machine Learning on my website:\n--",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 411,
        "person_name": "https://www.linkedin.com/company/megagon-labs/",
        "text_description": "🌟 Happy New Year! ✨ \n\nAs we recently finished our planning for next year, we are increasingly focusing on how we can put LLMs into production and leverage them in research and product development, specifically within the human resources (HR) space. We plan to concentrate on exploiting LLMs with proprietary data and services in end-user-facing use cases. “Data-AI” integration and “Human-AI” collaboration are going to be key research themes for our lab. For example, we are keen to tackle the augmentation of LLMs with structured, semi-structured, and graph data sources. This focus also includes distillation and retrieval, as well as how to plan and reason about complex tasks, and how to find and query data sources in an enterprise setting. Given in production settings, there is a large variety of both tasks and data sources; as such, how to use them and integrate them becomes critical. We also plan to look into multi-agent orchestration frameworks to integrate all the above key components for production use. As for the end-user-facing use cases, the problems we are tackling are explaining, rationalizing, annotating, verifying, and fact-checking output, not only from LLMs generation but also in multi-agent scenarios.\n\nStay tuned for cool demos, interesting papers, and open-source projects on these topics from us!",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 412,
        "person_name": "https://www.linkedin.com/in/farahabdallah",
        "text_description": "🌟 Wishing everyone a transformative 2024! This year, I'm committed to:\nGrowing and Taking Risks: Embracing new challenges, both personally and professionally.\nLeading with Empathy: Fostering a workplace where every voice is valued and differences are celebrated.\nBalancing Humanity and Technology: Ensuring technology enhances our work, without overwhelming our well-being.\nBest wishes to you all !",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 413,
        "person_name": "https://www.linkedin.com/in/bjarnestroustrup",
        "text_description": "A talk I gave just got posted:\n\nA talk in honor of Sorin Istrail: C++: Evolving a Useful Language at the Sorinfest at Brown University.With the exception of mine, the talks are about biology (especially genomics and computational biology), including John vonNeumann Distinguished Lectures.My talk is a lightning tour of key ideas behind C++ (14 minute plus Q&A).\n\nHappy New Year.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 414,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "We are hiring a Product Manager for Search and Discover AI\nThis person will partner with our remarkable Discover AI team to define discovery products helping our customers find the best places to live and discover Zillow services to help them in their customer journey.\n\nPlease, apply at myworkdayjobs (the link is below)",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 415,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "We are hiring a Product Manager for Search and Discover AI\nThis person will partner with our remarkable Discover AI team to define discovery products helping our customers find the best places to live and discover Zillow services to help them in their customer journey.\n\nPlease, apply at myworkdayjobs (the link is below)",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 416,
        "person_name": "https://www.linkedin.com/in/sidjreddy",
        "text_description": "The concept of using spherical linear interpolation (SLERP) to combine the weights of different models is quite interesting and appears to be a growing trend in the field. Maxime Labonne's use of the Marcoroni-7B-v3 and Mistral-7B-Merge-14-v.0.1 models, which includes his own NeuralHermes-2.5 model, to create a new 7B parameter LLM is impressive. While this method is still experimental and may have issues like leaderboard hacking, it's exciting to see such experiments in the LLM space. The fact that this doesn't require a GPU and can be done using a Google Colab notebook makes it accessible to a wider range of people interested in AI and machine learning.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 417,
        "person_name": "https://www.linkedin.com/in/jeff-cutler-85639920",
        "text_description": "Zillow Home Loans is looking for talented Purchase Loan Officers to join our team in Irvine, CA, Overland Park, KS, and Seattle, WA. With a vast network of real estate agents and high-intent buyer leads, we're dedicated to delivering exceptional experiences to our North Star. Ready to take the next step in your career? Apply now and become part of our world-class brand.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 418,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "“ The tools of artificial intelligence — neural networks in particular — have been good to physicists. For years, this technology has helped researchers reconstruct particle trajectories in accelerator experiments, search for evidence of new particles, and detect gravitational waves and exoplanets. While AI tools can clearly do a lot for physicists, the question now, according to Max Tegmark, a physicist at the Massachusetts Institute of Technology, is: “Can we give anything back?”\nTegmark believes that his physicist peers can make significant contributions to the science of AI, and he has made this his top research priority. One way physicists could help advance AI technology, he said, would be to replace the “black box” algorithms of neural networks, whose workings are largely inscrutable, with well-understood equations of physical processes.”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 419,
        "person_name": "https://www.linkedin.com/in/imranchaudhri",
        "text_description": "Using advanced \nhashtag\n#openAI model Question and answer responses as training material with weighted Rewards seems to be working for fine tuning \nhashtag\n#openchat.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 420,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "”Most of the analysts Eurasianet spoke to see Armenia exiting the CSTO as a logical possible outcome of the current strained relations between Armenia and Russia.”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 421,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "My favorite part of Machine Learning is Machine Learning System Design! How do we go from the idea of a product to the different technical requirements that lead to viable machine-learning solutions? I often do it for fun: \"How would I design this or that?\".\n\nWhen I interviewed at Meta, I went through their ML system design interview, and I really enjoyed it. I think it is the most efficient to assess a candidate for ML. Probing someone's ability to reframe a business problem into a Machine Learning solution. They asked me \"how to design the Facebook friends suggestion feature.\"\n\nWhen I construct a design for an ML solution, I tend to always follow the same playbook:\n\n- What is the Machine Learning problem?\n- What are the business metrics?\n- What are the online metrics?\n- What are the architectural components?\n- How do we get the training data?\n- What are the offline metrics?\n- What are the features?\n- What is the model?\n\nThe idea is to establish multiple potential options that we can explore further down the line. A junior ML engineer would start to think about the ML model to design a solution, whereas a senior ML engineer would think about everything but the specific algorithm that will be used for inference. The model is really the least important piece when it comes to building a viable ML solution.\n\n\n--\n👉 Learn more Machine Learning on my website:\n--",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 422,
        "person_name": "https://www.linkedin.com/in/barbara-plank-5045a4b",
        "text_description": "Looking forward to the DDSA conference in Denmark in February 2024",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 423,
        "person_name": "https://www.linkedin.com/in/gauravchak",
        "text_description": "All of us building \nhashtag\n#recommendersystems usually minimize cross-entropy of the estimator and assume that ranking by this predicted estimate of the label will lead to a higher occurrence of it. For instance, podcast \nhashtag\nmight predict probability of listening and then hope that when podcasts ranked by this p(Listen) are shown to users, users will listen more. But is that true? Does the math work out?\n\nIn this post, we will derive the gradient step of minimizing cross entropy and maximizing reward under REINFORCE ( policy gradient\n) and show that it is not the same. That maximizing reward could lead to a different outcome.\n\nIf you have insight into this, or feel that I have made a mistake somewhere I would be happy to learn and stand corrected.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 424,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "You should learn to deploy your Machine Learning models! The way to deploy is dictated by the business requirements. You should not start any ML development before you know how you are going to deploy the resulting model. There are four main ways to deploy ML models.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 425,
        "person_name": "https://www.linkedin.com/in/mahaniok",
        "text_description": "2023 in numbers at Geek Ventures:\n- 34 investments made\n- 19 new companies\n- 14 different countries of origin of the founders\n- 1 exit\n- 2 new team members\n- 3 SPVs for LPs\n- $23M fund closed with 70 LPs\n- 7 offline events for founders in 4 cities\n- 7 online events",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 426,
        "person_name": "https://www.linkedin.com/in/ab-abhishek-bhowmick",
        "text_description": "Big news, I am thrilled to announce that Samooha by Snowflake has entered into a definitive agreement to be acquired by Snowflake! I am excited about continuing our mission of simplifying secure data collaboration with Snowflake. I’m thankful for my partnership with my co-founder,, whose vision, drive, and passion have been invaluable. She's more than just a co-founder - she's a trusted confidant and a true companion. Here's to our unstoppable partnership!\n\nI am absolutely looking forward to our next chapter as we continue transforming secure data collaboration for enterprises with security, privacy, and ease-of-use as foundational principles.\n\nThis wouldn’t have been possible without the partnership and unwavering support of.\n\nThis milestone is also a testament to the hard work of our incredible team - your passion, dedication, and creativity have been the fuel for our growth. Keep being the amazing humans that you are. This is just the beginning of our journey!\n\nSnowflake Blog -\nSamooha Blog -\nAxios-\nTechCrunch-",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 427,
        "person_name": "https://www.linkedin.com/in/eliodamaggio",
        "text_description": "I’m happy to share that I’m starting a new position as Head of Product Management - Amazon Q Code Transformation at Amazon Web Services (AWS)!",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 428,
        "person_name": "https://www.linkedin.com/in/dpatil",
        "text_description": "We've been working on something new for you. Hope you'll check it out to learn from the OGs in data science like Todd Park, Jonathan G., danah boyd, Monica Rogati, Ben Lorica 罗瑞卡, and Roger Magoulas",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 429,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "We are hiring a Product Manager for Search and Discover AI\nThis person will partner with our remarkable Discover AI team to define discovery products helping our customers find the best places to live and discover Zillow services to help them in their customer journey.\n\nPlease, apply at myworkdayjobs (the link is below)",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 430,
        "person_name": "https://www.linkedin.com/in/mehtabhairav",
        "text_description": "One of the biggest challenges for practitioners using Pytorch has been figuring out why things run out of memory. Some amazing new tools in Pytorch 2.1 give insight into memory usage. It can even be used to classify the usage into familiar buckets (e.g., activations, gradients).",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 431,
        "person_name": "https://www.linkedin.com/company/huggingface/",
        "text_description": "New multimodal chatbots are now available: LLaVa and BakLLaVa. See below for more info:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 432,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "an interesting opinion by Matt Welsh. (former CS Professor at Harvard, principal engineer at Google and Apple, founder of Fixie)\nvery interesting examples to illustrate his thoughts",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 433,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "an interesting opinion by Matt Welsh. (former CS Professor at Harvard, principal engineer at Google and Apple, founder of Fixie)\nvery interesting examples to illustrate his thoughts",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 434,
        "person_name": "https://www.linkedin.com/in/jyri-raitasalo-4b96bb96",
        "text_description": "“The Ukrainian advantage in electronic warfare seems to be holding… The number of Ukrainian drone attacks have increased. A lot. The number of Russian drone attacks have decreased by the same amount.”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 435,
        "person_name": "https://www.linkedin.com/in/bryanperozzi",
        "text_description": "By request, here are the slides from our \nhashtag\n#neurips2023 presentation yesterday! We really enjoyed the opportunity to present the different aspects of the work being done by our group at the intersection of Graph Learning and Artificial Intelligence. The future here is exciting!\n\n(Too many slides? Don't worry, here are some shortcuts to work discussed in detail during the talk:)\n\n➡ Talk like a graph: Encoding graphs for Large Language Models (preprint)\n📩\n\n➡ TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs (NeurIPS'23)\n📩\n\n➡ Learning Large Graph Property Prediction via Graph Segment Training (NeurIPS'23)\n📩\n\n➡ HUGE: Huge Unsupervised Graph Embeddings with TPUs (KDD'23)\n📩\n\n➡ Unsupervised Embedding Quality Evaluation (TAG-ML'23)\n📩\n\nFeaturing work from,,,,,,,,,,,, and many more collaborators!",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 436,
        "person_name": "https://www.linkedin.com/in/satyanadella",
        "text_description": "From Llama 2 Models as Service and GPT-4 Turbo with Vision, to new open-source models, we’re bringing the most comprehensive selection of cutting-edge AI models to Azure AI, as we work to ensure our customers have more choice and flexibility to meet their specific needs.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 437,
        "person_name": "https://www.linkedin.com/in/thom-wolf",
        "text_description": "As a non-CS person, what an honor to sit among the authors of these 4 awarded papers at NeurIPS 2023 (out of 13k papers submitted 🤯)\n\nI was only an enabler, all props should go to the amazing(starting soon grad school...) as well asand Colin Raffel\n\nTopic of interest: as we're reaching the limits of the web/all-human-written-content in LLMs, should we start repeating data in our trainings?\n\nShort TLDR: yes\n👍For ~4 epochs repeated data is ~as good as new data\n🎢Parameters and epochs need to be scaled together 📷Optimal allocation deviates from Chinchilla with more additional compute going to more epochs rather than parameters\n💻Doubling the size of a natural language dataset by adding code results in no degradation in performance\n\nAnd the best explanation of this paper will be found in Niklas thread here:\n\nLink to the awards:",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 438,
        "person_name": "https://www.linkedin.com/in/wang-chiew-tan-1b84751",
        "text_description": "I am grateful for the opportunity to contribute to the inaugural Singapore Conference AI for the Global Good (SCAI) together with a diverse group of brilliant minds and leaders. Looking forward to seeing Singapore's National AI Strategy 2.0 in action!",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 439,
        "person_name": "https://www.linkedin.com/in/clementdelangue",
        "text_description": "The same way all tech companies can write good code today, all AI companies will be able to train good models in the future, based on open-source. If OAI, Anthropic, Meta, Mistral, Stability, RunwayML, HF and dozens of others can do, you can do it too!\n\nThe challenge is taking a bet on AI, starting your company learning curve by using open-source and not falling back into the easy and short-term approach of outsourcing AI building to APIs.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 440,
        "person_name": "https://www.linkedin.com/in/adam-goldberg-b8569b6",
        "text_description": "“We present a new research direction for superalignment, together with promising initial results: can we leverage the generalization properties of deep learning to control strong models with weak supervisors?”",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 441,
        "person_name": "https://www.linkedin.com/in/charlesmartin14",
        "text_description": "Last call. If you are at NeurIPs this week I hope you can make it to our workshop on Heavy Tails tomorrow, Friday, Dec 15. I’ll be talking at 3:30 on the theory behind the weightwatcher project.\n\nThis talk is a culmination of nearly 10 years of work developing a new theory of learning that can make quantitative diagnostic predictions about real world, production quality DNNs. The first half will discuss published results on the HTSR theory of learning, and the second half will present new theoretical results based on statistical mechanics, random matrix theory, and correlated systems.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 442,
        "person_name": "https://www.linkedin.com/company/megagon-labs/",
        "text_description": "🤔 Factual knowledge in LLMs isn’t binary so why still use binary ranking methods to assess LLM knowledge? In this article, we introduce a novel framework based on information theory, utilizing entropy and KL-divergence for knowledge measurement.\n\nBy applying these metrics to tasks like factual alignment and hallucination detection, we showcase their potential to provide more accurate assessments of LLMs' factual knowledge.\n\nThe research paper will be presented at\nbyon Friday, December 15th at 6pm EST.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 443,
        "person_name": "https://www.linkedin.com/in/rajko-rad",
        "text_description": "Very excited to partner with Arthur Mensch, Guillaume Lample, Timothee Lacroix and the incredible team at Mistral AI !!! \n\n,and I wrote up some thoughts on the investment + open source AI here:",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 444,
        "person_name": "https://www.linkedin.com/in/drjimfan",
        "text_description": "Mistral, founded < 1 yr ago, closed a $400M Series A at $2B valuation. Deliverable: an open mixture-of-expert model comparable to LlaMA-2-70B and GPT-3.5 but much faster to run. This is execution at light speed! My notes from their official blog:\n\n- Inference speed is equivalent to a 12B dense model.\n- Context length: 32k tokens.\n- Each token is only processed by 2 experts out of 8 at every layer. But the 2 experts can be different ones at different layers, enabling more complex processing paths.\n- Significantly better than LlaMA-2 on multilingual: French, German, Spanish, and Italian. Mistral is a French startup after all!\n- The model is named \"Mixtral\". Clever!\n- The team submitted a pull request to the open-source vLLM project to help the community integrate better. Rad move!\n- There's a stronger \"Mistral-medium\" model that is not open for now but accessible via API endpoint.\n\nMixtral blog:\nAPI platform:",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 445,
        "person_name": "https://www.linkedin.com/in/grantingersoll",
        "text_description": "Quick update, as I'm excited to be joining Bookend.ai as their CTO. Bookend is a new \nhashtag\n#startup focused on two things I've long thought are needed in the space of \nhashtag\n#artificialintelligence, \nhashtag\n#nlp, \nhashtag\n#machinelearningand\n: 1) simplicity and 2) trust and safety. I learned my lesson on no. 1 the hard way atin the early days of's rise and I saw the second one up close and personal while at theas we worked hard to establish and bring to production\npreserving, secure machine learning approaches that benefited both editors and consumers.\n\nMore important than the technology, though, is the team I'm excited to be joining. I've long been a fan and friend ofand have quickly come to respect's leadership and strategic insights. Beyond the two founders, we've already hired a number of great engineers across both AI and infrastructure and a working MVP.\n\nIf you are a\nlooking to make your apps smarter through\n, but your boss doesn't want company or customer data leaking out, check out what we are up to ator feel free to reach out.",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 446,
        "person_name": "https://www.linkedin.com/in/levineben",
        "text_description": "🔥🔥🔥",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 447,
        "person_name": "https://www.linkedin.com/in/adammgrant",
        "text_description": "How to keep an open mind:\n\n1. Think like a scientist: treat your opinions as hypotheses and decisions as experiments\n\n2. Embrace confident humility: argue like you’re right, listen like you’re wrong\n\n3. Build a challenge network: seek out people who sharpen your reasoning",
        "time": "2mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 448,
        "person_name": "https://www.linkedin.com/in/sujitpal",
        "text_description": "I am presenting \"Building LTR models for search using LLMs\" ( https://lnkd.in/eVFgeXmV ) at the PyData Global\n2023 conference next week. More details about the presentation on my blog post. Hoping to connect and compare notes with fellow attendees and others!",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/eVFgeXmV"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 449,
        "person_name": "https://www.linkedin.com/in/lopatenko",
        "text_description": "Challenges and Applications of Large Language Models\n\nA paper published in the middle of the 2023 year\nA compendium of the challenged and open problems of Large Language Problems\nA very comprehensive survey of challenges at different stages of LLM training/inference/applications, various domains",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 450,
        "person_name": "https://www.linkedin.com/in/armand-ruiz",
        "text_description": "Top Open Source LLMs available for Commercial Use:\n\n𝟭/ 𝗟𝗹𝗮𝗺𝗮 𝟮\n\n- Creator: Meta\n- Parameters: 7B, 13B, and 70B\n- Link:\n\n𝟮/ 𝗙𝗮𝗹𝗰𝗼𝗻\n\n- Creators: Technology Innovation Institute in Abu Dhabi\n- Parameters: 7B and 40B\n- Link:\n\n𝟯/ 𝗗𝗼𝗹𝗹𝘆 𝟮.𝟬\n\n- Creators: Databricks\n- Parameters: 12B\n- Link:\n\n𝟰/ 𝗠𝗣𝗧\n\n- Creators: MosaicML (acquired by Databricks)\n- Parameters: 7B and 30B\n- Link:\n\n𝟱/ 𝗙𝗟𝗔𝗡-𝗧𝟱\n\n- Creators: Google\n- Parameters: 80M to 11B\n- Link:\n\n𝟲/ 𝗚𝗣𝗧-𝗡𝗲𝗼𝗫-𝟮𝟬𝗕\n\n- Creators: EleutherAI\n- Parameters: 20B\n- Link to test:\n\n𝟳/ 𝗢𝗣𝗧-𝟭𝟳𝟱𝗕 (𝗢𝗽𝗲𝗻 𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿 𝗺𝗼𝗱𝗲𝗹)\n\n- Creators: Meta\n- Parameters: 175B\n- Link:\n\n𝟴/ 𝗕𝗟𝗢𝗢𝗠\n\n- Creators: BigScience\n- Parameters: 176B\n- Link:\n\n9/ 𝐁𝐚𝐢𝐜𝐡𝐮𝐚𝐧-13𝐁\n\n- Creators: Baichuan\n- Parameters: 13B\n- Link:\n\n𝟭𝟬/ 𝗕𝗘𝗥𝗧\n\n- Creators: Google\n- Parameters: 340M\n- Link:\n\n𝟭𝟭/ 𝗩𝗶𝗰𝘂𝗻𝗮\n\n- Creators: LMSYS\n- Parameters: 7B, 13B and 33B\n- Link:\n\n𝟭𝟮/ 𝗠𝗶𝘀𝘁𝗿𝗮𝗹\n\n- Creators: Mistral AI\n- Parameters: 7B\n- Link:\n\n____\n\nIf you like this content, please repost it ♻️ and follow me,, for more similar posts.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 451,
        "person_name": "https://www.linkedin.com/in/jeffboudier",
        "text_description": "Breaking: we got 1,200 tokens per second for Llama 2 7B on H100! 🔥🔥🔥\n\nCheck out our new project optimum-NVIDIA with fp8 inference support and first benchmarks for LLM acceleration:\n\nWith optimum-NVIDIA, accelerate your LLM inference with 1 line change:\n🏎 Get up to 28X speedup on H100, L40S, 4090\n⛳️ Enable FP8 with a simple flag\n🔋 Powered by TensorRT-LLM!\n\nExcited about this new collaboration withto accelerate models, we're just getting started! 🚀",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 452,
        "person_name": "https://www.linkedin.com/in/jordirib1",
        "text_description": "Have you ever been frustrated trying to find relevant web results for complex search queries? Today we’re introducing Deep Search, a new feature in Microsoft Bing which leverages GPT-4 to expand your query into many query variations and multiple probable intents, so that you can find highly relevant results if you just give Bing some extra time. We are seeing large relevance gains in hard query sets. Learn more:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 453,
        "person_name": "https://www.linkedin.com/in/debapriyabasu",
        "text_description": "What a year this has been for Bing Monetization - from delivering brand new innovations like monetizing Bing chat to landing and acing revenue goals - a very big thank you to our awesome team and fantastic colleagues.\n\nOnwards and upwards with,, Janani Venkataramani,, Maged,,,,,,,, Hank,, Justin,, Gary,,,,,, Eugene,,and last but always the biggest support",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 454,
        "person_name": "https://www.linkedin.com/in/jamesfdean",
        "text_description": "HSBC's partnership with Google Cloud to develop and implement an Anti Money Laundering AI solution is yielding amazing results that identifies two to four times as much suspicious activity as the previous system, spots known money-laundering patterns, reduces the number of alerts by 60%1 all while reducing processing time.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 455,
        "person_name": "https://www.linkedin.com/in/jennyluciav",
        "text_description": "I’m happy to share that I’m starting a new position as Senior Solutions Architect - AIML Startups at Amazon Web Services (AWS)! Thanks to my incredible managers Paul Smith and Nicolas Tarducci for their support since I joined the team and of course thanks to all my colleagues for their patience, trust and help. I'm happy to be part of an amazing and high skilled team 🚀",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 456,
        "person_name": "https://www.linkedin.com/in/michaelpcassidy",
        "text_description": "13,000 satellites will be built and launched over 5 years (including this year). Featured in Approach Venture's November newsletter released today. Great chart and data by Quilty Space! Wow!!! :) :)",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 457,
        "person_name": "https://www.linkedin.com/in/marcomontali",
        "text_description": "Today I talked about \nhashtag\n#artificialintelligence and \nhashtag\n#processmining at the ICT circle (with participants from Bologna, Bolzano, Schio, Trento - organized here in Bolzano by \nhashtag\n#SIAG). \n\nI invited to shift the focus from artificial general intelligence to integrative artificial intelligence, pointing out that by integrating different AI techniques we can support humans in the execution and understanding of their processes.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 458,
        "person_name": "https://www.linkedin.com/in/andrew-iain-jardine",
        "text_description": "🚨New benchmark by Hugging Face and Meta reveals Humans still actually better than \nhashtag\n#gpt4 😂.......See the leaderboard 👇\n\nAs development of AI systems continue, evaluation of those systems will be a barrier to practical adoption. Our existing\nbenchmarks often don't reflect real-world uses and suffer from contamination, diminishing their reliability.\n\nGAIA (General AI Assistants) a new benchmark aims to address these problems.\n\n𝐆𝐀𝐈𝐀:\n✨ 466 questions + images/files/spreadsheets for real-world tasks\n✨ Questions focus on finding and transforming information from sources to determine answers\n✨ Measures fundamental abilities not specialized knowledge\n✨ 3 levels of difficulty L1 - L3, requiring more steps or use of tools\n\n𝐈𝐧𝐬𝐢𝐠𝐡𝐭𝐬:\n🥇 Humans outperform AI systems on GAIA with a score of 92%\n😩 GPT4 + plugins achieved only a score of only 15%\n☢️ Manually curated Q&A pairs + unique multi-modal dataset reduce risk of data contamination\n\nPaper 👉\nLeaderboard 👉",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 459,
        "person_name": "https://www.linkedin.com/in/oleksiikuchaiev",
        "text_description": "Today we release SteerLM-aligned version of llama2-70B model, llama2-70B-SteerLM-Chat. This model gets 7.54 on MT-Bench. Important aspect of this model is that it is NOT using outputs of stronger models (e.g. ChatGPT) during alignment which allowed us to keep llama2 license.\nTry it right now on NGC catalog (no install or login necessary):\nAnd here is this model onmodel hub\nAlso, many thanks to Meta for releasing Llama2 to the community!",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 460,
        "person_name": "https://www.linkedin.com/in/bilogorskiy",
        "text_description": "Collecting $95+ million and managing it transparently is neither simple nor easy. \n\nWe could not have done it without ourtech team, led by Rodion Yaryyand our unique tech stack with Salesforce CRM at the core.\n\nAs our charity was founded mostly by engineers, we knew from the start to automate as much as possible. Partnering with Noltic, we deployed a customized end-to-end grant management platform and it enabled us to sustainably manage massive growth:\n\n- 205k donation transactions processed\n- 24 hour payment processing turnaround time\n- 5k grant applications processed\n- 21 million kg of humanitarian aid delivered\n- 4.5 million people helped!\n\nRead our Nova Ukraine story on, as we are one of the featured customers, along with Coca Cola, General Electric and L'Oréal:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 461,
        "person_name": "https://www.linkedin.com/in/bjarnestroustrup",
        "text_description": "Nice summary of my CppCon'23 keynote:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 462,
        "person_name": "https://www.linkedin.com/in/mdelprete",
        "text_description": "I’m really excited — and fairly nervous — to announce my biggest NEW PROJECT since I started teaching in 2018. It's called the Real Estate Innovation Hub. Let me explain…\n\nI LOVE working with people in the industry. I enjoy going to conferences, seeing old friends, and learning new things. But afterwards (and during) I feel exhausted. I’ve thought a few times, “wouldn’t it be fun if we weren’t just here for a conference, but we all worked together in the same building?”\n\nSo…that’s what I’m doing. I want to create a UNIQUE experience that combines the learning of a conference, the hands-on nature of a workshop, and the collaboration of an office. And I’m doing that by bringing together a small group of up to 12 real estate leaders and experts for three days of trend exploration, idea acceleration, and connection — all here in Boulder, Colorado.\n\nIt’s all about kinetic and potential. My goal is to have a thoughtful balance of interactive sessions, engaging connections, AND creative downtime to maximize learning that lasts.\n\nThe long-term idea is to hold these events on a regular basis, but to start, I’m trying one in January, February, and March. The first is January 30–February 1, 2024.\n\nSome of the invited guests joining me in January are,,,, and.\n\nApplications for the January event are due by December 15th. You can find a full description and draft agenda here:\n\nAnd the application form here:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 463,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "It has been quite a journey to arrive at a ChatGPT model! It took some time before we thought about modeling language as a probabilistic generative process. NLP studies the interactions between computers and human language, and it is as old as computers themselves.\n\nWarren Weaver was the first to suggest an algorithmic approach to machine translation (MT) in 1949, and this led to the Georgetown experiment, the first computer application to MT, in 1955. In 1957, Chomsky established the first grammar theory. ELIZA (1964) and SHRDLU (1968) can be considered to be the first natural-language understanding computer programs.\n\nThe 60s and early 70s marked the era of grammar theories. During the 70s, the concept of conceptual ontologies became quite fashionable. Conceptual ontologies are similar to knowledge graphs, where concepts are linked to each other by how they are associated. The famous ones are MARGIE (1975), TaleSpin (1976), QUALM (1977), SAM (1978), PAM (1978), Politics (1979) and Plot Units (1981).\n\nThe 80s showed a great period of success for symbolic methods. In 1983, Charniak proposed Passing Markers, a mechanism for resolving ambiguities in language comprehension by indicating the relationship between adjacent words. In 1986, Riesbeck and Martin proposed Uniform Parsing, a new approach to natural language processing that combines parsing and inferencing in a uniform framework for language learning. In 1987, Hirst proposed a new approach to resolving ambiguity: Semantic Interpretation.\n\nThe 90s saw the advent of statistical models. It was the beginning of thinking about language as a probabilistic process. In 1989, Balh proposed a tree-based method to predict the next word in a sentence. IBM presented a series of models for statistical machine translation. In 1990 Chitrao and Grishman demonstrated the potential of statistical parsing techniques for processing messages and Brill et al introduced a method for automatically inducing a part-of-speech tagger by training on a large corpus of text. In 1991, Brown proposed a method for aligning sentences in parallel corpora for machine translation applications.\n\nIn 2003, Bengio proposed the first neural language model, a simple feed-forward model. In 2008, Collobert and Weston applied multi-task learning with ConvNet. In 2011, Hinton built a generative text model with Recurrent Neural Networks. In 2013, Mikolov introduced Word2Vec. In 2014, Sutskever suggested a model for sequence-to-sequence learning. In 2017, Vaswani gave us the Transformer architecture that led to a revolution in model performance. In 2018, Devlin presented BERT, which popularized Transformers. And in 2022, we finally got to experience ChatGPT, which completely changed the way the public perceived AI!\n\n\n\n--\n👉 Checkout my website to learn more about it:\n--",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 464,
        "person_name": "https://www.linkedin.com/company/aiaustria/",
        "text_description": "🔙 Throwback to the incredible AI Governance Forum in Graz! 🚀✨\n\nWhat an amazing journey it was at the heart of groundbreaking discussions and innovative spirit Friday last week at the AI Governance Forum in Graz! 🤖🌐 From AI governance frameworks to AI implementation and adoption, every conversation was a step forward in shaping the future of AI governance. 🌱💡\n\nHats off to the brilliant organizersat the, in collaboration withandand the lead team aroundand, for orchestrating such a memorable event! 👏🏽 You truly made this forum a hub of knowledge and collaboration. 🙌🏼\n\nWe learned\n... 🚈 how to accelerate businesses with generative AI, from().\n... 🤝 what AI can do for you as a co-pilot, from().\n... ⚙ about AI in quality management, from().\n... 🤖 how to dive into potential and challenges of AI governance with,,, Patrick Ratheiser and, moderated Markus Fallenböck.\n... 🌍and about AI governance and the current regulatory road, presented by our Jeannette Gorzala.\n\nLet's keep the momentum going and continue to build a responsible and inclusive future for AI! The AI Austria team thanks you for this great kick-off of our collaboration, we look very much forward to expand and intensify in the future 🌍💻\n\n🎉🔍🤝",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 465,
        "person_name": "https://www.linkedin.com/in/li-yin-ai",
        "text_description": "We all know that search has been forever changed by LLMs, but how exactly? This talk by Marc Najork provides the best answer:\n\n— Closed-book systems, where all knowledge is captured by LLMs such as ChatGPT and the Differentiable Search Index (DSI), which can cite sources., the primary author of DSI, is now a co-founder of, pushing the boundaries of superintelligent multimodal AI with $60M funding.\n\n— Open-book systems, where the LLM can access external memory, the well-known Retrieval-Augmented Generation (RAG) systems.\n\nLink to the slides:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 466,
        "person_name": "https://www.linkedin.com/in/gauravchak",
        "text_description": "Early ranking in large scale recommenders has helped to reduce costs, improve latency and improve user satisfaction. In this post we explain what it is, why is it needed, what is measure of success for a good early ranker and show a baseline implementation of it. In future posts, I will talk about an optimal setup for early ranking.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 467,
        "person_name": "https://www.linkedin.com/in/pramodith",
        "text_description": "A lot of new prompting techniques were proposed in the last month or so. Here’s a list of some of the ones that I came across:\n\n1. Chain of Note: Ask the LLM to generate notes of the relevant parts of the input text and then produce a response.\n\n2. Contrastive Chain of Thoughts: Using correct and wrong explanations in your few-shot examples.💭\n\n3. System 2 Attention: Ask the LLM to extract the relevant parts of the input text, ignoring any user-stated biases, irrelevant info, etc., in step 1. Step 2 uses the output of step 1 to answer the user’s query.🔍\n\n4. Thread of Thoughts: Useful in long contexts or RAG applications where multiple docs can be returned. The first stage asks the LLM to apply the chain of thought to moderately sized segments of the input, each segment is accompanied by an analysis generated by the LLM. The second stage uses the output of the first stage to produce an output.🧵\n\n5.Take a Deep Breath: Just add that phrase to your prompt 😅🌬️\n\n6. Emotional Prompts: Let LLMs know that you really need them to answer correctly😢\n\n7. Simulation Theory of Mind: Stage 1, take note of the perspective the LLM is supposed to assume, ask it what they know from that perspective. Stage 2 answer the question based on the perspective and the knowledge that comes with it.🤖\n\n8. Step Back Prompting: Ask the LLM to take a step back and generate some fundamental knowledge that can be used to answer the question and then provide the final answer.🔙\n\nSome common themes across these are that 2 stage prompts, where the first prompt refines the initially provided documents/text and the second prompt uses this refinement/additional info to produce the final answer.\n\nFound these techniques by following awesome creators like,,.\n\nIf you know of other prominent techniques that I’ve missed, drop a comment below, for everyone to learn! 💬",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 468,
        "person_name": "https://www.linkedin.com/in/ronnyk",
        "text_description": "Jakub Linowski asked if sequential experiments are useful.\nYes, but use group-sequential and on the negative tail:\n\n1. Group-sequential tests are better in practice. You don't need to evaluate after every user, but every group of a few hours or every day. There are procedures based on Lan and DeMets where you plug the design and you get the alpha thresholds.\n\n2. It's much more appropriate to abort early, as harm tends to be bigger due to bugs. Some 10% of experiments at Bing aborted during the first day. For the positive experiments, I would just run classical fixed-horizon (e.g., two weeks) for three reasons:\nA. Results are upward biased when you abort early. That's not important when you abort, but more important when you declare and share a success.\nB. I want the additional data to better understand segments. If you barely have power for some key metrics, you'll be under-powered for segments.\nC. Wins are rare, say 20% of experiments. Are you trying to save a few days on those 20% that were going to run for two weeks? The value of launching 3 days early is minimal relative to something that will be deployed \"forever.\" There is negligible cost to continue to run until the pre-determined completion.\n\nSee a more detailed description at",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 469,
        "person_name": "https://www.linkedin.com/in/barbara-plank-5045a4b",
        "text_description": "Come join us in Munich! We have several open positions at MaiNLP lab, CIS, LMU Munich: PhD, Postdoc, scientific programmer, IT technician. \n\nFor details please check out our job post:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 470,
        "person_name": "https://www.linkedin.com/in/toddunder",
        "text_description": "tl;dr: I’ve joined Open AI and am hiring SREs. \n\nI’m happy to announce that I’ve started a role at Open AI (makers of ChatGPT). I’ll be working on research platform reliability.  (For curious locals: I will be staying in Pittsburgh and working remotely; I’ll be in San Francisco often).\n\nAt Google I created the Machine Learning Site Reliability Engineering (ML SRE) organization. We founded it in 2016 (there was already a Cloud ML SRE team; we built one for internal services and then combined them). After a reorganization split those teams up, I went off to work on Capital Engineering (a really cool team with a fascinating mission—a tale for another time).  Recently, I really wanted to get back to more SRE work but also to move closer to the ML infrastructure, especially the training infrastructure.  Hence Open AI!\n\nThings I will comment on: Open AI is a fascinating place at a very interesting juncture of technology and society.  They are small, nimble, moving fast, and making some very interesting investments, both in research and in infrastructure. There’s a ton for me to learn and I’m excited to get started. They also seem like excellent people.\n\nThings I won’t comment on publicly in any detail: last weekend’s drama. I started on Wednesday of last week. I will say that this was a slightly more interesting onboarding than I have ever had at any job. The full story might require a beverage and a relaxed setting. The structure of Open AI is one of the things that attracted me. I reviewed the membership of the nonprofit board several weeks ago and understood its size and composition as a potential risk before joining. I was not prepared for the particular happenings on Friday and over the weekend, however.\n\nAnyway, what’s next: I will be building an SRE team for research (the applied side already has SREs working on the inference and API products—honestly, I still know almost nothing about how anything works, so assume that I might be mistaken about literally everything).  Open AI is based in San Francisco and we’ll probably be hiring mostly in SF and London or Dublin initially (possibility for remote positions in the Canada, Ireland, the UK, and the US for exceptional folks, which I assume is many people I know!).\n\nI’ve worked with a ton of amazing people over the years, many of whom have worked on large scale ML systems as long as I have. I’m now in a position to build a new team of ML training infrastructure at some interesting scale (even interesting for folks coming from Google, I dare say).  This is a team that will need to be involved in the infrastructure from the ground up to the model, with opportunities to work on hardware health of accelerators, job orchestration and execution, model dynamics, and of course a special focus on metrics and measurement.\n\nIf you or someone you know might be interested in considering taking a plunge like this, reach out and I’ll connect you with a recruiter who can help you evaluate the possibilities.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 471,
        "person_name": "https://www.linkedin.com/in/benjaminjsoto",
        "text_description": "Gen AI in Google Search goes global, more languages, more countries, more features, and improved context.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 472,
        "person_name": "https://www.linkedin.com/in/cobusgreyling",
        "text_description": "Considering recent studies on prompting approaches for LLMs, there are a number of reoccurring themes…\n\nThe one theme is that Emergent Abilities are not hidden or unpublished LLM capabilities which are just waiting to be discovered, but rather the phenomenon of Emergent Abilities can be attributed to new approaches of In-Context Learning which are being developed.\n\nAnother theme is the importance of In-Context Learning (ICL) for LLMs, which have been highlighted in numerous studies. To facilitate ICL, context is being delivered to the LLM via methods like RAG, Fine-Tuning, or both.\nRelated to ICL, is accurate demonstrations for the LLM to emulate which is presented at inference.\n\nThe need for tools or connectors are being recognised. Cohere refers to this as grounding, where supplementary data is retrieved by the LLM-based agent, from external data sources.\n\nIt is being recognised that stand-alone, isolated prompting techniques have vulnerabilities. These vulnerabilities include hallucination and error propagation in the case of CoT.\n\nThe study identifies the problem of incorrect intermediate steps creating an error during the reasoning steps, which are propagated further down the chain. And subsequently negatively impacts the final conclusion.\nThere is an increased focus on data, and a human-in-the-loop approach to data discovery, design and development.\n\nlastly, inspectability along the chain of LLM interaction is receiving attention and approaches of data delivery are divided between gradient approaches which is more opaque. And non-gradient approaches which is ideal for human inspection and discovery.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 473,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "When it comes to Deep Learning, I think nobody symbolizes the field better than Geoffrey Hinton, the father of Deep Learning. He even coined the term! Here are his biggest contributions to the field:\n\n- 1986 - He invents the Boltzmann machines\n- 1985 - He proposes a new learning algorithm for Boltzmann machines\n- 1986 - He is credited as one of the inventors of the Back-propagation algorithm\n- 1991 - He invents the Mixture of Experts\n- 2006 - He proposes an algorithm to train Deep Belief Nets. This is the article that led to the term \"Deep Learning\"\n- 2006 - He shows how to build Autoencoders with Neural Networks\n- 2008 - He invents t-SNE, a new technique for dimension reduction\n- 2009 - He presents an algorithm to train Deep Boltzmann machines\n- 2009 - Trains Restricted Boltzmann Machines and Deep Belief Networks with the CIFAR-10 dataset\n- 2010 - Shows the improved performance of Restricted Boltzmann Machines with ReLU\n- 2011 - Shows how to build a generative text model with Recurrent Neural Networks\n- 2012 - Invents RMSprop in a course lecture (!!!)\n- 2012 - Proposes the Feature Dropout technique to improve networks\n- 2012 - Suggests mini-batch gradient descent in a course lecture\n- 2012 - Deep Learning for speech recognition\n- 2014 - Revolutions computer vision capabilities with AlexNet (the most cited paper of his whole career)\n- 2014 - He proposes the Dropout technique to reduce overfitting\n- 2014 - the CIFAR 10 dataset is made available\n- 2015 - He invents the Distillation Network to reduce the size of models\n- 2016 - He invents the Layer Normation technique (used in every Transformer architecture\n- 2017- He proposes CapsNets, or Capsule networks aiming to overcome some limitations of CNNs, particularly in the area of understanding hierarchical relationships between objects and their parts within an image\n- 2022 - He presents a new alternative to the Back-propagation algorithm: the Forward-forward algorithm\n\nNow, the guy has 327 publications, so I couldn't capture everything here, but I believe this encapsulates his most impactful works. Considering the trend, it seems a lot more is going to come from him in the coming years!\n\n\n--\n👉 Checkout my website if you enjoy ML:\n--",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 474,
        "person_name": "https://www.linkedin.com/in/cfregly",
        "text_description": "Happy to announce the release of our newest book, Generative AI on AWS in full color!! This is the first OReilly book focused entirely on modern Generative AI including the latest large-language and multimodal models. The ebook is available right now; the print version starts shipping next week! Thanks to my wonderful co-authorsand- as well as the awesome folks at- for working extra hard to release this in time for our big reInvent conference next week!! Come meet us at the OReilly booth in Vegas on Thursday at 12:30pm PT.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 475,
        "person_name": "https://www.linkedin.com/in/barbara-plank-5045a4b",
        "text_description": "I'm excited to share that I was elected VP-elect for the ACL (Association for Computational Linguistics), the premier international society for NLP research.\n\nThis is an incredible honor, as it means to become president in 2026. I expect it will also be challenging at times. I will try to do my best in the four years to come.\n\nI'm looking forward to join the board in 2024 together with\n\nThanks everyone for supporting my candidacy",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 476,
        "person_name": "https://www.linkedin.com/company/zillow/",
        "text_description": "At Zillow, we value meeting top talent where they are. We had an incredible time at the AfroTech conference, and we're thrilled to share the highlights of our main event: The Housewarming with special house guest Jermaine Dupri!\n\nAfroTech's commitment to culture and community resonates deeply with us; join us in celebrating the power of community and our shared vision of making home a reality for all. Check out the video below to feel the energy and excitement! To stay connected and see where we’re going next, head toand join our Talent Network!",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 477,
        "person_name": "https://www.linkedin.com/in/damienbenveniste",
        "text_description": "When it comes to Deep Learning, I think nobody symbolizes the field better than Geoffrey Hinton, the father of Deep Learning. He even coined the term! Here are his biggest contributions to the field:\n\n- 1986 - He invents the Boltzmann machines\n- 1985 - He proposes a new learning algorithm for Boltzmann machines\n- 1986 - He is credited as one of the inventors of the Back-propagation algorithm\n- 1991 - He invents the Mixture of Experts\n- 2006 - He proposes an algorithm to train Deep Belief Nets. This is the article that led to the term \"Deep Learning\"\n- 2006 - He shows how to build Autoencoders with Neural Networks\n- 2008 - He invents t-SNE, a new technique for dimension reduction\n- 2009 - He presents an algorithm to train Deep Boltzmann machines\n- 2009 - Trains Restricted Boltzmann Machines and Deep Belief Networks with the CIFAR-10 dataset\n- 2010 - Shows the improved performance of Restricted Boltzmann Machines with ReLU\n- 2011 - Shows how to build a generative text model with Recurrent Neural Networks\n- 2012 - Invents RMSprop in a course lecture (!!!)\n- 2012 - Proposes the Feature Dropout technique to improve networks\n- 2012 - Suggests mini-batch gradient descent in a course lecture\n- 2012 - Deep Learning for speech recognition\n- 2014 - Revolutions computer vision capabilities with AlexNet (the most cited paper of his whole career)\n- 2014 - He proposes the Dropout technique to reduce overfitting\n- 2014 - the CIFAR 10 dataset is made available\n- 2015 - He invents the Distillation Network to reduce the size of models\n- 2016 - He invents the Layer Normation technique (used in every Transformer architecture\n- 2017- He proposes CapsNets, or Capsule networks aiming to overcome some limitations of CNNs, particularly in the area of understanding hierarchical relationships between objects and their parts within an image\n- 2022 - He presents a new alternative to the Back-propagation algorithm: the Forward-forward algorithm\n\nNow, the guy has 327 publications, so I couldn't capture everything here, but I believe this encapsulates his most impactful works. Considering the trend, it seems a lot more is going to come from him in the coming years!\n\n\n--\n👉 Let me help you become better at Machine Learning:\n--",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 478,
        "person_name": "https://www.linkedin.com/in/julia-kiseleva-24842710",
        "text_description": "Unlocking the utility of LLM-powered solutions for end users is crucial. We're excited to introduce a very first version of \nhashtag\n#agenteval — a framework built with \nhashtag\n#autogen to help you gauge the utility of your applications. We've outlined our initial thoughts and experiments in this direction. We'd love to hear what you think! It's important to do it together! It would be fantastic if you could try\nfor your application and share your insights with us.\n\nCheck out the details here:\n\n\n\n\nwithand amazing\nteam",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 479,
        "person_name": "https://www.linkedin.com/in/danielvanstrien",
        "text_description": "How can we use LLMs to create better Named Entity Recognition (NER) systems? \n\nMost LLMs don't perform that well at Named Entity Recognition tasks out of the box. For the cost of inference, you can often get better results using a model explicitly trained for Named entity recognition. These models will also give you much more predictable outputs and won't hallucinate.\n\nHowever, it seems a shame not to try to leverage LLMs in creating NER systems. Recently, various approaches have been proposed that leverage LLMs more specifically for NER tasks.\n\nUniversalNER from,,, et al. propose a recipe for targeted distilling. The broad goal of their paper is to distil the capacities of a large model into a much smaller model, in this case, one for NER.\n\nHow does this work?\n\nCreate instruction data for NER: Instructing tuning has become essential for distilling LLMs capabilities into smaller models. The first step to instruction tuning is to create a dataset for this. A key difference in the approach of this paper, however, is that they don't try to distil everything but rather focus on one capacity NER.\n\nThis data is constructed using sample data from the Pile corpus. The authors then used gpt-3.5-turbo-0301 to generate entity mentions and their associated types based on the sampled passages.\n\nInstruction tuning: While the data generated by an LLM could be used for 'traditional' supervised fine-tuning, the authors found conversational style tuning performed better than traditional NER-style tuning.\n\nEvaluate on a massive benchmark! The authors of the paper wanted to evaluate this approach across a large number of datasets. To do this they created the Universal NER Benchmark \"the largest NER benchmark to date\". I will write a full post about this dataset in the future since it's a super valuable contribution.\n\nThe paper includes an extensive evaluation of results but some key findings are:\n\n- in the zero-shot setting the UniNER-13B model outperforms, ChatGPT and VVicunna-7B.\n- when the model is further fine-tuned using human data the model's performance grows. This suggests that some domain-specific fine-tuning could work well without having to 'start from scratch'.\n- when fine-tuning UniNER-7B in a supervised setting the model \"achieves an average F1 of 84.78% on 20 datasets, surpassing both BERT-base and InstructUIE-11B by 4.69% and 3.62%, respectively\nthe 20 datasets\"\n\nYou can find the dataset and models on theHub 🤗\n\n- Project page:\n- Datasets:&\n- Model",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 480,
        "person_name": "https://www.linkedin.com/in/bendee983",
        "text_description": "We still have a lot to learn about how LLMs process natural language and how we can get them to provide more accurate answers.\nOPRO, an interesting technique developed by Google DeepMind, uses LLMs to optimize their own prompts, helping you find the best prompt-engineering tricks to improve the performance of your models.\nHere is everything you need to know about OPRO and how you can use it in your own applications.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 481,
        "person_name": "https://www.linkedin.com/in/emilychangbloomberg",
        "text_description": "I conducted a late-breaking interview with Microsoft CEO Satya Nadella today. My assessment: This may be the wildest story I have ever covered in my career. Even after listening to this interview, I can't figure out what Sam Altman may have done wrong to be fired from OpenAI and Satya shows no inkling of knowing anything untoward. Also, he definitely signals he wants Altman to be reinstated. Watch and tell me what you think! Who will be CEO of OpenAI tomorrow?",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 482,
        "person_name": "https://www.linkedin.com/in/novakovska",
        "text_description": "I’m happy to share that I’ve started a new position as Staff AI engineer, Generative AI Solutions.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 483,
        "person_name": "https://www.linkedin.com/in/kschutz",
        "text_description": "A small - but cool! - reference to Snorkel AI in Coatue's 115-page AI report released today, full report here: https://lnkd.in/eEjXEyGj",
        "time": "3mo ",
        "url_links": [
            "https://lnkd.in/eEjXEyGj"
        ],
        "url_texts": "\nThis link will take you to a page that's not on LinkedIn\nThis link will take you to a page that's not on LinkedIn\nBecause this is an external link, we're unable to verify it for safety.\nThis experience is optimized for Chrome, Edge, and Safari\n"
    },
    {
        "id": 484,
        "person_name": "https://www.linkedin.com/in/ashutoshjhaveri",
        "text_description": "Llama 2 is coming to Microsoft Azure AI Studio through Models as a Service. Together we hope to make it easier for developers to adopt these models and build new generative AI applications.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 485,
        "person_name": "https://www.linkedin.com/in/jaronwaldman",
        "text_description": "Our LLMs are now available on Azure in addition to AWS, GCP and Oracle Cloud. Cohere everywhere and anywhere you need it.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 486,
        "person_name": "https://www.linkedin.com/in/omarsar",
        "text_description": "An LLM doesn't know when it doesn't know. \n\nThis is a well-documented issue that leads to LLMs producing inaccurate responses, especially in scenarios where knowledge is lacking.\n\nIdeally, an LLM should respond with \"unknown\" when the answer is unattainable.\n\nThis issue is further amplified when a RAG system retrieves irrelevant data which leads to misguided responses.\n\nThis new work introduces Chain-of-Noting (CoN) to improve the robustness and reliability of retrieval-augmented language models in facing noisy, irrelevant documents and in handling unknown scenarios.\n\nCoN generates sequential reading notes for the retrieved documents, enabling an evaluation of their relevance to the given question and integrating this information to formulate the final answer.\n\nThe authors used ChatGPT to generate the CoN data used to train a Llama-2 7B model to incorporate the note-taking ability integral to CoN.\n\nCoN significantly outperforms standard retrieval-augmented language models and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.\n\nThis work shows a lot of potential in improving the accuracy of RAG systems. What I like about this paper is the combination of ideas like synthetic data, self-evaluation, retrievers, chain-of-thought, and more.\n\n\n\n---\nIf you are interested in keeping up with LLM research, I also provide technical summaries of the latest and most important AI research and developments here:",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 487,
        "person_name": "https://www.linkedin.com/in/arun-cowan",
        "text_description": "I am absolutely honoured to announce that I have been appointed to the Board of Manchester Tech Festival!\n\nOur mission has always been two fold at- to help advance the Tech4Good space through the utilisation of AI whilst further connecting Manchester to the global tech scene.\n\nBy being a part of this AMAZING team, we are able to to contribute locally as well as globally.\n\nHaving involvement in the events for the last few months, I can tell you that there is something so unique and special about the Manchester tech scene and I am over the moon to be part of this family.\n\nHuge shout out of course to,,,,,,,and the rest of the team and a huge thank you for the opportunity.\n\nI am so excited to see this journey unfold!",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 488,
        "person_name": "https://www.linkedin.com/in/juliakanter",
        "text_description": "Hard to believe it's been a year since we boomeranged back to Kansas City! 📍 \n\nIt seems only fitting to pay it forward after so many were welcoming to me in creating a soft landing to the KC tech community. If you've moved here in the last few years (either anew or as a boomerang) and are interested in creating connections with other fresh KC locals in tech, comment here or send me a DM!\n\nMore coming soon... 😏",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 489,
        "person_name": "https://www.linkedin.com/in/joshbroward",
        "text_description": "Today, I’m thrilled to announce my new role as CEO of Wisdom Partners, a leadership coaching and consulting firm. We help leaders evolve so they can help their organizations evolve so we can change the world together. If you have a big mission and you're on a growth trajectory, reach out for a free assessment. If you are CEO of a growing company with at least $5M in revenue, apply for our CEO Frontier Cohort.\n\nI want to change the world. I’ve dedicated my life to developing leaders and organizations who can make the world a better place - more justice, more health, more compassion, more peace, more abundance for everyone. When the metaphorical stars aligned to start Wisdom Partners with(the Scaling Guru) and(the Business Monk), I knew this is my second-half-of-life calling.\n\nWe are convinced that the path to maximum impact lies through cultivating an Ownership Culture in our organizations. This is when every individual is aligned to the mission, accountable for clear goals, and empowered to act autonomously within their area of responsibility. This results in deep personal agency and a dynamic synergy that catapults the organization and the individuals into enduring impact.\n\nIf this sounds like a movement you want to be part of, here are three ways you can connect.\n1. Follow Wisdom Partners on LinkedIn (for weekly updates).\n2. Get a free assessment to clarify your organization’s pathway to breakthrough.\n3. Apply for one of our CEO cohorts.",
        "time": "3mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 490,
        "person_name": "https://www.linkedin.com/in/omarsar",
        "text_description": "Retrieval APIs for Open Source LLMs\n\nBuilding a RAG application is fun but not cheap. \n\nYou really have to take into account cost, customization, ease of scaling, and the option to work with different LLMs (including open-source ones).\n\nAbacus AI recently announced its open-source retrieval APIs. It's probably the most comprehensive offering for building real-world RAG applications with custom LLMs. It's now better understood that RAG and fine-tuning can compliment each other so this is a promising solution that supports this.\n\nWith their comprehensive platform, you can build powerful apps to chat with your knowledge base within minutes using RAG and/or fine-tuned LLMs.\n\nTheir fine-tunes support a larger 32K context and are useful for specific enterprise use cases like Q/A and summarization. They report that their open-source APIs are 20x cheaper than GPT-4. This is important for cases where efficient cost and scaling are a priority.\n\nIt offers the ability:\n- to easily setup and scale RAG applications\n- pick any open-source LLM - Llama2, Giraffe, etc - to power your RAG applications\n- customize chunking, embedding, and retrieval strategies to suit your needs\n\nSign up here:",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 491,
        "person_name": "https://www.linkedin.com/in/omarsar",
        "text_description": "Hallucination in LLMs\n\nA comprehensive survey (50+ pages) on hallucination in LLMs.\n\nMost of the LLMs and prompting approaches we use today lead to some form of hallucination so it's becoming increasingly important to test for this. The first step is to be aware of and detect these issues and then be familiar with tactics to mitigate them. This paper is a great place to start.\n\nBookmark this one!\n\n\n\n---\nI also provide technical summaries of the latest and most important AI research and developments here:",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 492,
        "person_name": "https://www.linkedin.com/in/vasudev-lal-79bb336",
        "text_description": "Outstanding 2x performance leap on 175 Billion parameter GPT-3 training on Intel Gaudi2 with fp8",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 493,
        "person_name": "https://www.linkedin.com/in/orenetzioni",
        "text_description": "Impressive and practical progress on \nhashtag\n#greenai",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 494,
        "person_name": "https://www.linkedin.com/in/mohabdel",
        "text_description": "Effective distillation of the knowledge from white-box generative LLMs is still under-explored, but critical to deploying LLMs into production with limited latency budgets.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 495,
        "person_name": "https://www.linkedin.com/in/jenniferpetoskey",
        "text_description": "Join my team - Alex is an incredible leader and this is a great opportunity to help us build the foundational product at Zillow Home Loans!",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 496,
        "person_name": "https://www.linkedin.com/in/aboniasojasingarayar",
        "text_description": "👩‍💻 Multi-Vector Retriever for RAG on tables, text, and images\n \nIntegrating multimodal Large Language Models like GPT4-V, LLaVA, and Fuyu-8b with the Multi-Vector Retriever, especially when it comes to image-related queries. LLMs can acquire new information in two ways: weight updates (e.g., fine-tuning) and RAG (retrieval augmented generation), which passes relevant context to the LLM via prompt. RAG has particular promise for factual recall because it marries the reasoning capability of LLMs with the content of external data sources, which is particularly powerful for enterprise data.\n\n🛠️Techniques to Improve RAG\n\n🔸 Base case RAG: Top K retrieval on embedded document chunks for precise answers.\n🔸 Summary embedding: Retrieve document summaries for context and full doc for synthesis.\n🔸 Windowing: Top K retrieval on embedded chunks or sentences, expanding the context.\n🔸 Metadata filtering: Top K retrieval with chunks filtered by metadata.\n🔸 Fine-tune RAG embeddings: Tailor the embedding model to your data.\n🔸 2-stage RAG: Keyword search followed by semantic Top K retrieval for enhanced accuracy.\n\n📋 Multi-Modal data : 3 approaches to redefine image-related RAG queries\n\n1️⃣ Leveraging multimodal embeddings, such as CLIP, to combine image and text data, opening the door for similarity-based retrieval while seamlessly linking to images in a document store. The raw images and text chunks are then passed to a multimodal LLM for synthesis.\n2️⃣ Utilizing a multimodal LLM to generate text summaries from images, followed by embedding and retrieval for answer synthesis. Raw text chunks or tables are referenced from a document store, excluding images.\n3️⃣ Similar to Option 2, but with a focus on retrieving image summaries while maintaining references to the raw images. This approach is ideal for scenarios where multimodal embeddings are not feasible.\n\nLearn more 👇\n📌Multi-vector retriever :\n📌blog:\n\n📖 Cookbooks\n📌Semi-structured (tables + text) RAG :\n📌Multi-modal (text + tables + images) RAG :\n📌Private multi-modal (text + tables + images) RAG :",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 497,
        "person_name": "https://www.linkedin.com/in/vijay-k-narayanan-817b75",
        "text_description": "This is an interesting virtual conference on LLMOps happening this week organized by our friends at Fiddler AI. Feel free to attend if you're interested in learning more about LLMOps. https://lnkd.in/eMfwnwtM",
        "time": "4mo ",
        "url_links": [
            "https://lnkd.in/eMfwnwtM"
        ],
        "url_texts": "\nThe request is blocked.\nThe request is blocked.\n"
    },
    {
        "id": 498,
        "person_name": "https://www.linkedin.com/in/vazifedoost",
        "text_description": "It’s amazing how AI is revolutionizing healthcare. Utilizing voice technology is a valuable asset for insurance companies as well, streamlining their life insurance application processes and enhancing customer care, especially for long-term disability cases. Ensuring proper regulations for responsible and ethical AI use is the next important topic that comes to mind.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 499,
        "person_name": "https://www.linkedin.com/in/bradleyaberning",
        "text_description": "Zillow just reported solid financial results that meaningfully outperformed the broader real estate industry for the fourth consecutive quarter. We’ve made great progress rolling out and expanding our product offerings to more markets, and we’re looking to continue this into the new year.\n\nThank you to Zillow employees, investors, customers and other supporters for being part of our journey to simplify and streamline the real estate transaction.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 500,
        "person_name": "https://www.linkedin.com/in/andy-krause-7260ab17",
        "text_description": "We have a lot of data at Zillow, but occassionally we find that we are lacking key home attributes. Check out this work by Angela Burden on our newest approach to filling in that missing information:",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 501,
        "person_name": "https://www.linkedin.com/in/bendee983",
        "text_description": "Fine-tuning LLMs can be complicated and frustrating. Fortunately, Matt Shumer has created a useful tool, gpt-llm-trainer, which can generate data and fine-tune Llama 2 or GPT-3.5 Turbo with one instruction. \nThis is not the solution to all kinds of fine-tuning tasks. But it is certainly useful for cases where model distillation is helpful.\ngpt-llm-trainer uses GPT-4 to generate training examples and fine-tunes your model for you. Read all about gpt-llm-trainer on TechTalks.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 502,
        "person_name": "https://www.linkedin.com/in/behshad-behzadi-6978864",
        "text_description": "Google + Porsche Partnership = innovative and helpful in-vehicle experiences!\n\nhashtag",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 503,
        "person_name": "https://www.linkedin.com/company/zillow/",
        "text_description": "Zillow’s Q3 results exceeded our outlook for the quarter, with Residential revenue meaningfully outperforming the industry for the fourth consecutive quarter. We’ve been investing across our five growth pillars — touring, financing, seller solutions, integrating our services, and enhancing our partner network — as we build the housing super app, and our results denote great progress toward our vision.\n\nRead what CEOand CFOhad to say in their letter to shareholders about our Q3 results and progress toward building an integrated housing transaction experience:",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 504,
        "person_name": "https://www.linkedin.com/in/aveekkarmakar",
        "text_description": "This feels akin to dropout regularization in some ways, but for data and embeddings. The simplicity, result and the gains seem promising.",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 505,
        "person_name": "https://www.linkedin.com/in/parnazt",
        "text_description": "As Large Language Models (LLMs) are evolving and the technology is commoditizing, \nhashtag\n#assetmanagers are on the lookout for innovative methodologies that not only enhance accuracy and usability but also cut costs for production-ready solutions. 💼✨ I found the following article on leveraging parallel models a great approach for knowledge management use cases and other bespoke solutions for asset managers utilizing LLMs on proprietary data.\n\nKey Highlights:\n🔍 Llama 2:\noffers a broad spectrum of functionalities right out of the box, making them indispensable tools in knowledge management.\n\n🛠️ Custom Fine-Tuning: Enhancing performance on specific tasks is possible by fine-tuning these models with unique data sets, providing tailored results.a significant drawback of this classic fine-tuning approach is that the model undergoes irreversible modifications, possibly reducing its performance on the original task.\n\n💾 Memory and Compute Overhead: Running multiple, fine-tuned versions of LLMs, while beneficial, can lead to significant memory and compute overhead, a crucial factor for asset managers to consider.\n\n🤏 Low-Rank Adaptation (LoRA): LoRA uniquely optimizes LLMs by selectively adjusting a subset of crucial parameters, rather than the entire parameter space, ensuring efficient and cost-effective fine-tuning; it further acts as an adaptable plug-in, allowing for multiple independently trained LoRA models to be attached to the main LLM at runtime, catering to various downstream tasks.\n\n🔗 LoRAX Framework: Despite the technical complexities and potential costs associated with managing multiple LoRA models,'s\n, built atop Uber AI's low-code\n, simplifies this process. It allows for seamless integration and management of a base LLM and multiple\n, ensuring optimal performance and response generation based on user requests, even when adapter count surpasses server GPU memory capacity. This is made possible through a sophisticated caching system, facilitating smooth operation and minimal latency. Additionally, Predibase offers cloud services and a Python SDK for easy creation and integration of LoRA adapters, supporting various popular open-source models, thus providing a comprehensive and versatile solution for developers aiming to harness the benefits of fine-tuned LLMs.\n\n💰 Cost Efficiency at Scale: As LLM technologies become more commonplace, the ability to manage knowledge effectively while keeping costs low will become a key differentiator for asset managers. Leveraging LoRA and LoRAX provides a strategic advantage, ensuring robust performance without breaking the bank.\n\nRead more here by:",
        "time": "4mo ",
        "url_links": [],
        "url_texts": ""
    }
]