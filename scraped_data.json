[
    {
        "id": 0,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Amazon just released Amazon Titanüî•üî•üî•\n\nüöÄIntroducing Amazon Bedrock, a new service that makes Foundation Models (FMs) available from leading AI startups and Amazon via an API. Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, democratizing access for all builders.ü§ñ\n\nüí•Bedrock offers the ability to access a range of powerful FMs for text and images‚Äîincluding Amazon Titan FMs‚Äî through a scalable, reliable, and secure AWS managed service. Amazon Titan FMs are pretrained on large datasets, making them powerful, general-purpose models. Use them as is or privately customize them with your own data for a particular task without annotating large volumes of data.üíª\n\nüî•Amazon Titan models are built by leveraging Amazon‚Äôs decades of experience to make ML accessible to anyone who wants to use it. Think of it as a cloud-based and configurable alternative to OpenAI‚Äôs ChatGPT and DALL-E 2 aimed at businesses and developers.üíº\n\nüåüJoin the revolution and sign up to learn more about how you can use Amazon Bedrock to enhance your business with generative AI!\n\nüåü",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 1,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Worst Experience with GCP Vertex AI:\nI hade been setting up a project that uses OpenAI's API to generate embeddings on a data and store it in vector DB like Pinecone, FAISS, Google Match Engine. I choose Match Engine due to requirements from my client as the Vector DB. Little did I know that setting up Vector Index on Match Engine is so cumbersome as it requires you to setup a VPC, Peer it with Match Service, Create Endpoint on the Network, Deploy Index. Also all above steps should be done in same region. This is cumbersome then using Pinecone, FAISS etc but doable.\n\nNow for the query part they have mentioned in their sample notebook: That to query on index you need to create a new notebook that is on the network and region that you made index on such that request is generated from same network. Again a bit cumbersome but not the worse experience.\n\nAfter doing all above steps I still was getting connection timed out which they mentioned can happen if network/region of notebook does not match of index. After 4 days of straight debugging I tried to use google cloud run (A service that allows to run code and expose an API in a app like environment) service thinking that notebook might not be running in same network/region and cloud run should run in it. Voila it worked. After going through the docs a bit more I found this:\n\"Requests to your notebook from the JupyterLab interface may be routed through a different region than selected above depending on service availability.\"\n\nSummary: Google requires you to make a notebook in the same region as index to query index, but depending on availability it might be routed through another region making it impossible to use their service the way it is intended to use it.\n\nReference clause that tells NB to be created in same regions:",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 2,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Celebrate a woman who‚Äôs influenced your career and share how she‚Äôs helped you. #IWD2023 #EmbraceEquity\n\n‚úèÔ∏è Rija Farooqi For always supporting and trusting me with my career decisions",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 3,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Hello Connections,\nI am looking for individuals interested in getting their hands dirty with recent Large Language models, GPT, Bloom etc to build a pet project on knowledge enhanced Question Answering. Although it will be open-source project, but there will be loads of learning and an opportunity to build an online presence within a team. It will be a great opportunity for those especially who are looking to add some practical work to their profile for better job prospects in the field of AI.\nIf you are interested DM me and we can have a chat!!!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 4,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Had an amazing experience at London Information retrieval meetup at Bloomberg. Alessandro Benedetti gave an amazing talk on the working and building blocks of ChatGPT.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 5,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "If someone wants to get really good at system design for any domain including networking, backend, mobile, cloud, database do follow Alex Xu on LinkedIn. He posts amazing short articles on different system designs which are intuitive, quick read and very informative.\n#systemdesign",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 6,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "With the goal in my mind to make the web more accessible, I have designed a concept of an app Audle. It will help people with disabilities such as down syndrome and cerebral palsy to search through any video streaming platform through their voice command with special word matching ability.\n\nI have shared the first draft of mockups here and will soon be sharing a mid-fidelity prototype, video explanation, and a case study for the process of designing Audle.\n\nI am looking forward to getting feedback from seniors in the industry to make this concept more accessible and user-friendly.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 7,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "I‚Äôm happy to share that I‚Äôm starting a new position as Lead Data Scientist at Bazaar Technologies !",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 8,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "In 2022, do you want to be a boss or a leader?\n\n#inspiration #management",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 9,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Dear #datascientists, now, as my post about the logistic regression ( https://lnkd.in/ednkCbHw ) is slowly fading away from your feeds, let's check the level of understanding why the \"logistic regression is a regression\" (of course can be applied for classification, if you defined the threshold for the predicted probability).\n\nFor those, who didn't read my post - it's written against the rising level of nonsensical tutorials claiming wrongly that \"logistic regression is not regression\" (google for it).\n\nIn the meantime, let me recall:\n\n- regression is *never* about the raw DV (response) - which can be of any type, either numeric or categoric - but always about a conditional statistics of it - which is always numeric. Typically the statistics is the expected value (mean) but can be also a quartile or trimmed mean\n\n- regression, can be used for:\na) assessments of the magnitude, standard error and statistical significance of model coefficients to see the way each affects the response\n\n/Some use it in so-called \"stepwise methods of variable selection\", but just google for \"why we hate stepwise regression\" and forget it/\n\nb) assessment of the marginal and interaction effects - if you subject the regression model to the analysis of deviance\n\n/ ANOVA is a special case of the \"Analysis of Deviance\", and it's based on the general linear model. Analysis of Deviance isn't limited to any kind of a regression model and doesn't require any assumptions. In R it's called via car::Anova or stats::anova and can be applied to dozens of models, including quantile or Cox regression/\n\nc) extrapolation, including prediction of future values. This is a 2-step action: first you calculate the regression model, then you provide the range of new data to make the prediction based on that.\n\n/ this is where you use it for the classification: 1) you build a logistic regression model based on the training data, 2) then you use it to predict the probability of a success (that's the E(Y|X) - recall the binomial distribution to get it) per given values of the predictor, 3) you apply a decision rule to the predicted probability and obtain the final class /\n\n- logistic regression is no different (in terms of the \"engine\" for estimating the model coefficients) from any other special case of the Generalized Linear Model (with small extensions), like linear/Poisson/negative binomial/beta (VGLM)/gamma/ordinal LR & multinomial.\n\n- logistic regression was invented in the end of 19th century for regression techniques, long before the common framework of the GLM (detains in the post I linked above)\n\nIn the next year I will initiate a website \"logistic_regression_is_regression dotcom\", by the way.",
        "url_links": [
            "https://lnkd.in/ednkCbHw"
        ],
        "url_texts": "LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.\nSelect Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings.\nDiscover\n                  People\n                  Learning\n                  Jobs\n\nAdrian Olszewski's Post\nLinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.\nSelect Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings.\nDiscover                  People                  Learning                  Jobs\nAdrian Olszewski's Post\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this post                            Report              Report  BackSubmit\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nDear Data Scientists! Finally, I did it üôÇ - bought a domain ‚≠êlogistic-regression-is-regression _ com‚≠ê and hope to start soon making a rudimentary, single-page website, trying to straighten out this awkward situation, when people deny the LR being a regression ü§Ø / Just \\\"Google\\\" for \\\"logistic regression is not a regression\\\" or \\\"hundred page machine learning logistic regression is not\\\".\n/\n \nListen, it's OK to make a mental shortcut and name the logistic classifier (LC) the LR. No problem - everyone will understand what's going on. But it's unimaginable to a statistician that there are people, gurus and even authors of bestsellers in ML, who claim, that \\\"logistic regression is not a regression\\\". / LR is special case of the GLM: a binomial regression with logit link. It models the probability of a success. LC = LR + threshold for that probability, usually (not necessarily) 50%. See the attached pic. Check also the output from any statistical package. See the regression coefficients? Surprised? You can infer LC also from a 1L neural network and other approaches directly. But THIS does NOT mean that the LR is \\\"not a regression\\\".\n/ For me this topic is cringe, but in the past I've got a number of messages from various people, mostly students, who complained, that they failed a question during an exam or interview by repeating this nonsense.  What are the sources of this so common misconception? Simply, lack of understanding the basics of regression, visible in common fallacies, that:\nüö´ regression requires the DV to be numeric *only*\nüö´ either or both IV and DV should follow the normal distribution \nüö´ the linear regression produces straight lines *only*\nüö´ skewed DV must be log-transformed\nüö´ everyone today use the LR for classification (maybe only a few guys use it for anything else) Of course neither of the above is true. / BTW, naming dozens of thousands of econometricians, epidemiologists, biostatisticians, psychologists, physicists and researchers-in-any-domain using the LR for regressing odd ratios on daily basis for more than 100 years (19th century: \\\"Origins of the Logistic Regression\\\" https://lnkd.in/gYChNvt ) \\\"a few guys\\\" is a \\\"little\\\" exaggeration, isn't it?ü§¶‚ôÇÔ∏è\n / People confuse the logistic classifier with logistic regression, because they were told, that \\\"the outcome of the LReg. is binary while the regression needs only numerical outcomes\\\". Sigh, all is confused here. Repeat after me: üîäthe outcome of any single regression is numeric. A regression is about a relationship between the predictor and the conditional statistics* of the DV. The DV does NOT need to be numerical itself.  / *Typically it's the expected value (the mean), but it can be also the quantile, e.g. median in quantile regression, or trimmed mean in robust regression. / #statistics #biostatistics #datascience #machinelearning #ml #data #logisticregression #classification #ai #datascientists #python #rstats #regression #automl #modelling #dataanalysis\n\n\nCopy   LinkedIn   Facebook   Twitter\nCopy\nLinkedIn\nFacebook\nTwitter\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nThis time it's about a new kind of nonsense that flooded the Internet, especially the places, where beginners learn: denying that LR is a regression ü§¶‚ôÇÔ∏è People, who believe in that, not only risk failing an interview or an exam on basics of statistics, but also limit themselves as hell by throwing out:\n- over 100 years of the the theory of a regression,\n- over 100 of the history of the LR (a link to an article on its origins is in the post),\n- over 50 years of the theory of the GLM + modern extensions (LR with random effects via mixed models and repeated/clustered data via Generalized Estimating Equations),\n- generalizations (proportional-odds model - directly related to non-parametric statistical tests, non-proportional model, fractional LR, multinomial LR)\n- and dozens of areas, where a regression over categorical response (binary, n-ary nominal and ordinal) for modelling odds ratios is an everyday tool, used by dozens of thousand people (to recall at least epidemiology, clinical laboratory diagnostics. longitudinal clinical trials and RWD in pharma & medicine) ü§¶‚ôÇÔ∏è\nChayan Kathuria, Jennifer Cooper, MBA, Harpreet Sahota, Manjunatha Gummaraju, Greg Coquillo, PIYUSH PATHAK, Jackleen Rasmy, Dr Christian Setzkorn, Andriy Burkov\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nBTW, don't miss a free webinar on the logistic regression from The Analysis Factor! https://www.linkedin.com/posts/the-analysis-factor-llc_home-page-activity-6868587873239670784-ul6r\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nBy the way - just recent use of the LR exactly as a regression to assess the main and interaction effects (taken from another topic on the LinkedIn): https://www.dropbox.com/s/wwknh7iyrq5z5v6/competition.pdf?dl=0\nData Science Decadenarian | Skilled Worker and H1B visa holder | Singapore citizen\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nYou bought the domain with your own funds?  Adrian, please create a Patreon, and folks, please donate to it!\nTechnical Leader of Data Science at Baker Hughes\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nI actually used logistic regression not for classification. The data I wanted to model shifted from one constant, stable value to another constant and stable value and I wanted to interpolate some data during the transition and stabilization between the two values and I used it for that. Was fun.\nBiostatistician\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nI always send this stackexchange page I bookmarked ages ago to anyone who asks/says that LR is \\\"just\\\" a classifier: https://datascience.stackexchange.com/a/484\nPhD student i Machine Learning and Causal Inference\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nThat's nice of you. You could also just ask them to take stats 101 at uni... The bar for passing as a \\\"data scientist\\\" i too low.\nStatistician at Center for Medicare and Medicaid Innovation\nReport this comment                            Report              Report  BackSubmit\nReport this comment\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nThis is a great!\nTo view or add a comment, sign in\nSee other posts by Adrian\nAdrian Olszewski                              Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è                                                                                                                                                                22h                                  Report this post                            Report              Report  BackSubmit          Probably not every #machinelearning or #datascience specialist will work for life only doing prediction, classification and pattern detection, training ML models. Data analysis is a wider topic full of different problems and approaches. One day they may be asked to step out from their domain and do something in other fields, like the experimental research (randomized controlled trials), for example to temporarily replace a sick person at some analytical task. Below is an illustration showing a situation from my work a few years ago. There was a longitudinal clinical trial with a binary endpoint to analyze in a very routine manner. It was assumed to use the #logisticregression via GEE estimation with some small-sample adjustment and additional \\\"fixes\\\" due to missing observations. The tasks covered: ‚û°Ô∏è comparison of the probability of success between the study arms at certain timepoints, accounting for the repetitive measurements, and provision of both raw and multiplicity-adjusted inferential outcomes (p-values, confidence intervals) via exact parametric method + calculate appropriate effect size measures. ‚û°Ô∏è performing a non-inferiority test (employing clinical significance) at two selected timepoints via appropriate confidence intervals of the difference in probabilities - via average marginal effect. ‚û°Ô∏è an assessment of the impact (magnitude, direction) of certain covariates on the clinical success and provide the covariate-adjusted EM-means for their main effects, their interactions and finally - appropriate contrasts to explore the nature of the (2 and 3-level) interactions in details. ‚û°Ô∏è analyzing the over-time within-arm trends of successes for the treatment persistence. A biostatistician assigned to this task got sick for a few days, and no other was free. So they asked a data scientist from the R&D department - \\\"data analysis is data analysis\\\" ü§∑‚ôÇÔ∏è the manager said, unaware of potential problems. The task wasn't difficult, but required some time to spend on. The data scientist was asked to perform the steps above, but this reaction was specific and nervous. He said it's impossible with the LR as it's not a regression that we have to use other tools (couldn't say which) üôÑ Eventually he refused to do a \\\"nonsensical work\\\", as if trying learning something new was a \\\"disgrace\\\" to himü§¶‚ôÇÔ∏è He didn't think a second that if people ask him to do something \\\"routine\\\" in their field, it is *rather doable*. Another person from the same team of RnD completed the task with no problem, after a brief introduction to a new topic. He never used the GEE before and didn't use the LR for testing hypotheses, but knowing the GLM he learned the extension and new application quickly.  üîîDon't let your daily routine to limit you. #statistics #research                              75                                             4 Comments                           Like             Comment                      Share               Copy   LinkedIn   Facebook   Twitter           To view or add a comment, sign in                           Adrian Olszewski                              Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è                                                                                                                                                                5d                                             Edited                                                    Report this post                            Report              Report  BackSubmit          Hi folks! I can't even count how many times I've ranted about the common approach of assuming a normal distribution for unknown distributions üòÖ I've discussed this topic extensively, but providing in-depth explanations in LinkedIn comments isn't the most convenient way to do so ü§∑‚ôÇÔ∏è  My employer (#2KMM CRO) was pressuring me to write a new post for our corporate blog, which had long been out of date, and I thought this topic would be perfect for a new post üòâ  Here it is! https://lnkd.in/dZS_k7fm / üî¥ errata will be issued soon. For 2 panels of plots for the \\\"free-ride CLT\\\", the QQ plots should be labelled with sums, not products. Copy/pasting is evil üëø See a comment for details / This is especially true since I come from the field of #clinicaltrials üíä which is known for its prevalence of üëâ skewed (both left & right) distributions. A significant portion of my daily work involves clinical biochemistry, pharmacokinetics, pharmacodynamics, and therapeutic areas, which are rich in datasets that are naturally described by (extremely) skewed distributions and neither symmetry nor normality is natural here (similarly, the normal distribution becomes para-normal üëª) Did I say \\\"naturally\\\"?ü§î Yes, here the skewness arises from processes acting multiplicatively alone and in combination, from analysing truncated observations, from time-to-event analyses, from kinetic reactions, and from the way the enzymes and hormones act in living organisms. Mixtures of distributions (especially those of varying dispersion) that are not easily separable due to a lack of domain knowledge are additional source of the skewness. Last, but not least, special conditions can distort even typically symmetric distributions into highly skewed ones (7 orders of magnitude, observations lying 10xSD from the mean) and ignoring this by assuming normality would lead to serious failure ‚ö†üí£  / And yes, I have personally experienced the painful consequences üôÑ of such data treatment a few times, but the lesson was not in vain. Today I can share what I learned with you üë©üè´/ It took me some time to gather all the thoughts and ideas I've been sharing with you over the past decade on this topic. So, dear Specialists in #datascience, #machinelearning, #statistics and #biostatistics, and any other kind of #dataanalysis, üì¢ let me introduce you to my perspective on the \\\"cult of normality\\\".   Will I deny that the normal distribution is common in the Universe?By no means! It is ubiquitous, and like many others, I have observed it many times.  üëÄ    Instead I will show you that asymmetry is an inherent part of the universe, and it arises from fundamental mechanisms. üïµ‚ôÇÔ∏è üîî PS: Stay tuned for the next post, where I will address the common misconception that parametric analysis requires a normal distribution. It will be a continuation of the current topic, illustrating an exemplary outcome of the \\\"cult of normality,\\\" as if no other distributions ever existed in the world... ü§¶‚ôÄÔ∏è                               181                                             10 Comments                           Like             Comment                      Share               Copy   LinkedIn   Facebook   Twitter           To view or add a comment, sign in                        Adrian Olszewski                              Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è                                                                                                                                                                1mo                                             Edited                                                    Report this post                            Report              Report  BackSubmit          Always remember to use the correct model when teaching others. Examples of misleading teaching:1) \\\"logistic regression is not a regression\\\" == \\\"regression requires continuous response\\\"2) \\\"all parametric tests need normally distributed data\\\"3) \\\"p-value is the probability that H0 is true\\\"4) \\\"confidence interval is a range in which the true value of some parameter is located with 95% probability\\\"5) \\\"Mann-Whitney test compares medians\\\" (not without strong assumptions, like IID and symmetry about medians)6) \\\"The Central Limit Theorem relaxes the distributional assumptions (esp. at N>30)\\\"7) \\\"Non-parametric methods are always better than parametric; No assumptions = no problems!\\\" (if only they knew how BIG can be the NEW problems!)8) \\\"p > alpha means that the null hypothesis is proven\\\"and many more.9) arithmetic mean is always meaningful in symmetrically distributed data (always? R: set.seed(353); x <- rbeta(100, .1, .1); hist(x); abline(v=mean(x), col=\\\"red\\\");)10) regression needs normally distributed variables (response and/or predictor)and many more... PS: Tim Morris is correct, I should draw blue squares not just lines. Just wanted to show the direction in which the strings should pull the straw and myself I missed the point. SQUARES says it all. #statistics #datascience #ml #machinelearning #dataanalysis #research                             477                                             42 Comments                           Like             Comment                      Share               Copy   LinkedIn   Facebook   Twitter           To view or add a comment, sign in                              Adrian Olszewski                              Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è                                                                                                                                                                1mo                                             Edited                                                    Report this post                            Report              Report  BackSubmit          Let's talk about a few misconceptions about parametric methods:üö© They need \\\"normality\\\".No!This is the straight consequence of the fact that everyone everywhere teaches about the z/t tests and AN[C]OVA procedure as if they were the only parametric tests in the world.In the world, in which we have 850+ statistical tests üòÇü§¶‚ôÇÔ∏è lots of which are parametric. Parametric means \\\"based/using parameters of some distribution chosen to *facilitate* the analytical process. The normal one is just a 1 out of dozens. And since all parametric tests of locations are somehow based on regression models (either through Wald's or LRT or Rao tests of contrasts over them), it naturally pertains to those classic tests. ‚û°Ô∏è the entire GLM (Generalized Linear Model) family deals with both Gaussian and üî∏non-Gaussianüî∏ conditional distributions and mean-variance dependency. It covers skewed and U-shaped distributions.The special cases:üöÄlogistic regression: to compare/model proportionsüöÄ beta regression to model ratios, percentages and deal with 0-1 truncated support,üöÄ Poisson and negative-binomial to model/compare counts / integers,üöÄ gamma regression to deal with highly skewed data,üöÄexponential distribution family (incl. Weibull), log-normal, Gompertz, Pareto in parametric survival analysis (e.g. AFT - accelerated failure time). Surprised? You can compare two skewed as hell distributions if they approximately match gamma parametrically. No need for non-parametric stuff or transformations! ‚û°Ô∏è How about the non-parametric permutation framework applied to *parametric methods*? Permutation Welch t-test doesn't need the normality assumption, as long as comparing means for such data üî∏is meaningful üî∏ Happy? üòä ‚û°Ô∏è how about a semi-parametric GEE (Generalized Estimating Equations) extension of the GLM? If you choose the Gaussian conditional distribution + identity link, it gives you parametric response but without the need for the normality of residuals.And it also handles heteroscedasticity & dependency in data ü§ó   üö©They need equal variances.No. Neither of the below:‚û°Ô∏è The Welch t-test, Welch ANOVA‚û°Ô∏è The GLS (Generalized Least Square) estimation‚û°Ô∏èThe GEE (OK, it's semi-parametric, but still gives you parametric answer: the GLM on steroids üòéüèãÔ∏è‚ôÄÔ∏è)‚û°Ô∏è (Generalized) Mixed Models üí°BTW, GLS + a single binary predictor + Satterthwaite degrees of freedom = Welch t testü§∑‚ôÇÔ∏è üö© They need independent observations.Nope.‚û°Ô∏è practically ALL tests have their paired versions‚û°Ô∏è GLS‚û°Ô∏è GEE‚û°Ô∏è Mixed models (LMM & GLMM) Now, before you switch to a non-parametric method, which will entirely CHANGE your research hypothesis, mess with Box-Cox and similar transformations, just look at your statistical toolbox. I'm 200% sure you'll find lots of *parametric* or *giving you parametric result* tools. And remember - t-test / z-test don't exhaust the family of parametric methods! #statistics #datascience #dataanalytics #dataanalysis #ml #machinelearning #research                              518                                             38 Comments                           Like             Comment                      Share               Copy   LinkedIn   Facebook   Twitter           To view or add a comment, sign in\nAdrian Olszewski\n            \n \n\n                Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\n            \n\n\n                  \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      22h\n  \n \n \n\n \n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nProbably not every #machinelearning or #datascience specialist will work for life only doing prediction, classification and pattern detection, training ML models. Data analysis is a wider topic full of different problems and approaches.\n\nOne day they may be asked to step out from their domain and do something in other fields, like the experimental research (randomized controlled trials), for example to temporarily replace a sick person at some analytical task.\n\nBelow is an illustration showing a situation from my work a few years ago.\n\nThere was a longitudinal clinical trial with a binary endpoint to analyze in a very routine manner. It was assumed to use the #logisticregression via GEE estimation with some small-sample adjustment and additional \"fixes\" due to missing observations.\n\nThe tasks covered:\n\n‚û°Ô∏è comparison of the probability of success between the study arms at certain timepoints, accounting for the repetitive measurements, and provision of both raw and multiplicity-adjusted inferential outcomes (p-values, confidence intervals) via exact parametric method + calculate appropriate effect size measures.\n\n‚û°Ô∏è performing a non-inferiority test (employing clinical significance) at two selected timepoints via appropriate confidence intervals of the difference in probabilities - via average marginal effect.\n\n‚û°Ô∏è an assessment of the impact (magnitude, direction) of certain covariates on the clinical success and provide the covariate-adjusted EM-means for their main effects, their interactions and finally - appropriate contrasts to explore the nature of the (2 and 3-level) interactions in details.\n\n‚û°Ô∏è analyzing the over-time within-arm trends of successes for the treatment persistence.\n\nA biostatistician assigned to this task got sick for a few days, and no other was free. So they asked a data scientist from the R&D department - \"data analysis is data analysis\" ü§∑‚ôÇÔ∏è the manager said, unaware of potential problems. The task wasn't difficult, but required some time to spend on.\n\nThe data scientist was asked to perform the steps above, but this reaction was specific and nervous. He said it's impossible with the LR as it's not a regression that we have to use other tools (couldn't say which) üôÑ Eventually he refused to do a \"nonsensical work\", as if trying learning something new was a \"disgrace\" to himü§¶‚ôÇÔ∏è He didn't think a second that if people ask him to do something \"routine\" in their field, it is *rather doable*.\n\nAnother person from the same team of RnD completed the task with no problem, after a brief introduction to a new topic. He never used the GEE before and didn't use the LR for testing hypotheses, but knowing the GLM he learned the extension and new application quickly. \n\nüîîDon't let your daily routine to limit you.\n\n#statistics #research \n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n                    75\n              \n\n\n \n\n \n\n\n\n\n\n        \n                4 Comments\n            \n      \n\n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n\n\n              Share\n            \n\n\n\n\n\nCopy\n\n\n\n\n\nLinkedIn\n\n\n\n\n\nFacebook\n\n\n\n\n\nTwitter\n\n\n\n\n \n\n\n\n      To view or add a comment, sign in\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\nCopy\nLinkedIn\nFacebook\nTwitter\n\nAdrian Olszewski\n            \n \n\n                Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\n            \n\n\n                  \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      5d\n  \n                    \n\n                      Edited\n                    \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nHi folks! I can't even count how many times I've ranted about the common approach of assuming a normal distribution for unknown distributions üòÖ I've discussed this topic extensively, but providing in-depth explanations in LinkedIn comments isn't the most convenient way to do so ü§∑‚ôÇÔ∏è \n\nMy employer (#2KMM CRO) was pressuring me to write a new post for our corporate blog, which had long been out of date, and I thought this topic would be perfect for a new post üòâ \n\nHere it is! https://lnkd.in/dZS_k7fm\n\n/ üî¥ errata will be issued soon. For 2 panels of plots for the \"free-ride CLT\", the QQ plots should be labelled with sums, not products. Copy/pasting is evil üëø See a comment for details /\n\nThis is especially true since I come from the field of #clinicaltrials üíä which is known for its prevalence of üëâ skewed (both left & right) distributions.\n\nA significant portion of my daily work involves clinical biochemistry, pharmacokinetics, pharmacodynamics, and therapeutic areas, which are rich in datasets that are naturally described by (extremely) skewed distributions and neither symmetry nor normality is natural here (similarly, the normal distribution becomes para-normal üëª)\n\nDid I say \"naturally\"?ü§î Yes, here the skewness arises from processes acting multiplicatively alone and in combination, from analysing truncated observations, from time-to-event analyses, from kinetic reactions, and from the way the enzymes and hormones act in living organisms. Mixtures of distributions (especially those of varying dispersion) that are not easily separable due to a lack of domain knowledge are additional source of the skewness.\n\nLast, but not least, special conditions can distort even typically symmetric distributions into highly skewed ones (7 orders of magnitude, observations lying 10xSD from the mean) and ignoring this by assuming normality would lead to serious failure ‚ö†üí£ \n\n/ And yes, I have personally experienced the painful consequences üôÑ of such data treatment a few times, but the lesson was not in vain. Today I can share what I learned with you üë©üè´/\n\nIt took me some time to gather all the thoughts and ideas I've been sharing with you over the past decade on this topic. So, dear Specialists in #datascience, #machinelearning, #statistics and #biostatistics, and any other kind of #dataanalysis, üì¢ let me introduce you to my perspective on the \"cult of normality\".  \n\nWill I deny that the normal distribution is common in the Universe?\nBy no means! It is ubiquitous, and like many others, I have observed it many times.  üëÄ   \n\nInstead I will show you that asymmetry is an inherent part of the universe, and it arises from fundamental mechanisms. üïµ‚ôÇÔ∏è\n\nüîî PS: Stay tuned for the next post, where I will address the common misconception that parametric analysis requires a normal distribution. It will be a continuation of the current topic, illustrating an exemplary outcome of the \"cult of normality,\" as if no other distributions ever existed in the world... ü§¶‚ôÄÔ∏è \n\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n                    181\n              \n\n\n \n\n \n\n\n\n\n\n        \n                10 Comments\n            \n      \n\n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n\n\n              Share\n            \n\n\n\n\n\nCopy\n\n\n\n\n\nLinkedIn\n\n\n\n\n\nFacebook\n\n\n\n\n\nTwitter\n\n\n\n\n \n\n\n\n      To view or add a comment, sign in\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\nCopy\nLinkedIn\nFacebook\nTwitter\nAdrian Olszewski\n            \n \n\n                Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\n            \n\n\n                  \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      1mo\n  \n                    \n\n                      Edited\n                    \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nAlways remember to use the correct model when teaching others.\n\nExamples of misleading teaching:\n1) \"logistic regression is not a regression\" == \"regression requires continuous response\"\n2) \"all parametric tests need normally distributed data\"\n3) \"p-value is the probability that H0 is true\"\n4) \"confidence interval is a range in which the true value of some parameter is located with 95% probability\"\n5) \"Mann-Whitney test compares medians\" (not without strong assumptions, like IID and symmetry about medians)\n6) \"The Central Limit Theorem relaxes the distributional assumptions (esp. at N>30)\"\n7) \"Non-parametric methods are always better than parametric; No assumptions = no problems!\" (if only they knew how BIG can be the NEW problems!)\n8) \"p > alpha means that the null hypothesis is proven\"\nand many more.\n9) arithmetic mean is always meaningful in symmetrically distributed data (always? R: set.seed(353); x <- rbeta(100, .1, .1); hist(x); abline(v=mean(x), col=\"red\");)\n10) regression needs normally distributed variables (response and/or predictor)\nand many more...\n\nPS: Tim Morris is correct, I should draw blue squares not just lines. Just wanted to show the direction in which the strings should pull the straw and myself I missed the point. SQUARES says it all.\n\n#statistics #datascience #ml #machinelearning #dataanalysis #research\n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n                    477\n              \n\n\n \n\n \n\n\n\n\n\n        \n                42 Comments\n            \n      \n\n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n\n\n              Share\n            \n\n\n\n\n\nCopy\n\n\n\n\n\nLinkedIn\n\n\n\n\n\nFacebook\n\n\n\n\n\nTwitter\n\n\n\n\n \n\n\n\n      To view or add a comment, sign in\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\nCopy\nLinkedIn\nFacebook\nTwitter\n\n\nAdrian Olszewski\n            \n \n\n                Biostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\n            \n\n\n                  \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      1mo\n  \n                    \n\n                      Edited\n                    \n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n                      Report this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nLet's talk about a few misconceptions about parametric methods:\nüö© They need \"normality\".\nNo!\nThis is the straight consequence of the fact that everyone everywhere teaches about the z/t tests and AN[C]OVA procedure as if they were the only parametric tests in the world.\nIn the world, in which we have 850+ statistical tests üòÇü§¶‚ôÇÔ∏è lots of which are parametric.\n\nParametric means \"based/using parameters of some distribution chosen to *facilitate* the analytical process. The normal one is just a 1 out of dozens.\n\nAnd since all parametric tests of locations are somehow based on regression models (either through Wald's or LRT or Rao tests of contrasts over them), it naturally pertains to those classic tests.\n\n‚û°Ô∏è the entire GLM (Generalized Linear Model) family deals with both Gaussian and üî∏non-Gaussianüî∏ conditional distributions and mean-variance dependency. It covers skewed and U-shaped distributions.\nThe special cases:\nüöÄlogistic regression: to compare/model proportions\nüöÄ beta regression to model ratios, percentages and deal with 0-1 truncated support,\nüöÄ Poisson and negative-binomial to model/compare counts / integers,\nüöÄ gamma regression to deal with highly skewed data,\nüöÄexponential distribution family (incl. Weibull), log-normal, Gompertz, Pareto in parametric survival analysis (e.g. AFT - accelerated failure time).\n\nSurprised? You can compare two skewed as hell distributions if they approximately match gamma parametrically. No need for non-parametric stuff or transformations!\n\n‚û°Ô∏è How about the non-parametric permutation framework applied to *parametric methods*? Permutation Welch t-test doesn't need the normality assumption, as long as comparing means for such data üî∏is meaningful üî∏\n\nHappy? üòä\n\n‚û°Ô∏è how about a semi-parametric GEE (Generalized Estimating Equations) extension of the GLM? If you choose the Gaussian conditional distribution + identity link, it gives you parametric response but without the need for the normality of residuals.\nAnd it also handles heteroscedasticity & dependency in data ü§ó  \n\nüö©They need equal variances.\nNo. Neither of the below:\n‚û°Ô∏è The Welch t-test, Welch ANOVA\n‚û°Ô∏è The GLS (Generalized Least Square) estimation\n‚û°Ô∏èThe GEE (OK, it's semi-parametric, but still gives you parametric answer: the GLM on steroids üòéüèãÔ∏è‚ôÄÔ∏è)\n‚û°Ô∏è (Generalized) Mixed Models\n\nüí°BTW, GLS + a single binary predictor + Satterthwaite degrees of freedom = Welch t testü§∑‚ôÇÔ∏è\n\nüö© They need independent observations.\nNope.\n‚û°Ô∏è practically ALL tests have their paired versions\n‚û°Ô∏è GLS\n‚û°Ô∏è GEE\n‚û°Ô∏è Mixed models (LMM & GLMM)\n\nNow, before you switch to a non-parametric method, which will entirely CHANGE your research hypothesis, mess with Box-Cox and similar transformations, just look at your statistical toolbox.\n\nI'm 200% sure you'll find lots of *parametric* or *giving you parametric result* tools.\n\nAnd remember - t-test / z-test don't exhaust the family of parametric methods!\n\n#statistics #datascience #dataanalytics #dataanalysis #ml #machinelearning #research \n \n\n\n\n \n\n\n\n\n\n\n\n\n\n\n                    518\n              \n\n\n \n\n \n\n\n\n\n\n        \n                38 Comments\n            \n      \n\n\n\n\n\n\n      Like\n    \n\n\n\n\n\n      Comment\n    \n\n\n\n\n\n\n\n              Share\n            \n\n\n\n\n\nCopy\n\n\n\n\n\nLinkedIn\n\n\n\n\n\nFacebook\n\n\n\n\n\nTwitter\n\n\n\n\n \n\n\n\n      To view or add a comment, sign in\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\n\nCopy\nLinkedIn\nFacebook\nTwitter\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this post                            Report              Report  BackSubmit\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nProbably not every #machinelearning or #datascience specialist will work for life only doing prediction, classification and pattern detection, training ML models. Data analysis is a wider topic full of different problems and approaches. One day they may be asked to step out from their domain and do something in other fields, like the experimental research (randomized controlled trials), for example to temporarily replace a sick person at some analytical task. Below is an illustration showing a situation from my work a few years ago. There was a longitudinal clinical trial with a binary endpoint to analyze in a very routine manner. It was assumed to use the #logisticregression via GEE estimation with some small-sample adjustment and additional \\\"fixes\\\" due to missing observations. The tasks covered: ‚û°Ô∏è comparison of the probability of success between the study arms at certain timepoints, accounting for the repetitive measurements, and provision of both raw and multiplicity-adjusted inferential outcomes (p-values, confidence intervals) via exact parametric method + calculate appropriate effect size measures. ‚û°Ô∏è performing a non-inferiority test (employing clinical significance) at two selected timepoints via appropriate confidence intervals of the difference in probabilities - via average marginal effect. ‚û°Ô∏è an assessment of the impact (magnitude, direction) of certain covariates on the clinical success and provide the covariate-adjusted EM-means for their main effects, their interactions and finally - appropriate contrasts to explore the nature of the (2 and 3-level) interactions in details. ‚û°Ô∏è analyzing the over-time within-arm trends of successes for the treatment persistence. A biostatistician assigned to this task got sick for a few days, and no other was free. So they asked a data scientist from the R&D department - \\\"data analysis is data analysis\\\" ü§∑‚ôÇÔ∏è the manager said, unaware of potential problems. The task wasn't difficult, but required some time to spend on. The data scientist was asked to perform the steps above, but this reaction was specific and nervous. He said it's impossible with the LR as it's not a regression that we have to use other tools (couldn't say which) üôÑ Eventually he refused to do a \\\"nonsensical work\\\", as if trying learning something new was a \\\"disgrace\\\" to himü§¶‚ôÇÔ∏è He didn't think a second that if people ask him to do something \\\"routine\\\" in their field, it is *rather doable*. Another person from the same team of RnD completed the task with no problem, after a brief introduction to a new topic. He never used the GEE before and didn't use the LR for testing hypotheses, but knowing the GLM he learned the extension and new application quickly.  üîîDon't let your daily routine to limit you. #statistics #research\n\n\nCopy   LinkedIn   Facebook   Twitter\nCopy\nLinkedIn\nFacebook\nTwitter\nTo view or add a comment, sign in\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this post                            Report              Report  BackSubmit\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nHi folks! I can't even count how many times I've ranted about the common approach of assuming a normal distribution for unknown distributions üòÖ I've discussed this topic extensively, but providing in-depth explanations in LinkedIn comments isn't the most convenient way to do so ü§∑‚ôÇÔ∏è  My employer (#2KMM CRO) was pressuring me to write a new post for our corporate blog, which had long been out of date, and I thought this topic would be perfect for a new post üòâ  Here it is! https://lnkd.in/dZS_k7fm / üî¥ errata will be issued soon. For 2 panels of plots for the \\\"free-ride CLT\\\", the QQ plots should be labelled with sums, not products. Copy/pasting is evil üëø See a comment for details / This is especially true since I come from the field of #clinicaltrials üíä which is known for its prevalence of üëâ skewed (both left & right) distributions. A significant portion of my daily work involves clinical biochemistry, pharmacokinetics, pharmacodynamics, and therapeutic areas, which are rich in datasets that are naturally described by (extremely) skewed distributions and neither symmetry nor normality is natural here (similarly, the normal distribution becomes para-normal üëª) Did I say \\\"naturally\\\"?ü§î Yes, here the skewness arises from processes acting multiplicatively alone and in combination, from analysing truncated observations, from time-to-event analyses, from kinetic reactions, and from the way the enzymes and hormones act in living organisms. Mixtures of distributions (especially those of varying dispersion) that are not easily separable due to a lack of domain knowledge are additional source of the skewness. Last, but not least, special conditions can distort even typically symmetric distributions into highly skewed ones (7 orders of magnitude, observations lying 10xSD from the mean) and ignoring this by assuming normality would lead to serious failure ‚ö†üí£  / And yes, I have personally experienced the painful consequences üôÑ of such data treatment a few times, but the lesson was not in vain. Today I can share what I learned with you üë©üè´/ It took me some time to gather all the thoughts and ideas I've been sharing with you over the past decade on this topic. So, dear Specialists in #datascience, #machinelearning, #statistics and #biostatistics, and any other kind of #dataanalysis, üì¢ let me introduce you to my perspective on the \\\"cult of normality\\\".   Will I deny that the normal distribution is common in the Universe?\nBy no means! It is ubiquitous, and like many others, I have observed it many times.  üëÄ    Instead I will show you that asymmetry is an inherent part of the universe, and it arises from fundamental mechanisms. üïµ‚ôÇÔ∏è üîî PS: Stay tuned for the next post, where I will address the common misconception that parametric analysis requires a normal distribution. It will be a continuation of the current topic, illustrating an exemplary outcome of the \\\"cult of normality,\\\" as if no other distributions ever existed in the world... ü§¶‚ôÄÔ∏è\n\n\nCopy   LinkedIn   Facebook   Twitter\nCopy\nLinkedIn\nFacebook\nTwitter\nTo view or add a comment, sign in\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this post                            Report              Report  BackSubmit\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nAlways remember to use the correct model when teaching others. Examples of misleading teaching:\n1) \\\"logistic regression is not a regression\\\" == \\\"regression requires continuous response\\\"\n2) \\\"all parametric tests need normally distributed data\\\"\n3) \\\"p-value is the probability that H0 is true\\\"\n4) \\\"confidence interval is a range in which the true value of some parameter is located with 95% probability\\\"\n5) \\\"Mann-Whitney test compares medians\\\" (not without strong assumptions, like IID and symmetry about medians)\n6) \\\"The Central Limit Theorem relaxes the distributional assumptions (esp. at N>30)\\\"\n7) \\\"Non-parametric methods are always better than parametric; No assumptions = no problems!\\\" (if only they knew how BIG can be the NEW problems!)\n8) \\\"p > alpha means that the null hypothesis is proven\\\"\nand many more.\n9) arithmetic mean is always meaningful in symmetrically distributed data (always? R: set.seed(353); x <- rbeta(100, .1, .1); hist(x); abline(v=mean(x), col=\\\"red\\\");)\n10) regression needs normally distributed variables (response and/or predictor)\nand many more... PS: Tim Morris is correct, I should draw blue squares not just lines. Just wanted to show the direction in which the strings should pull the straw and myself I missed the point. SQUARES says it all. #statistics #datascience #ml #machinelearning #dataanalysis #research\n\n\nCopy   LinkedIn   Facebook   Twitter\nCopy\nLinkedIn\nFacebook\nTwitter\nTo view or add a comment, sign in\nBiostatistician at 2KMM CRO ‚¶ø R in Clinical Trials ‚¶ø Not a Data Scientist; no ML/AI/Big data ‚¶ø a frequentist ‚¶ø üöóü•©üíµüëÅÔ∏è\nReport this post                            Report              Report  BackSubmit\nReport this post\n                    \n    \n\n\n\n\n \n\n\nReport\n\n\n\n \n\n\n          Report\n\n\n\n\nBack\nSubmit\n\n\n\n\n\n\nReport\nReport\n\n\nLet's talk about a few misconceptions about parametric methods:\nüö© They need \\\"normality\\\".\nNo!\nThis is the straight consequence of the fact that everyone everywhere teaches about the z/t tests and AN[C]OVA procedure as if they were the only parametric tests in the world.\nIn the world, in which we have 850+ statistical tests üòÇü§¶‚ôÇÔ∏è lots of which are parametric. Parametric means \\\"based/using parameters of some distribution chosen to *facilitate* the analytical process. The normal one is just a 1 out of dozens. And since all parametric tests of locations are somehow based on regression models (either through Wald's or LRT or Rao tests of contrasts over them), it naturally pertains to those classic tests. ‚û°Ô∏è the entire GLM (Generalized Linear Model) family deals with both Gaussian and üî∏non-Gaussianüî∏ conditional distributions and mean-variance dependency. It covers skewed and U-shaped distributions.\nThe special cases:\nüöÄlogistic regression: to compare/model proportions\nüöÄ beta regression to model ratios, percentages and deal with 0-1 truncated support,\nüöÄ Poisson and negative-binomial to model/compare counts / integers,\nüöÄ gamma regression to deal with highly skewed data,\nüöÄexponential distribution family (incl. Weibull), log-normal, Gompertz, Pareto in parametric survival analysis (e.g. AFT - accelerated failure time). Surprised? You can compare two skewed as hell distributions if they approximately match gamma parametrically. No need for non-parametric stuff or transformations! ‚û°Ô∏è How about the non-parametric permutation framework applied to *parametric methods*? Permutation Welch t-test doesn't need the normality assumption, as long as comparing means for such data üî∏is meaningful üî∏ Happy? üòä ‚û°Ô∏è how about a semi-parametric GEE (Generalized Estimating Equations) extension of the GLM? If you choose the Gaussian conditional distribution + identity link, it gives you parametric response but without the need for the normality of residuals.\nAnd it also handles heteroscedasticity & dependency in data ü§ó   üö©They need equal variances.\nNo. Neither of the below:\n‚û°Ô∏è The Welch t-test, Welch ANOVA\n‚û°Ô∏è The GLS (Generalized Least Square) estimation\n‚û°Ô∏èThe GEE (OK, it's semi-parametric, but still gives you parametric answer: the GLM on steroids üòéüèãÔ∏è‚ôÄÔ∏è)\n‚û°Ô∏è (Generalized) Mixed Models üí°BTW, GLS + a single binary predictor + Satterthwaite degrees of freedom = Welch t testü§∑‚ôÇÔ∏è üö© They need independent observations.\nNope.\n‚û°Ô∏è practically ALL tests have their paired versions\n‚û°Ô∏è GLS\n‚û°Ô∏è GEE\n‚û°Ô∏è Mixed models (LMM & GLMM) Now, before you switch to a non-parametric method, which will entirely CHANGE your research hypothesis, mess with Box-Cox and similar transformations, just look at your statistical toolbox. I'm 200% sure you'll find lots of *parametric* or *giving you parametric result* tools. And remember - t-test / z-test don't exhaust the family of parametric methods! #statistics #datascience #dataanalytics #dataanalysis #ml #machinelearning #research\n\n\nCopy   LinkedIn   Facebook   Twitter\nCopy\nLinkedIn\nFacebook\nTwitter\nTo view or add a comment, sign in\n17,743 followers\n407 Posts                                        2 Articles\n407 Posts\n2 Articles\nMore from this author\nWhy is transforming the response in regression analysis and hypothesis testing so dangerous? ‚ò†Ô∏è                      Adrian Olszewski                                                                                                                                                        7mo                                        Small Data in Clinical Research                      Adrian Olszewski                                                                                                                                                        5y\nWhy is transforming the response in regression analysis and hypothesis testing so dangerous? ‚ò†Ô∏è\n      \n \n\n\n\n\n\n            Adrian Olszewski\n          \n\n            \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      7mo\nSmall Data in Clinical Research\n      \n \n\n\n\n\n\n            Adrian Olszewski\n          \n\n            \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\n      5y\nWhy is transforming the response in regression analysis and hypothesis testing so dangerous? ‚ò†Ô∏è\nSmall Data in Clinical Research\nExplore topics\nMarketing                                Public Administration                                Healthcare                                Engineering                                IT Services                                Sustainability                                Business Administration                              See All\nMarketing\nPublic Administration\nHealthcare\nEngineering\nIT Services\nSustainability\nBusiness Administration\nSee All\nLinkedIn ¬© 2023                      About                                      Accessibility                                      User Agreement                                      Privacy Policy                                      Cookie Policy                                      Copyright Policy                                      Brand Policy                                        Guest Controls                                        Community Guidelines                                     ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)                              ƒåe≈°tina (Czech)                              Dansk (Danish)                              Deutsch (German)              English (English)                   Espa√±ol (Spanish)                              Fran√ßais (French)                              ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)                              Bahasa Indonesia (Bahasa Indonesia)                              Italiano (Italian)                              Êó•Êú¨Ë™û (Japanese)                              ÌïúÍµ≠Ïñ¥ (Korean)                              Bahasa Malaysia (Malay)                              Nederlands (Dutch)                              Norsk (Norwegian)                              Polski (Polish)                              Portugu√™s (Portuguese)                              Rom√¢nƒÉ (Romanian)                              –†—É—Å—Å–∫–∏–π (Russian)                              Svenska (Swedish)                              ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)                              Tagalog (Tagalog)                              T√ºrk√ße (Turkish)                              –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)                              ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))                              Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))                           Language\nLinkedIn\n\n¬© 2023\nAbout\nAccessibility\nUser Agreement\nPrivacy Policy\nCookie Policy\nCopyright Policy\nBrand Policy\nGuest Controls\nCommunity Guidelines\nÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n            \n\n\n\n\n                ƒåe≈°tina (Czech)\n            \n\n\n\n\n                Dansk (Danish)\n            \n\n\n\n\n                Deutsch (German)\n            \n\n\n\n\nEnglish (English)\n\n\n\n\n\n                Espa√±ol (Spanish)\n            \n\n\n\n\n                Fran√ßais (French)\n            \n\n\n\n\n                ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n            \n\n\n\n\n                Bahasa Indonesia (Bahasa Indonesia)\n            \n\n\n\n\n                Italiano (Italian)\n            \n\n\n\n\n                Êó•Êú¨Ë™û (Japanese)\n            \n\n\n\n\n                ÌïúÍµ≠Ïñ¥ (Korean)\n            \n\n\n\n\n                Bahasa Malaysia (Malay)\n            \n\n\n\n\n                Nederlands (Dutch)\n            \n\n\n\n\n                Norsk (Norwegian)\n            \n\n\n\n\n                Polski (Polish)\n            \n\n\n\n\n                Portugu√™s (Portuguese)\n            \n\n\n\n\n                Rom√¢nƒÉ (Romanian)\n            \n\n\n\n\n                –†—É—Å—Å–∫–∏–π (Russian)\n            \n\n\n\n\n                Svenska (Swedish)\n            \n\n\n\n\n                ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n            \n\n\n\n\n                Tagalog (Tagalog)\n            \n\n\n\n\n                T√ºrk√ße (Turkish)\n            \n\n\n\n\n                –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n            \n\n\n\n\n                ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n            \n\n\n\n\n                Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n            \n\n\n\n\n\n            Language\nÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\nƒåe≈°tina (Czech)\nDansk (Danish)\nDeutsch (German)\nEnglish (English)\nEspa√±ol (Spanish)\nFran√ßais (French)\n‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\nBahasa Indonesia (Bahasa Indonesia)\nItaliano (Italian)\nÊó•Êú¨Ë™û (Japanese)\nÌïúÍµ≠Ïñ¥ (Korean)\nBahasa Malaysia (Malay)\nNederlands (Dutch)\nNorsk (Norwegian)\nPolski (Polish)\nPortugu√™s (Portuguese)\nRom√¢nƒÉ (Romanian)\n–†—É—Å—Å–∫–∏–π (Russian)\nSvenska (Swedish)\n‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\nTagalog (Tagalog)\nT√ºrk√ße (Turkish)\n–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\nÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\nÊ≠£È´î‰∏≠Êñá (Chinese (Traditional))\nÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)                              ƒåe≈°tina (Czech)                              Dansk (Danish)                              Deutsch (German)              English (English)                   Espa√±ol (Spanish)                              Fran√ßais (French)                              ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)                              Bahasa Indonesia (Bahasa Indonesia)                              Italiano (Italian)                              Êó•Êú¨Ë™û (Japanese)                              ÌïúÍµ≠Ïñ¥ (Korean)                              Bahasa Malaysia (Malay)                              Nederlands (Dutch)                              Norsk (Norwegian)                              Polski (Polish)                              Portugu√™s (Portuguese)                              Rom√¢nƒÉ (Romanian)                              –†—É—Å—Å–∫–∏–π (Russian)                              Svenska (Swedish)                              ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)                              Tagalog (Tagalog)                              T√ºrk√ße (Turkish)                              –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)                              ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))                              Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\nÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\nƒåe≈°tina (Czech)\nDansk (Danish)\nDeutsch (German)\nEnglish (English)\nEspa√±ol (Spanish)\nFran√ßais (French)\n‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\nBahasa Indonesia (Bahasa Indonesia)\nItaliano (Italian)\nÊó•Êú¨Ë™û (Japanese)\nÌïúÍµ≠Ïñ¥ (Korean)\nBahasa Malaysia (Malay)\nNederlands (Dutch)\nNorsk (Norwegian)\nPolski (Polish)\nPortugu√™s (Portuguese)\nRom√¢nƒÉ (Romanian)\n–†—É—Å—Å–∫–∏–π (Russian)\nSvenska (Swedish)\n‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\nTagalog (Tagalog)\nT√ºrk√ße (Turkish)\n–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\nÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\nÊ≠£È´î‰∏≠Êñá (Chinese (Traditional))\n\n"
    },
    {
        "id": 10,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "What are the various patterns for personalizing recommendations and search? I dug into a couple of industry papers/blogs and summarized them into bandits, item embeddings + MLP, sequential models, graphs, and user embeddings.\n\n‚Ä¢ Want to keep exploring while minimizing regret? Bandits\n‚Ä¢ Starting with neural recommender and want something simple? Embeddings + MLP\n‚Ä¢ Have long-term user histories and sequences?? Sequential\n‚Ä¢ Have sparse behavior data but lots of metadata? Graphs\n‚Ä¢ Want generic embeddings for multiple problems? User models",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 11,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Daraz is hiring!\n\nAmazing opportunity!\n\nIf interested please DM",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 12,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "How do search engines match queries to documents, people, or products?\n\nI studied some examples & found three methods:\n‚Ä¢ Lexical: Rewriting, expansion, relaxation\n‚Ä¢ Graph: Mapping to concepts & relationships\n‚Ä¢ Embedding: Via (self-)supervised learning",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 13,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "I'm so excited! After almost 3 weeks of working on this non-stop, I'm finally open-sourcing my implementation of GAT - the Graph Attention Network! (PyTorch, of course)\n\nThe code is geared towards those who want to learn how GNNs (and more specifically GAT) work. I'll add the Jupiter notebook soon!\n\nI've added a lot of visualizations, lots of comments, I've profiled different sparse matrix formats so that you know which one you should use and when.\n\nThe code actually has 3 different GAT implementations! I've played a lot with what's faster, better, and what takes less memory. I've had some implementations which I've cut since they were slower or less understandable than the ones I finally published.\n\nImplementation 3 is, optimization-wise, in the ballpark of PyTorch Geometric.\n\nImplementations 1 and 2 are conceptually easier to understand but are much less efficient.\n\nMy main next step is to add an inductive example as the current code contains only a transductive setting (same as the other GAT implementations).\n\nI truly hope that the community will find this one useful. ‚ù§\n\nAgain: I want to thank for his patience I had a lot of questions over the last 2+ months. üòÇ",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 14,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "‚òÖ spaCy meets PyTorch-Transformers: Fine-tune BERT, XLNet and GPT-2\n\nIf you‚Äôre using spaCy and have been waiting to incorporate pretrained models in your applications, then look no further than spacy-pytorch-transformers.\n\nIt allows you to use models like BERT in spaCy by interfacing with Hugging Face‚Äôs PyTorch implementations.\n\nThe library also aligns the transformer features with spaCy‚Äôs linguistic tokenization, so you can apply the features to the actual words, instead of just wordpieces.\n\nRead more:\n\n-----------\nüìåOther great articles\n¬ª Transformers and Attention Mechanism\n\n\n¬ª Artificial Intelligence without the Human Touch\n\n\n¬ª What 70% of Data Science Learners Do Wrong\n\n\n\nüí¨ Blog: Follow also Thiago Allue here",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 15,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "‚òÖ GluonNLP v0.7.1 ‚Äî BERT Reloaded\n\nGluonNLP has just been upgraded for the better. The 0.7 release features a BERT Base model pre-trained on a large scale corpus, whose performance is comparable with the BERT Large model from the original BERT paper. Other highlights include more versions of BERT trained on specialized corpora, new models (ERNIE, GPT-2, ESIM etc.), and more datasets. The full release notes can be found here: .\n\nArtificial Intelligence\n-----------------------\n‚óè How to Get Started?\n\n‚óè A nice knowledge Roadmap\n\n‚óè Job - Interview Questions and Answers\n\n‚óè International and Advanced Specializations\n\nIf you like the posts, you can follow me here\n‚á¢ Website\n‚á¢ LinkedIn",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 16,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Add a prefix to all of your column names:\ndf.add_prefix('X_')\n\nAdd a suffix to all of your column names:\ndf.add_suffix('_Y')\nReverse column order in a DataFrame:\ndf.loc[:, ::-1]\n\nReverse row order:\ndf.loc[::-1]\n\nReverse row order and reset the index:\ndf.loc[::-1].reset_index(drop=True)\nFilter DataFrame by multiple OR conditions:\ndf[(df.color == 'red') | (df.color == 'green') | (df.color == 'blue')]\n\nShorter way:\ndf[df.color.isin(['red', 'green', 'blue'])]\n\nInvert the filter:\ndf[~df.color.isin(['red', 'green', 'blue'])]\nRead More at:\nResume:\nCertifications:\nCredits:",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 17,
        "person_name": "https://www.linkedin.com/in/qazi-saad",
        "text_description": "Excited to be speaking at AI Summit this year organized by ITCN Asia. Would be happy to meet if you're around! Feel free to reach out if you're interested to learn more about the work we do and join Daraz in driving e-commerce forward!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 18,
        "person_name": "https://www.linkedin.com/in/softwaredoug",
        "text_description": "Once again, a really inspiring MICES (Mix-Camp E-Commerce Search) agenda coming in June",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 19,
        "person_name": "https://www.linkedin.com/in/hassan-abbas-sqa",
        "text_description": "I‚Äôm happy to share that I‚Äôm starting a new position as Software Development Engineer in Test II at S&P Global!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 20,
        "person_name": "https://www.linkedin.com/in/jtkane",
        "text_description": "Algolia launches AI-powered Algolia NeuralSearch. They have implemented a hybrid approach that I and others have recommend. Combining LLMs with precise keyword search can provide the best of both technologies. In the context of open-source search technologies (#Solr, #ElasticSearch & #Vespa) with open-sourced LLM's (#Databricks, etc.) that are specific to your business content was actively discussed and presented at last week's Conference ().\n\nLast week, I attended the Haystack Conference and this week I am exploring several unique search product features that builds upon this hybrid approach. This is what Search do, we add value on top of a technology paradigm shift as is my .\nAt this point in time, I know similar search products are being actively developed, so all I can say is : Let the Games Begin!\n\nAlgolia is using \"Large Language Models (LLM) ‚Äì the same technology underpinning ChatGPT and generative AI ‚Äì and goes a step further with Algolia‚Äôs Neural Hashing‚Ñ¢ for hyper-scale and constantly learns from user interactions for better results.\"\n\nThis quote from Hayley Sutherland, Research Manager @ IDC: ‚ÄúBy adding Neural Hashing of vectors to its existing keyword-based  search within a single index, leveraging a single API, Algolia has the potential to disrupt AI-powered search with significantly better precision and recall, in a manner that requires less manual work to set up and update, while incurring fewer storage and processing costs.‚Äù",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 21,
        "person_name": "https://www.linkedin.com/in/khatritariq",
        "text_description": "Hello everyone,\n\nI'm happy to share my journey, from idea to 500+ sales for selling free products for my new experiment, .\n\nMy friend  and I got inspired to start this after seeing creators like   who are selling amazing templates on Notion.\n\nWe spent quite sometime researching, designing, and developing our first template, and after launching on Gumroad, we marketed our work in Facebook Notion groups.\n\nWe're still learning and improving, but hope to inspire others to create something they're passionate about.\n\nCheck out our journey in the following article :",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 22,
        "person_name": "https://www.linkedin.com/in/istvanfehervari",
        "text_description": "Prompt engineering isn't engineering.\n\nTweaking the inputs until you get the desired output from a blackbox that we fundamentally do not understand is ... experimentation.\n\nEngineers are not running blind trial-and-error, but instead use their understanding of the world - described by sciences, such as math or physics - to drive the process.\n\nThis sort of science-based guidance is missing when we tune prompts to make LLMs do what we want them to do.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 23,
        "person_name": "https://www.linkedin.com/in/dtunkelang",
        "text_description": "Paying for a ChatGPT-generated book amounts to paying for a prompt. I concede that some people are better than others at prompt engineering, but even so this feels like a scam that won't last long.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 24,
        "person_name": "https://www.linkedin.com/in/rodrigo-nogueira-730ba431",
        "text_description": "It seems that many NLP researchers now believe that RLHF is necessary to unlock the full potential of LLMs. However, PALM, which was solely trained on a language modeling objective, performs on par with GPT-3.5/ChatGPT across a diverse set of NLP tasks ()\n\nHence, most of the model's \"intelligence\" is due to self-supervision. The main benefit of RL is to improve zero-shot performance.\n\nWhile alternatives such as instruction tuning (e.g. FLAN) can be effective for smaller models (e.g. T5-11B), it is uncertain whether it provides significant advantages for larger models. For example, finetuning PALM-540B on instructions yields only a marginal improvement of 2-4 points on BBH and MMLU benchmarks.\n\nConsidering that PALM-540B will likely be classified as a \"mid-size\" model in the near future, it remains unclear if the benefits of instruction tuning will scale proportionally with larger models.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 25,
        "person_name": "https://www.linkedin.com/in/benjaminrogojan",
        "text_description": "If you work in data, you will have a SQL interview round.\n\nIt doesn't matter if you're a data scientist or data engineer.\n\nSQL is everywhere.\n\nHere are some quick tips from beginner to advanced you should know and be able to talk about in a SQL interview.\n\nBeginner \n- Be able to explain the different types of joins. I have sometimes seen this question asked by recruiters as a screener question just to make sure they want to pass you along\n- Know when HAVING is run in SQL's order of operations vs WHERE\n- Know the difference between UNION and UNION ALL\n\nMid\n- Understand how to use a CASE statement inside of a SUM or COUNT function\n- Know at least 1-2 ways you could optimize a query and if you decide one of those answers will be put an index on the table..you should also know what the pros and cons of an index are\n- Be able to implement both a subquery and CTE, but use a CTE if you want to prove that you have good SQL habits\n\nAdvanced\n- Know how to answer a problem with and without a window function. Usually you will need to implement a self join in order to imitate some window functions such as LAG and LEAD.\n- Understand and be able to explain what a correlated subquery is\n- Just reference that you would look at the query execution plan. They will be shocked you even know what that is.\n\nI would love to hear what questions you were asked your SQL interviews. Please share them below!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 26,
        "person_name": "https://www.linkedin.com/in/miguelmalvarez",
        "text_description": "The UK, and London in particular, used to be the 'no-brainer' destination in Europe for people to create companies and search for opportunity. This has changed significantly over the last years.\n\nAs an anecdotical data point, the question \"Will you still live in the UK in the next couple of years\" is a very common one among my friends and colleagues (many working in tech). The answer these days is very different than a decade ago. Many people are exploring living somewhere else (e.g., Lisbon, Berlin, Malaga, ...) and several have already left.\n\nThe UK definitely needs a reboot or we risk losing everything accomplished over the last few decades.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 27,
        "person_name": "https://www.linkedin.com/in/younes-belkada-b1a903145",
        "text_description": "Interested in applying RLHF (Reinforcement Learning with Human Feedback)? Try out trl!\n\nAt we now officially support RLHF training using PPO (Proximal Policy Optimization), and let the community easily train and share their models fine-tuned with this algorithm.\n\nThe first release of trl includes support for most of CausalLM and Seq2SeqLM suite from transformers, this includes supporting all popular models such as OPT, GPT-neo, GPT-J, T5, Flan-T5, BART, etc.\n\nTrain your model in single, multi-GPU setup with and without DeepSpeed for efficient training by leveraging accelerate library from Hugging Face. Fine tune your models now using RLHF and share your best models on the Hub!\n\nIf you are interested in scaling to 100B+ models, checkout the ongoing work of trlx with :\n\nLink to the library:\nLink to docs:",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 28,
        "person_name": "https://www.linkedin.com/in/miguelmalvarez",
        "text_description": "I am very happy to announce my new role as a The Royal Society Entrepreneur in Residence (EiR) at the University of Essex, where I will support the University‚Äôs staff and students on their entrepreneurship journey. Hopefully, I will be able to help new entrepreneurs avoid making some of the many mistakes I have seen and made over the last decade.\n\nThere are more details on the blogpost but, in general, the goal of the program is to ensure academics and students at the university have the right skills and knowledge to create and initially run a start-up. The program will focus on providing enough knowledge in each and all of the critical aspects of an early stage company.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 29,
        "person_name": "https://www.linkedin.com/in/benjaminrogojan",
        "text_description": "A lot of people have DMed me about getting advice for starting their own consulting/freelancing business.\n\nI can't possibly have 1:1s with all of them so this week I am going to do a Youtube live focused on giving advice on just that.\n\nThere are a lot of pros and cons to starting a consulting business. I have made plenty of mistakes along the way and if I can help accelerate anyones experience I would love to do that!\n\nIf you're interested, then comment below and I will add you to the email list so you can get the link!\n\nAlso, if there is a subject you'd like me to focus on, please comment below! Here are a few ideas.\n\n- Pros and Cons of Consulting\n- Building a brand\n- Client Acquisition\n- Project Management\n- Etc",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 30,
        "person_name": "https://www.linkedin.com/in/chiphuyen",
        "text_description": "What I‚Äôm excited for in 2023\n\n1. Data contracts, which are basically ‚Äúschemas on steroids‚Äù. Schemas currently describes the shape of data: {column name: type}. Data contracts can do more. They can also describe:\n\n- Each column‚Äôs expectations (e.g it can only be non-negatives, it has to be within this range) for data validation.\n\n- Column-level privacy policy: if a column is marked as sensitive, it can be automatically masked before anyone accessing it. If column A is marked as sensitive, any column that derives from A should also be marked as sensitive.\n\n- Ownership for auditing.\n\n2. Data engineering - data science convergence. More and more data scientists are picking up best engineering practices (either by choice or by needs) and crossing over to data engineering. Data engineer roles might even be in higher demand than data science roles!\n\nThis convergence will create a new generation of tools that cater to both personas, and change the MLOps workflow significantly.\n\n3. Truly unified data processing. Until now, unified batch and streaming is at API level ‚Äî batch and streaming processing have similar APIs but they‚Äôre still different computation engines underneath. When doing data processing, say to extract the average transaction value, users still need to be aware whether the transaction data is batch or streaming.\n\nI‚Äôm excited to see an engine that eliminates this boundary altogether. It might be a batch engine with more powerful time semantics to handle data in motion, or a streaming engine with more efficient optimization techniques for data at rest, or a new engine altogether. I'm seeing so many exciting projects in the space!\n\n4. More use of generative AI in data workflow. I‚Äôve seen ChatGPT do data analysis, generate fake data, write ML models, find bugs in our code, and write docs. More powerful language models will come, and I‚Äôm hoping for more powerful products built for data scientists/engineers on top of them.\n\n5. Real-time ML goes to small and medium companies. In 2022, we saw an explosion of blog posts by big companies about their real-time ML pipelines (LinkedIn, Snap, Instacart, DoorDash, Pinterest, Binance, Zillow, etc.). It‚Äôs only a matter of time before smaller companies embracing it too.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 31,
        "person_name": "https://www.linkedin.com/in/qasimalitariq",
        "text_description": "DR. Waleed who got 29 GOLD MEDALS in MBBS.\n\nHow many of us know him?\n\nNo one, I REPEAT NO ONE.\n\nWe only know the Tiktokers, youtubers who make shit content and we praise them",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 32,
        "person_name": "https://www.linkedin.com/in/rinor-restelica",
        "text_description": "ChatGPT Plugins are now available to everyone.üëá\n\n1) Slack: Querying Slack\n\n2) Zapier: Interact with 5000+ apps like Google Sheets, and Docs.\n\n3) Expedia: Bring your trip plans to life in one place\n\n4) Klarna Shopping: Search and compare prices from 1000s of online shops.\n\n5) Vogue: Search through vogue articles\n\n6) TODO plugin: Manage a TO-DO list in ChatGPT\n\n7) Lowes: Find the right tools for all of your home building needs.\n\n8) Speechki: Just simply ask ChatGPT to turn your text into audio\n\n9) FigGPT: Design using Figma in ChatGPT\n\n10) Noteable: Create notebooks in Python, SQL, and Markdown to explore and visualize data\n\n11) KAYAK: Plan & book your next trip in ChatGPT\n\n12) LangChain Docs: Up to date info for the LangChain Python library.\n\n13) Weather report: Get up to date weather data on every city within seconds\n\n14) Crypto Prices: Get the price on any crypto\n\n15) NBA: Up-to-date NBA standings & Stats\n\n16) Qdrant: Plugin to search through Qdrants documentation\n\n17) Open Table: Search and get booking at restaurants anywhere, anytime.\n\n18) Zilliz Plugin: Search through your documentation and talk to it.\n\n19) Wolfram: Access computation, math, curated knowledge and real time data\n\n20) Pricerunner: Get the perfect shopping suggestions\n\n21) DesignerGPT: Generate a website within ChatGPT\n\n22) Milo Family AI: Turn any 20 minites with your kids into magic\n\n23) Chess: Play chess in ChatGPT\n\n24) Instacart: Order groceries from your near by store\n\n25) Send Email: Send the perfect emails through ChatGPT\n\n26) FiscalNote: Access real time data sets for legal and political purposes\n\n27) DAN: Change ChatGPT‚Äôs personality\n\n28) United Nations: Search through select UN documents\n\n29) Kraftful: Quickly get product-market fit, scale your user base and grow revenue\n\n30) Golden: This is a all in one plug-in that you can customize to your needs!\n\n31) Tutory: Access tutoring anytime, anywhere.\n\n32) Shimmer: Track meals and get healthier options for restaurants.\n\n33) One word Domain: Describe your business and get the perfect one word domain for it.\n\n34) Redfin: Do housing market research for your next house or investment.\n\n35) Portfoliopilot: This is your all-in-one investing guide.\n\n36) Zillow: Search listing, home details and more\n\n37) Giftwrap: Get gift ideas, shop them and ship it all in one platform.\n\n38) Tasty: Discover new recipes and meal plans that work for you.\n\n39) Hauling Buddies: Find trusted animal transporters in your area\n\n40) Tablelog: Find restaurants in Japan with availability at any moment\n\n41) Yabble: Create surveys, collect data and analyze.\n\n42) Algorithma: Vrtual life in a immersive simulator\n\n43) Vivian Health: Healthcare job in the area you want\n\n44) CreatiCode: Display scratch programs as images & write 2D/3D programs\n\n45) Argil Ai: Generate images in ChatGPT\n\nFollow for more üëã",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 33,
        "person_name": "https://www.linkedin.com/in/dtunkelang",
        "text_description": "Attention is all you need...or not? Will more efficient, sub-quadratic convolution approaches have the last laugh?",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 34,
        "person_name": "https://www.linkedin.com/in/sanyambhutani",
        "text_description": "LLMs for Data Augmentation! ü§Ø\n\nMassachusetts Institute of Technology has just published Dr. LLaMA which shows the promise of augmenting data using GPT-3.5 and 4 for domain specific models. This is a very simple trick that helps tiny (1.6B) domain specific model outperform a LLM. The authors take a look at Medical QA dataset:\n\n‚úÖ BioGPT 1.6B is tested against GPT-4, LLaMA-7B & Alpaca-7B\n‚úÖ ChatGPT rephrases examples of Medical QA as augmentation\n‚úÖ 1.6B model fine-tuned on new datasets outperforms everything\n\nThere is two more special learnings from the paper:\n\n1Ô∏è‚É£ GPT-3.5 isn't great at creating new QA examples, compared against GPT-4 because of its lack of training on Medical dataset.\n\n2Ô∏è‚É£ Alpaca-7B doesn't generalise well compared to LLaMA-7B implying instruction fine-tuned models struggle to generalise.\n\nLast week, I had shared the promise of using LLMs via CoT for distillation, today we've seen another simple augmentation example. Although the authors test their approach on a simple dataset. It's quite easily applicable to any domain.\n\nThe paper is a crispy ready, you can find it here:",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 35,
        "person_name": "https://www.linkedin.com/in/sudalairajkumar",
        "text_description": "Cohere has created an LLM University to learn about LLMs and NLP.\n\nThe course covers various aspects of LLMs including\n‚¶ø How do LLMs work?\n‚¶ø What are LLMs useful for?\n‚¶ø How can I use LLMs to build and deploy apps?\n\nLink:\n\nFull Stack Deep Learning announced their free LLM course last week. Read more about that here -\n\nHappy Learning!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 36,
        "person_name": "https://www.linkedin.com/in/eugeneyan",
        "text_description": "An insider's view on China's scale and tech, the 996 work ethic, and Alibaba's acquisition of Lazada. https://lnkd.in/gTQkBnEz\n\nYears later, I'm still boggled by the scale and how we had to use a completely different tech stack (spoiler alert: it's mostly Ali Java).\n\n---\n\nYea, there were one-click deploys, rollbacks, A/B tests‚Äîyou name it. Also, there were SQL commands that were both powerful and scary (in many aspects). Any data analyst on the street became a median data scientist.\n\n---\n\nThe work ethic was punishing. Burnout was common. While the Asians could bear it, folks from cultures that emphasized work-life balance suffered.\n\n---\n\nWe learned the Alibaba-Lazada was the biggest project then, directly contributing to Alibaba's i18n plans. A swat team was assembled to migrate in months instead of years.\n\n(In comparison, LinkedIn's migration to Azure was estimated to take ‚â• 3 years: )\n\n---\n\nOverall, it was a crazy fun experience and I learned a ton. Wouldn't be who I am today without it.\n\nHighly recommend reading/listening to this podcast in its entirety:",
        "url_links": [
            "https://lnkd.in/gTQkBnEz"
        ],
        "url_texts": "Home \nSubscribe To Podcast \nEpisodes By Date \nEpisode By Theme \nAbout \nDonate\n\n\nCORECURSIVE #082\nHome Subscribe To Podcast Episodes By Date Episode By Theme About Donate\n\nCORECURSIVE #082\nSoftware World Tour\nWith Son Luong Ngoc\nToday's story is from Son Luong Ngoc, who shares what it was like for him to work and live in many different countries around the world, including working for Alibaba at the Xixi campus in Hangzhou, China (Son's photos of the Alibaba campus).\nIt's a story of a software developer finding a place that fits them, a place that suits them.\nSon Luong Ngoc\n00:00 - Intro02:12 - Building Games03:26 - Night Classes04:09 - University of Waterloo07:30 - Back To Vietnam10:18 - Lazada 91113:22 - Double Eleven Day14:12 - The Problem18:02 - Enter Alibaba20:28 - Ali Java22:45 - Hangzhou, China24:00 - Langauge Barrier27:30 - AliPay Problems29:45 - China Scale32:34 - 996 in China37:02 - Ladder Climbing and KPIs39:41 - Singapore42:14 - Amsterdam42:40 - Work Life Balance44:34 - Reflection47:28 - Outro - Bonus Content From Son\n00:00 - Intro\n02:12 - Building Games\n03:26 - Night Classes\n04:09 - University of Waterloo\n07:30 - Back To Vietnam\n10:18 - Lazada 911\n13:22 - Double Eleven Day\n14:12 - The Problem\n18:02 - Enter Alibaba\n20:28 - Ali Java\n22:45 - Hangzhou, China\n24:00 - Langauge Barrier\n27:30 - AliPay Problems\n29:45 - China Scale\n32:34 - 996 in China\n37:02 - Ladder Climbing and KPIs\n39:41 - Singapore\n42:14 - Amsterdam\n42:40 - Work Life Balance\n44:34 - Reflection\n47:28 - Outro\n- Bonus Content From Son\nTranscript\nIntro\nAdam:\nHi. This is CoRecursive and I'm Adam Gordon Bell.\nDo you know the song, The Safety Dance by Men Without Hats? It's like (singing).\nEarly in my career, the second developer job that I had, it was in 2008. I worked at a web agency. It was a smallish office and they played music in the office over speakers, mainly, I think, to drown out the people on the phone, support people, salespeople. The owner picked the station and I think it was beamed right from 1982 because it was all '80s pop hits all the time.\nI know this isn't true, but in my mind when I remember it, it was 50% or more The Safety Dance by Men Without Hats. I still have no idea what that song is about, but I left that job at some point and I went to Operitel. It was a software product company and the first day, I noticed how quiet it was. It was so nice.\nThe web agency was culturally like a sales place, but Operitel, it was quirky and it was nerdy. I could tell right away, I just fit in better there. But maybe I wouldn't have known it without The Safety Dance, without hearing that song so much.\nThat's what today's story is about. It's not a story of a piece of software being built, it's a story of a software developer finding a place that fits them, a place that suits them. It's also going to be a bit of a world tour where we spend some time in various countries, including Canada and China and, I don't want to spoil too much, but elsewhere.\nBuilding Games\nAdam:\nCan you tell me who you are and what you do?\nSon:\nYeah. My name is Son Luong. I contribute to various different open source project including Bazel. I have some contribution into Git and GitLab itself and yeah, that's a short summary.\nAdam:\nThe earliest glimmer of Son's career started with a lie. He was in high school at a friend's house and the friends showed him a game.\nSon:\nI think the name of it was Bounce or something where you are a ball and you were just trying to get from one platform to another and eating candy or something like that, getting points.\nAdam:\nHis friend claimed that he knew how to make games and that he could make this game.\nSon:\nHe was completely BSing by the way. He cannot make that. You need a graphic team with images to build that game. He alone cannot build that.\nAdam:\nSo Son tried to make his own game.\nSon:\nAnd I went home and I just keep remember like how the hell do I make anything work with this Visual Basic program, trying to drag the boxes and drag the button? This button is not the game that I'm looking to build. I can click it and create like a hello world. I can click it and create a text game where you answer yes or no and then choose your path, but that's not cool.\nNight Classes\nAdam:\nAnd so Son started taking some night classes in computer programming.\nSon:\nWhere they teach you how to make computer program like web server and stuff. I remember the teacher there was like, \"Man, you're in grade 10. Why are you here?\" My classmate were all final year university students and people who already have a job and trying to learn more. I was the only high school student there.\nAdam:\nThe class was fun, but making a game still seemed like a mystery. So when Son finished high school, because he was good at math and liked the idea of building a game, he went to the University of Waterloo in Waterloo, Ontario in Canada as a foreign student. Waterloo was a culture shock.\nUniversity of Waterloo\nSon:\nSitting next to me is one Asian guy who was born and raised in Canada and he was using Vim and LaTeX to take notes. I was like, \"What the hell is LaTeX? What the hell is Vim?\" And sitting on the left-hand side of me is an Indian guy who's taking a double degree program in both science and pure math. The guy end up being a doctor. I remember a conversation on first year. He was like, \"Oh, did you buy this stock?\" and I was like, \"What? You're just turning 18, dude. How the hell do you know anything about stock?\" Turns out his parent taught him all this stuff. I remember on third year, I met a couple who, after the final exam, was going to the library to borrow math books to go back and study after the final exam.\nWhat the hell are you studying for? I was like, man, that entire environment was way too competitive. Coming from Asia, I was not expecting people to be that active. Learning in Asia was something very passive. Whatever you are given to by the teacher, you learn that and that's it, but in North America and I think in Europe, if you were to learn something, then you must have a passion for it because you have a choice of choosing to learn that topic. So once you have a passion for it, it means that you enjoy learning about it and you would spend a lot of time self-studying about it. I did not have any of that. Even though I love math, I didn't love it that much. I just love it simply because I was good at it.\nAdam:\nThis was Son's first lifestyle adjustment and there's going to be more, but finding his place at Waterloo was a challenge. It's a very serious school, but Son found a way to make it work.\nSon:\nI remember I was having a really interesting conversation about, hey, what do you think gravity is? And people was actually replying to you like, \"Okay, here are the theoretical physics about gravity.\"\nThe university taught me two things. One is obviously, the computer science stuff is quite cool, but what it taught me is I need to value this environment. When I enter a competitive environment, I should not be competitive, but I should befriend all these people because turns out later in life when I look back, I cannot find the same quality of people, the same smartness, the same speed that I was able to find in Waterloo.\nAdam:\nThere's a path from being a foreign student in Canada to becoming a permanent resident and that was Son's goal, but all his friends who are Canadians seemed to be getting jobs in the US at Google or at Facebook or wherever. In Canada, we'd call this the brain drain and maybe with remote work, this'll slow down a little bit, but for Son, with his friends gone, he felt trapped in Waterloo and in Canada.\nSon:\nSo I was getting really depressed. I was starting straight for eight years without seeing my family at all, so no video calls, no air flight, nothing like that. So I decided, okay, I need a mental reset and this mental reset mean that I should go back to Vietnam and try to find something.\nBack To Vietnam\nAdam:\nAfter the quiet life of a math department and a small city in Canada, being back in Vietnam seemed like the future.\nSon:\nA lot of expats, French, American, Russian, everybody was moving to Vietnam or another Southeast Asian country because they like life over there. The weather is wonderful, beaches. If you ever had a chance to take a vacation, I definitely strongly recommend Southeast Asia.\nAdam:\nDuring this time, at least in his head, Son was still keeping his game dev dreams alive.\nSon:\nI begin in Vietnam starting to think like, hey, after one or two years of working there, I can collect enough money to open up a game company.\nAdam:\nTo build his bank roll, he started a job as a business analyst at CSE.\nSon:\nThey were making insurance type software for various different companies and turns out back in 2016, hello, insurance tech company was still running on COBOL. Turning that into Java is a hugely profitable and yeah, we were having deal with various different insurance company.\nAdam:\nAt the insurance company, Son started to see a problem with this game-making plan.\nSon:\nWhen you get into insurance, you start learning the value of reliability, the value of being able to assure something. People pay serious money for that and I was like, why do all these people who are obviously very smart able to afford a lot of money to buy these insurance packages? Why do they value this so much?\nAdam:\nAs a Waterloo math grad, Son easily had the skills for understanding insurance and actuarial math, and it all started to make sense to him.\nSon:\nThere's some value here. You don't need to gamble everything, but there are percentage and there are value in safety net. You can actually quantitatively calculate that using an actuarial model behind all that insurance and then all that math from Waterloo start kicking it. I was like, \"Oh, this all makes sense now. Okay, I can actually model out my life and my income and all that in a much better way than just gambling everything away.\"\nAdam:\nAnd through this new lens, Son's plan to start a game dev company, it just didn't seem to make sense.\nSon:\nI was like, \"Oh, shit, that is actually gambling and there's a lot of risk into all that and I should not be pursuing that if I want to have a sustainable life, a healthy life.\"\nAdam:\nSo Son starts learning more software development, but he has a different perspective now because it's not about gaming, it's just about building things and learning. Maybe being a developer can be his calling if he just throws himself into it.\nLazada 911\nSon:\nI had a lot of time to study and I was learning AWS, all that good stuff and I landed a job in Lazada, onto a team called 911. You can guess what that team do, emergency support for the tech platform. So Lazada is an e-commerce company in Southeast Asia. It's either number one or number two at the time. I think they were number one back in 2017 and they were selling in six different countries.\nAdam:\nPopulation wise, Southeast Asia is huge and diverse. It includes Vietnam and Thailand and Laos, but also Singapore and the Philippines, Malaysia, Indonesia and more. It's got 700,000 people and each of those countries, people speak a different language or have different customs. Lazada was serving that market out of their main office in Vietnam. A few hundred people worked there.\nSon:\nI think at least 200 technical and then we have the business side as well. I remember we had at least three floor on a building and I think each floor was carrying around hundred people.\nYou get free flow of drinks. You have a cappuccino machine, but you don't need to work long hours unless you don't want it to. Plus it's like to work in Vietnam, very competitive, very cutthroat, competitive and I think it's not solely for Vietnam, but it's just like that for Asian countries in general.\nThe tech stack is a PHP Monolith was being converted to microservice Golang running on Kubernetes. Back in 2017, this is very modern, moving really fast.\nWe have an R&D center in Moscow. We have an R&D center in Vietnam and a data engineering center I think either in Thailand or Singapore or both. We have three different data center that we manage by ourself.\nAdam:\nThe 911 team, well, it had a clear job.\nSon:\nReceiving escalation from business side, because we have business entity in six different countries. Including Hong Kong, it's seven.\nWe were receiving escalation from business entity and then we escalate that over to tech side and the job there was like to speed up the troubleshooting. So instead of 911 having to forward the ticket to the development team, then 911, just fix it. Just go there, find what's wrong, fix it immediately. I had my hands on all over the system and yeah, I was learning to be what now they would call an SRE back before Google released the definition of SRE.\nBut yeah, with the background in insurance, I was immediately able to recognize why business was valuing this, because they want that stability, they want that reliability and yeah, it is just a very interesting way for my previous job to somehow met the criteria of my new job and it was very fun. It was a very fun time.\nDouble Eleven Day\nAdam:\nFor an e-commerce chain in Southeast Asia, the biggest day of the year isn't Black Friday. It's something else.\nSon:\nOh, yeah. Back in Southeast Asia, folks in North America and the European country don't know this, but Southeast Asia is heavily influenced by China and China have a big sales on Double 11 Day. So on the 11th of November, it's the biggest sales in China. It's similar to Black Friday in North America or Boxing Day in Canada. So Double 11th is also the biggest sales in Lazada at the time. We open up sales at midnight for all the countries and before that sales coming in, we had to do stress testing for month before that in able to prepare for the load.\nThe Problem\nAdam:\nBut on the big day, a problem occurred.\nSon:\nThe midnight of the 10th of November to the morning of the 11th of November, we detect the problem 30 or 20 minutes in. We were like, \"Oh, we had a lot of order in Malaysia. Yay! Wait. Why everything stuck in pending payment? Wait a minute. Something is wrong here. Go back and check.\" Oh, turns out everything was being invalidated by payment processor. Turns out on the night, our credit card processor, a third party credit card processor in Malaysia just went down. They just completely given up. All the transaction we had sent over there was completely disappearing. When you build up an e-commerce platform, you cannot build everything. You cannot build the e-logistics side. You cannot build the payment. You cannot build the notification part, so you need to use third party. When third party integration goes wrong, you need to have a backup plan for it.\nAt the time, we don't have one for Malaysia. We had a war room going on in the 911 room and everybody was there, the payment team, the CEO, the CTO. Everybody was in the room and I had to scan the logs to see what's wrong. The CEO and CTO was looking at me like, \"Hey, look at the lock and see what's wrong,\" and turns out it was payment. All these was way before event-driven microservice. We had to repair text lock or JSON lock into some sort of pseudo CSV file and then we have to manually data fix the database and then replay all that state. Turns out if you turned off all the cron job that invalidate the order that was not paid for one hours, every other cron job also die in the system.\nSo yeah, the system back then was not at the best state, but that's how we keep all the orders. The revenue was huge and yeah, it was quite fun. It was quite a night. In the morning, we end up drinking a lot of Red Bull and then everybody was sleeping all over the place. We see the CEO was sleeping over there. The CTO was sleeping over here. Yeah, it was quite a night. I think it was 2017, I think. The 2018 has got a completely different story.\nAdam:\n2018 was different because Alibaba got involved. Alibaba had bought Lazada in 2016, but it had mainly left it alone. Southeast Asian software companies were used to this: to being left to do things their own way.\nSon:\nWhen you're working in Southeast Asia, you need to move fast, but at the same time in Southeast Asia, you have six different countries speaking six different language with six different culture. How the hell do you move fast? That's very difficult. Instead of building one website, now you have to build six different website with eight different languages and at the same time, you have to deal with regulation of six different countries. For example, Indonesia, they have laws where similar to GDPR where the data cannot leave the border of the country, the physical border of the country. So now you have to go into the country and build a data center, and this is back before AWS was a thing over there. So you have to go in there. You need to establish a data center and then you need to deploy a different stack, an isolated stack compared to the rest of the country. Then now you have to handle data synchronization between different tenants in your system, different data center. It makes the problem a lot more complex and overcoming this complexity get you ahead.\nAdam:\nIt's reasons like that that Uber and many other hypergrowth companies lost money going into Southeast Asia. It's not just American companies who are out looking for growth. China has tech giants as well and Alibaba is a big one.\nEnter Alibaba\nSon:\nAlibaba is the owner of Tmall and Taobao. That's the equivalent of Amazon. One is C2C and one is B2C. They own Ali Cloud. If you want to do business in China, you need to host that somewhere and that somewhere is Ali Cloud. Amazon footprint over there is very small. The comparison of Baidu and Tencent are very small. The most reliable one is Ali Cloud. Most of the major banks are tenants of Ali Cloud. Most of the major banks out there are hosting their services on Ali Cloud, so they were very big. Aside from that, they do various different business. For example, Youku is a video streaming platform that's the size of YouTube. They also operate something that most people don't know about called Cainiao. Cainiao is a logistic platform that handle all the shipping so stuff that you are buying on AliExpress today or even on the Amazon today most likely, that will ship over from China to your country to your port with Cainiao. That's a global logistic platform that they acquire. That's very big. They were all over the place. They were really big. They are a big tech giant.\nAdam:\nThe acquisition caused some friction at Lazada.\nSon:\nThe company made the decision sometime around 2016 and that's very early to move to use Go and Kubernetes instead of deploying on bare metal VM. So everything was very modernized and high volume, but was not that high compared to the like of Google or Amazon, but definitely, it was one of the highest [traffic] services at the time in Southeast Asia.\nThere were really a lot of change and change come with uncertainty and that's innovation. The rate of innovation was really high, so the platform in turn becomes a little bit less reliable. That's just a business risk that the entire group willing to accept. But once Alibaba came over, these folks are highly experienced. They are used to handling 100 time, 1,000 time higher traffic without sacrificing that resiliency. So they decide, \"Hey, we want the system to be more reliable. Let's re-platform everything to something that we know that going to work.\"\nAli Java\nAdam:\nFor some developers, switching tech stacks was a big ask. It's throwing away a lot of knowledge, and Alibaba stack is all Java and Spring.\nSon:\nEverybody was like, \"Java sucks. We like Golang. Get the hell out of here. Where's your Kubernetes? Oh, you don't have Kubernetes? Where's your Docker? Oh, you have your own Docker? Nah.\" But it turns out, there are good stuff in Chinese tech that worth learning about and I think some of that, Westerners still don't know about today. It was quite an eyeopener.\nAdam:\nAn acquired company being forced to take their working software and re-platform it to however the parent company does things, it's a classic failure mode of acquisitions. The re-platform will take years and you'll end up with worse solutions because of assumptions built deep into how the acquired company did things. But in a bigger way, re-platforms just fail because they end up being bigger investments than the company thought they were going into it. And here's the thing about Alibaba. Alibaba was not afraid to invest in Lazada.\nSon:\nThat's just something that the company decided and they said target and they actually put quite significant investment into it. Later on, I learned that this project is actually the biggest project in Alibaba at the time. What they call it at the time was a C-level project and a C-level project, meaning that the CEO of Alibaba was actually quite invested into it and they managed to extract all the talents from all different business unit in Alibaba into a special team to make this happen. Other time I couldn't understand why the hell would you invest in Southeast Asia? But later on when I start working for Alibaba, I start realizing, oh, this is actually a much, much bigger picture than I ever imagined. Oh, my God. Turns out it related to a long-term planning that pivoting with the Chinese Belt and Road Initiative and yeah, it was way bigger than we ever imagined. It all makes sense after we learned about that, but before that, everybody was like, Java sucks.\nHangzhou, China\nAdam:\nEveryone at Lazada starts learning about Alibaba-flavored Spring. For Son, this was a chance to learn about a whole different world of software development, how things worked at the scale of Alibaba Group.\nSon:\nThey flew a lot of us to China to work with the team over there to learn the tech stack in Alibaba as well as to recode everything using the same business logic, but in Java, Spring Java of Alibaba.\nAdam:\nAlibaba does things their own way. They have their own JVM. They have forks of Git. They have forks of MySQL. So there was a lot to learn, but Son's team was a SRE team. They didn't own any code.\nSon:\nFor 911, we merged with a team called GOC, Global Operations Center, in Alibaba. Turns out GOC is the overseeing eyes of the entire Ali Group. It was essentially a thing of SRE team in Google is the centralized SRE team in Alibaba who measure all the monitoring system and handle all the escalation from different business unit and routed back to tech side.\nLangauge Barrier\nAdam:\nSo now, Son and his team are SREs for Alibaba Group, but there's a problem. The internal software used is all in Chinese, which Son can neither read nor speak. So his new project involves a lot of translation.\nSon:\nSo all the UI, all the system was in Chinese. All the documentation was in Chinese. We were over there trying to build new features and it was quite fun. It was quite an experience. I get to spend one month in Hangzhou, in Alibaba campus and boy, it was quite impressive.\nAdam:\nSo is there a language barrier?\nSon:\nOh, yeah, definitely. They put us in a hotel and we don't speak the language, so we couldn't order foods. We were ordering stuff by pointing fingers. Turns out you cannot get Google Translate in China by the way, so putting up your phone is not even an option. Later on, I learned that, oh, Microsoft Translate actually work, so I need to download Microsoft Translate instead, but it's still quite difficult to communicate in China and that's just living. Working is even more difficult. Everything need to go through translation, all the documentation. And sometime, the translation would be wrong, and then over time, you need to learn the dialect of the translation. For example, they'd like to translate a use case into something called scenes, like scenes in the movies. Every time you want to talk about use cases, they would translate it into scene. I was like, \"What the hell is a scenes?\" Turns out later on, I was like, \"Oh, they meant scenario.\" So over time, you just learn, pick up little, little things and everything start making a lot of more sense. But yeah, everything was very difficult.\nAdam:\nYeah. But how did it work to code if you were working on this ticketing system, but the other guys didn't speak English or Vietnamese?\nSon:\nSome of them do speak English, but not at the level where you can communicate technical with them. We have to let the code speak for itself. Now, fortunately enough, I was blessed with a very talented teammate at the time and he was able to navigate through the entire full stack ecosystem from ReactJS down to Spring Java and he was impressively reteaching me a lot of that. But the app itself was quite simple. The thing about ticketing system is your CRUD and then you can attach several metadata. You have a state machine to handle different statuses. The most difficult part there is integration. Alibaba is a huge ecosystem. You have a lot of thing to integrate.\nFor example, you don't have Slack over there. You have DingTalk. DingTalk is a chat system of Alibaba. They also have what they call it, a change management system so that all the changes in Alibaba, all of the deployment, all the network changes can be automatically revert by one click. The moment when you deploy and some things go wrong that deviate from the metric baseline that the AI engine put out, then you have an option to click one button and revert all that change immediately. From database schema to deployment, to migration, everything, network changes, one click of a button. But yeah, it was a lot of fun.\nAliPay Problems\nAdam:\nThe team Son merged with was also ops for Alipay, which became its own problem.\nSon:\nIn China, if you live in a megacity, people don't accept cash. When we go over there, we have a really hard time paying for food because people would only accept either WePay or Alipay. We couldn't go out and buy bubble tea because people will like, \"Oh, that cash? I don't want to bother having to go to the bank and put this into the machine. That is just too much work. Alipay do.\" We were having to beg our teammates so that they would exchange cash that we got from Vietnam, and these are Chinese cash, it's not Vietnamese cash, we were begging them, \"Hey, could you take this to the bank and then just transfer some of your Alipay credit?\"\nAdam:\nSo yeah, using Alipay was a challenge, but a bigger challenge was just the scale that the Alibaba group operated at.\nSon:\nThree rooms away from my room is a network operation center. That's where you have the big TVs with all the graphic. A Chinese team was sitting in there watching all the alerts, watching all the graph. It was very cool that you can see the all of throughput of all the platform, all the traffic, all the networks, and boy, is it massive. We were very proud of the traffic that we have in Southeast Asia. It's not even a drop in the bucket compared to China. Turns out when you have billions of people speaking the same language watching the same live stream, having the same culture, turns out that create a huge traffic spike.\nImagine you put a laptop on sales for $100 and you have 10 stock. Often in time, when you build an e-commerce system that would be one row in your SKU database, the stock keeping unit, and then that one row when somebody order it, it will change the status to sold. What happen if 10 billion people were trying to place an order on a single row in database? How do you design a system for that?\nChina Scale\nAdam:\nIt's an interesting problem. How much is reliability worth at scale and how much does it cost? How do you build for 11/11 day when you have a billion users all at once?\nSon:\nThings was massively impressive in China, the way they have to re-architect things in order to accommodate for the lot that they were having. I just don't see the same thing in Europe. I don't see the same thing in North America. Even the population in North America, like US and Canada combined, it's not even half the population of China. The lot we are talking about here is massive. The scale is completely massive and they don't have access to Google. They don't read Stack Overflow. They have their own stuff. When I was over there, I had to learn all of my own stuff over there and I was like, oh, my God. They just went completely different route. We were very fascinated with Golang and Kubernetes. This guys were Java, Java, Java, Java, Spring Java, Spring Java. They're just completely innovating without us knowing in Western world. Nobody was discussing how China was building stuff.\nTurns out they fork their own MySQL. They were forking their own JVM. They fork their own Nginx. They fork Git so that it would use their internal object storage platform instead of on disc. So a Git hosting everywhere in GitHub, GitLab require you to have a disc, a physical hard drive, but this guy was forking it, inserting libgit2 into it and then let libgit2 talking to their internal implementation of S3. They were able to scale their DevOp platform to a cloud native scale before anybody else can. Even today, GitHub and GitLab still struggling decoupling storage and compute and this guy already have that. Similarly, they do the same thing with MySQL. They build on top of the knowledge they learn from Facebook about forking the storage engine underneath MySQL to RocksDB and then this guy were running into problem where RocksDB is an LSM-tree database storage engine.\nThe problem with that is that the compaction of these different layer of logs was getting quite slow and consume a lot of CBU. So guess what they did? They offload that computation to an FPGA. These guys were running custom hardware, custom chips so that their database can handle the lot better and that's just massively impressive. That's just not something that you can find in any tech company. Your custom chips like, what the hell?\n996 in China\nAdam:\nAnother surprising thing at Alibaba was just how much everybody worked.\nSon:\n996! When I went over there, everybody told me about 996. You work from 9:00 AM to 9:00 PM six days a week, so Monday to Saturday and that's pretty much the life. Over there inside the office it's very common for you to see beds, even camping tents. Yeah, no, seriously. In Alibaba, in the hallway, you would be able to find several camping tents, gender-separated so that folks can go to sleep, not at night, but middle of the day after lunch. Sleeping in Asia is quite common, but especially during campaign night, like Double 11th. People stay over at the campus all the time.\nAdam:\n996 works out to 72 hours per week, but don't worry, nap time's included. At first, Son didn't like or understand this at all.\nSon:\nBut later on, I start realizing why it is the way it is. If you are a tech company and you tell people to work 9:00 to 9:00 six days a week, that's not sustainable. Nobody going to work for you. So be able to tell people to work from 9:00 to 9:00, six days a week and people still wanting to line up to work for you, there must be something special there. And turns out later on when I start talking to my Chinese teammates over there, they start telling me stories, stories that make quite a lot of sense. Think about it like this. What do you do after you gain a stable job? You get married. Before you get married in China, my friend was telling me this, you need to have at least a house and a car. If you go work for Alibaba, because they also co-own a lot of Alipay, which is a financial company that people were using instead of buying in cash, they also do loans. They can loan to you your mortgage at a very low interest rate.\nThe more senior you are in Alibaba, the better interest rate you get. Similar to car. You can loan from the company to buy a car and that the more senior you are, the better interest rate you get. That's not the only thing. Turns out if you work for a big company, your credit score is very high. So credit card provider would actually try to line up outside of the company so that when you left, they can hand you flyers. Here are free money, please loan from us. So that's the first thing. The second thing is the lifestyle on campus. Everybody was eating in the cafeteria. They have nine different buildings and four different cafeteria in the nine buildings. The food was being mass produced so they were able to produce that very cheap, so they were able to provide these food to the employees very cheap.\nAdam:\nNow I haven't worked at a big tech campus, but some of this is starting to sound not that dissimilar from a Meta campus or a Google campus. Son took a lot of photos while he was there and if you want to see some of them, I've shared them on Twitter, tents to sleep in, massage stations, places to eat, and just the scale of things, but yeah, people weren't heads down working all the time and the company had lots of perks to offer them.\nSon:\nWork day end at 6:00 officially, but the food between 6:00 and 7:00 PM would be sold at discount. So employees are incentivized to stay on campus after 6:00. Then at 9:00, they could sell you free snack. So single employees who have nothing to do at home can stay on campus until 9:00 to get those free snack. And taxi, because Alibaba was owning that taxi company, if you go home, now you get free taxi. Oh, my god. Now you start seeing all the integrations start coming into play. Hey, this guy actually provide a lot of incentive for people to work 9:00 to 9:00. Then the next thing you know, if you need to go out and buy anything, hey, Tmall and Taobao would deliver that to the doorstep of your building. You get fastest shipment on campus compared to going home.\nSo you would route all of your e-commerce purchases to the company address. Just tell what building is it and then go down and get it. So it's more convenient to live on campus compared to going home. People was getting all the perks everywhere that affecting their life. They're willing to work for it.\nLadder Climbing and KPIs\nAdam:\nAll these perks means a lot of people want to work there, and then the higher you climb the corporate ladder, the more perks they are, which leads to the fact that internal competition can get pretty intense.\nSon:\nThey're willing to compete for the jobs and that's why Alibaba had a very cutthroat culture so that they can digest out low performers every years. I think 30% of the low performers are digest out. Everybody was competing for the top because turns out, one more perks, which is your final bonus, your yearly annual bonus can be up to 100% of your salary.\nAdam:\nOh, wow. That's huge.\nSon:\nSo people was competing to be the top performers. This guy know what they were doing.\nAdam:\nHow do you manage a giant company of software developers who all want to work really hard and all really want to advance their careers? Well, maybe you already know the answer.\nSon:\nThe way Alibaba manage is very top-down, very KPI-heavy. Every business unit are given a certain KPI. The leader of those business unit would assign KPI to each of the sub-department and each of the sub-department would assign KPI down to their employees. They have metrics to meet. This guy were very metric-driven.\nAdam:\nThis KPI or OKR system, much like any top-down system thrown at motivated people, leads to some unintended consequences. For instance, porting Lazada to the internal platform was one thing, but actually running the company, maybe that was a less exciting project.\nSon:\nThis is hearsay, so I'm not saying this is a source of truth, but there were rumors to be like they think Lazada to be a not profitable place for them to meet the KPI, so nobody wanted to be a CEO of a failing position. Lazada actually changed CEO quite a lot because nobody was meeting their KPI. The culture there is quite cutthroat and people were competing for nice position so that they can be set up for success.\nAdam:\nThere's a way it sounds very foreign. There's another way that it sounds like Google.\nSon:\nWell, I cannot tell you because I have never worked for Google, but what I can tell you is that they move very fast and I don't think anybody has been moving as fast. Especially if you work in a Western country, in European, or in North America, you are not moving fast. This guy move a lot faster.\nHello, I make CoRecursive because I love it when someone shares the details behind some project, some bug, or some incident with me.\nNo other podcast was telling stories quite like I wanted to hear.\nRight now this is all done by just me and I love doing it, but it's also exhausting.\nRecommending the show to others and contributing to this patreon are the biggest things you can do to help out.\nWhatever you can do to help, I truly appreciate it!\nThanks! Adam Gordon Bell\nSingapore\nAdam:\nOkay. So Alibaba and China move fast and that can be exciting, but for Son, it had downsides.\nSon:\nAlibaba obviously providing a lot, but I was interested in learning what else is there and I was a little bit burned-out with all of the Google Translate of Chinese documentation. I was looking for other opportunities and my boss was like, \"Hey, I'm going to the exact same company but in Singapore.\"\nAdam:\nSo Son stayed working for Lazada, but moved to Singapore where his time on site made him the expert for his team.\nSon:\nThen I get to teach my teammates about Alibaba tech. They were using Python at the time, a lot of Python, Django, Flask self-hosted on Ali Cloud and I was like, \"No, guys. Here's Ali Java. Here's all the goodness. Hey.\" Imagine Spring Java, but everything come with battery-equipped. You want a database? Here's a database. You want a message queue? Here's a message queue. You don't need to worry about infrastructure. Everything came out of the box, and that's was actually a downside because I start to realize that the world outside was moving quite fast. Even though I was quite an expert in internal technology in Alibaba, I know nothing about the world outside.\nAdam:\nSon wanted to keep growing and learning and not pigeonhole himself into one company, into one tech stack, so he set a deadline for himself.\nSon:\nSo I set up a plan that after I arrive in Singapore for a year, I would need to learn new technology and explore the job market. At the time, I was actually interviewing with Google in Singapore. I attend several of their tech talk, get to talk to the G Pay team over there in Singapore. They were doing a lot of interesting things.\nAdam:\nGoogle famously has a super long hiring process and in the meantime, Son found an opportunity to do something completely different.\nSon:\nAt the time, I was getting to know my girlfriend at the time and now my wife. I was like, okay, I need to settle down someplace and good luck buying a house in Singapore. Singapore was way too expensive. Your best luck is being able to rent a nice apartment and that's pretty much it. You would not even get a car. It would cost you millions to get a cars and that's insane.\nAmsterdam\nAdam:\nSon got an offer from Booking.com. Booking.com is an online travel agency based in the Netherlands in Amsterdam, and in some ways, Amsterdam is the opposite of Singapore.\nSon:\nI took that opportunity and said goodbye to Singapore even though I love Singapore so much. It's such a nice country, wonderful, strongly recommend.\nAdam:\nWhere's the food better?\nSon:\nOh, Singapore, definitely. Netherland is like the worst. Oh, my god. No, seriously. Even Dutch hate the food in the Netherland. So I left Singapore for the Netherland August 2019, and then me and my wife officially get married on the cusp of COVID.\nWork Life Balance\nSon:\nI remember my product manager on the first day I came to work. He sit me down with my team lead. My team lead was actually joining with me. We did the orientation together. My product manager sit both of us down and explain, \"Hey, guys. Booking paid a lot of money to relocate you guys here. We want you to success here. That means you need to prioritize your life first. You need to be able to find a house to rent. You need to be able to do all your paperwork. So in the next upcoming month, if you need to take time out, just go and then come back and tell us later,\" and that is such a huge opener because that's just a mental shift, completely 180 compared to Alibaba.\nBecause Alibaba, everything was work, work, work. Your life is in the company. Your metric is important. Booking was not like that. Booking was like, \"No. We respect you as a human. We know that you have a family. We know that you have needs. Take care of them and then come back to us. Work your best,\" and that's just changed everything. In Booking, I have teammates who are blind. I have teammates who are gay, lesbian, disabled. We were all able to hang out together. No problem. There's no discrimination. It's just a huge, open, and welcome culture. Very diverse pool of folks over there. I just love it so much. We still have massive traffic. We still have problem at scale that we have to take care of and yeah, it was very fun trying to find work-life balance in midst of all that, in midst of COVID, in midst of relocation, and in midst of new marriage as well.\nReflection\nAdam:\nSo for now, Son has found his place in the world. He and his wife are still in the Netherlands. He found his home and a place where the work culture works best for who he is, but he only knows that because of the experiences he had. If he had started at Booking first thing, maybe he wouldn't have realized what he had because he'd have no comparison. That fact shaped the advice that he offers to others:\nSon:\nLife is diverse. Before I get to work in China, it was a black box to me. It was not occurred to me that there was a payment system that's so advanced that people would refuse using cash. Yeah, I know credit card exists, but that doesn't mean when I hand you cash, you will refuse to receive it. Just at the same time, going to Southeast Asia, seeing various different startup trying to get themself a place on the map was hugely game-changing because it enabled me to think in term of business, in term of thinking, how would I operate this if I were the CEO or CTO? I also learned how to party. In Waterloo, they drink bubble tea. You can get way better party going to Toronto than trying to do it in Waterloo. Going back to Vietnam, every one week or two weeks is a drinking party.\nA lot of beers we consume, a lot of hot pot. The food is wonderful over there. When I go to Singapore, I learn what planning can bring. You can just enter the city and you can see. The first thing you see in Singapore is the best, most beautiful looking airport in the world. You can see like, \"Oh, my God. How much would it take for my government to build this airport?\" Infinity money would not change this because this is not a problem with money. This is a problem with planning. Then Netherland, they taught me to be open-minded. They taught me about work-life balance and all that is just come back to be travel more.\nThis is not an advertisement for Booking.com, but travel more and learn more from it and keep an open mind, because yeah, maybe Western media can tell you a lot of shit about China, about how evil things can be, but try to go over there and try to see how people live. Being able to see thousands of people eating lunch all at once surrounding you or looking like you, because I'm Asian, make you humble. It does have a humble feeling entering that cafeteria and see hundred of people lining up for the same dish. It make you feel insignificant. It's a massive world out there. Travel more.\nOutro\nAdam:\nThat was the show!\nThank you so much to Son! You can find him on Twitter. I'll put a link on the episode page.\nIn an upcoming bonus episode for podcast supporters, Son is going to share his views on the future of Chinese software development and a little bit about the Chinese expansion via the Belt and Road Initiative.\nIt's super interesting stuff!\nA lot of the innovations that Alibaba is doing are out there on GitHub and they're open sourced and they're for the taking, but outside of certain system researcher circles, it feels like nobody's paying attention. That's going to change. So support the podcast and on Patreon, you can get access to that episode that I'll release in the future.\nAlso, Son's career keeps evolving, so you'll hear a little bit about his latest career pivot, which is how I ended up meeting him. Also, if you like to hear a little bit of the fun backstory behind the episodes, then this'll be a great opportunity to hear that.\nUntil next time, thank you so much for listening.\nBonus Content From Son\nSon: I think just words could be pretty dry, so here are some visual aid from my trip to Alibaba Campus in 2018.\nSon: Here is the main campus map. U shaped with the main residence of Jackma being in the middle of it (the grey part)\nSon: Here is the campus by day:\nSon: Here is the campus at 7-9PM, 9 buildings fully lit on all floors:\nSon: Here are the tents in the hall way, gender separated for folks to sleep in whenever.\nSon: For satelite campuses, offices would have bed like this instead of the tents:\nSon: The entrance to each building is equipped with badge scanner, facial scanner was newly added in summer 2018:\nSon: On campaign days, there would be maids handing out swags to engineers:\nSon: or free massages from professionals\nHello, I make CoRecursive because I love it when someone shares the details behind some project, some bug, or some incident with me.\nNo other podcast was telling stories quite like I wanted to hear.\nRight now this is all done by just me and I love doing it, but it's also exhausting.\nRecommending the show to others and contributing to this patreon are the biggest things you can do to help out.\nWhatever you can do to help, I truly appreciate it!\nThanks! Adam Gordon Bell\nA podcast about building software.\nFavorite EpisodesSubscribe to PodcastSubscribe via EmailFollow @CoRecursive on TwitterAbout\nFavorite Episodes\nSubscribe to Podcast\nSubscribe via Email\nFollow @CoRecursive on Twitter\nAbout\nSon's Blog DragonWell JVM Belt And Road Initiative\nSon‚Äôs Blog\nDragonWell JVM\nBelt And Road Initiative\nRelease Date: 02 Nov, 2022Updated Date: 02 Nov, 2022Download Audio FilePermalinkRSS\nRelease Date: 02 Nov, 2022\nUpdated Date: 02 Nov, 2022\nDownload Audio File\nPermalink\nRSS\n2x  1.5x  1x  0.75x\n2x\n1.5x\n1x\n0.75x\nSoftware World Tour\nPodcasts List\nPodcasts List\nFrom Project Management to Data Compression InnovatorMay 02, 2023 59min     JSON vs XMLApr 03, 2023 49min     Sun's Mobile BlundersMar 02, 2023 51min     Shipping Graphing CalculatorFeb 02, 2023 46min     The Unfulfilled EngineerJan 02, 2023 42min     DOOMed to Fail: A Horror StoryDec 02, 2022 45min     Software World TourNov 02, 2022 48min     Android's Unlikely SuccessOct 03, 2022 60min     From Prison To ProgrammingSep 02, 2022 46min     CPANAug 01, 2022 56min     The History and Mystery Of ElizaJul 05, 2022 44min     Why still 80 columns?Jun 01, 2022 39min     LISP in SpaceMay 02, 2022 38min     April Fools' Is Cancelled (2014)Apr 01, 2022 38min     The Story GraphMar 02, 2022 48min     Serenity OSFeb 02, 2022 41min     The Internet Is Made of Duct TapeJan 02, 2022 42min     Cocoa CultureDec 02, 2021 44min     Leaving DebianNov 02, 2021 40min     The Original Remote DeveloperOct 04, 2021 41min     Quines, Polyglot Code and Other Fun ComputationsSep 02, 2021 61min     Full-Time Open SourceAug 02, 2021 46min     The Untold Story of SQLiteJul 02, 2021 38min     From Competitive Programming to APLJun 02, 2021 53min     Smart Contract RescueMay 02, 2021 34min     Apple 2001Apr 03, 2021 48min     Video Game Programming From ScratchMar 01, 2021 41min     Reinforcement Learning At FacebookFeb 01, 2021 38min     2020 Year EndJan 01, 2021 34min     Frontiers of PerformanceDec 01, 2020 47min     The Birth of UNIXNov 01, 2020 51min     To The AssemblyOct 01, 2020 41min     Memento MoriSep 01, 2020 40min     We're Teaching Functional Programming WrongAug 03, 2020 46min     Software That Doesn't SuckJul 01, 2020 37min     Unproven Techology Case StudyJun 10, 2020 39min     Krystal's StoryMay 18, 2020 40min     Learning a new languageMay 05, 2020 35min     Portal Abstractions with Sam RitchieApr 17, 2020 35min     Loving Legacy Code with Jonathan BoccaraApr 03, 2020 26min     The Reason For TypesMar 16, 2020 36min     Karl L Hughes on Conference TalksMar 02, 2020 50min     Don and Adam Discuss FoldsFeb 15, 2020 36min     David Heinemeier HanssonFeb 01, 2020 1min     React and Scala JSJan 16, 2020 38min     The Business Of Developer ToolsDec 17, 2019 37min     Software in ContextDec 02, 2019 53min     Beautiful and Useless CodingNov 16, 2019 52min     Tech EvangelismNov 01, 2019 67min     Language Oriented DesignOct 01, 2019 56min     Open Source Health and DiversitySep 15, 2019 41min     Learning About CompilersSep 01, 2019 58min     Advanced Software DesignAug 16, 2019 53min     Category TheoryAug 15, 2019 53min     Using TypeScript Like A ProJul 15, 2019 70min     Rethinking Technological PositivismJun 15, 2019 61min     How to Build a Programming LanguageMay 31, 2019 56min     Refinement TypesMay 15, 2019 50min     Rethinking DatabasesApr 30, 2019 58min     Learning to ThinkApr 15, 2019 53min     Data and ScaleMar 31, 2019 56min     Abstraction and LearningMar 15, 2019 49min     Modern Systems ProgrammingFeb 22, 2019 0min     Recreational CodingJan 25, 2019 62min     Software as a Reflection of ValuesDec 18, 2018 79min     The Little TyperDec 01, 2018 67min     Big Ball Of MudNov 14, 2018 60min     God's Programming LanguageOct 22, 2018 60min     Concurrency and Functional ProgrammingOct 03, 2018 62min     Test in ProductionAug 31, 2018 47min     Domain Driven Design and Micro ServicesAug 17, 2018 49min     Typeful Functional Streaming HTTPJul 27, 2018 50min     Moves and Borrowing In RustJul 03, 2018 64min     Dependent Types in HaskellJun 13, 2018 58min     Microservices ArchitectureJun 06, 2018 66min     Rust And Bitter C++ DevelopersMay 16, 2018 62min     Distributed SystemsMay 02, 2018 66min     GraphqlApr 18, 2018 55min     PureScriptApr 04, 2018 51min     Throwaway the IrrelevantMar 21, 2018 68min     Generic ProgrammingMar 07, 2018 60min     Total Programming Using SwiftFeb 12, 2018 53min     Type Driven Development and IdrisJan 29, 2018 59min     Algebraic Domain Modelling using FunctionsJan 22, 2018 58min     Design Principles From Functional ProgrammingJan 10, 2018 51min     Scala at DuolingoJan 07, 2018 53min     Incident ResponseJan 05, 2018 51min     Scala NativeJan 01, 2018 48min\nFrom Project Management to Data Compression Innovator\nMay 02, 2023\n\n59\nmin\nJSON vs XML\nApr 03, 2023\n\n49\nmin\nSun's Mobile Blunders\nMar 02, 2023\n\n51\nmin\nShipping Graphing Calculator\nFeb 02, 2023\n\n46\nmin\nThe Unfulfilled Engineer\nJan 02, 2023\n\n42\nmin\nDOOMed to Fail: A Horror Story\nDec 02, 2022\n\n45\nmin\nSoftware World Tour\nNov 02, 2022\n\n48\nmin\nAndroid's Unlikely Success\nOct 03, 2022\n\n60\nmin\nFrom Prison To Programming\nSep 02, 2022\n\n46\nmin\nCPAN\nAug 01, 2022\n\n56\nmin\nThe History and Mystery Of Eliza\nJul 05, 2022\n\n44\nmin\nWhy still 80 columns?\nJun 01, 2022\n\n39\nmin\nLISP in Space\nMay 02, 2022\n\n38\nmin\nApril Fools' Is Cancelled (2014)\nApr 01, 2022\n\n38\nmin\nThe Story Graph\nMar 02, 2022\n\n48\nmin\nSerenity OS\nFeb 02, 2022\n\n41\nmin\nThe Internet Is Made of Duct Tape\nJan 02, 2022\n\n42\nmin\nCocoa Culture\nDec 02, 2021\n\n44\nmin\nLeaving Debian\nNov 02, 2021\n\n40\nmin\nThe Original Remote Developer\nOct 04, 2021\n\n41\nmin\nQuines, Polyglot Code and Other Fun Computations\nSep 02, 2021\n\n61\nmin\nFull-Time Open Source\nAug 02, 2021\n\n46\nmin\nThe Untold Story of SQLite\nJul 02, 2021\n\n38\nmin\nFrom Competitive Programming to APL\nJun 02, 2021\n\n53\nmin\nSmart Contract Rescue\nMay 02, 2021\n\n34\nmin\nApple 2001\nApr 03, 2021\n\n48\nmin\nVideo Game Programming From Scratch\nMar 01, 2021\n\n41\nmin\nReinforcement Learning At Facebook\nFeb 01, 2021\n\n38\nmin\n2020 Year End\nJan 01, 2021\n\n34\nmin\nFrontiers of Performance\nDec 01, 2020\n\n47\nmin\nThe Birth of UNIX\nNov 01, 2020\n\n51\nmin\nTo The Assembly\nOct 01, 2020\n\n41\nmin\nMemento Mori\nSep 01, 2020\n\n40\nmin\nWe're Teaching Functional Programming Wrong\nAug 03, 2020\n\n46\nmin\nSoftware That Doesn't Suck\nJul 01, 2020\n\n37\nmin\nUnproven Techology Case Study\nJun 10, 2020\n\n39\nmin\nKrystal's Story\nMay 18, 2020\n\n40\nmin\nLearning a new language\nMay 05, 2020\n\n35\nmin\nPortal Abstractions with Sam Ritchie\nApr 17, 2020\n\n35\nmin\nLoving Legacy Code with Jonathan Boccara\nApr 03, 2020\n\n26\nmin\nThe Reason For Types\nMar 16, 2020\n\n36\nmin\nKarl L Hughes on Conference Talks\nMar 02, 2020\n\n50\nmin\nDon and Adam Discuss Folds\nFeb 15, 2020\n\n36\nmin\nDavid Heinemeier Hansson\nFeb 01, 2020\n\n1\nmin\nReact and Scala JS\nJan 16, 2020\n\n38\nmin\nThe Business Of Developer Tools\nDec 17, 2019\n\n37\nmin\nSoftware in Context\nDec 02, 2019\n\n53\nmin\nBeautiful and Useless Coding\nNov 16, 2019\n\n52\nmin\nTech Evangelism\nNov 01, 2019\n\n67\nmin\nLanguage Oriented Design\nOct 01, 2019\n\n56\nmin\nOpen Source Health and Diversity\nSep 15, 2019\n\n41\nmin\nLearning About Compilers\nSep 01, 2019\n\n58\nmin\nAdvanced Software Design\nAug 16, 2019\n\n53\nmin\nCategory Theory\nAug 15, 2019\n\n53\nmin\nUsing TypeScript Like A Pro\nJul 15, 2019\n\n70\nmin\nRethinking Technological Positivism\nJun 15, 2019\n\n61\nmin\nHow to Build a Programming Language\nMay 31, 2019\n\n56\nmin\nRefinement Types\nMay 15, 2019\n\n50\nmin\nRethinking Databases\nApr 30, 2019\n\n58\nmin\nLearning to Think\nApr 15, 2019\n\n53\nmin\nData and Scale\nMar 31, 2019\n\n56\nmin\nAbstraction and Learning\nMar 15, 2019\n\n49\nmin\nModern Systems Programming\nFeb 22, 2019\n\n0\nmin\nRecreational Coding\nJan 25, 2019\n\n62\nmin\nSoftware as a Reflection of Values\nDec 18, 2018\n\n79\nmin\nThe Little Typer\nDec 01, 2018\n\n67\nmin\nBig Ball Of Mud\nNov 14, 2018\n\n60\nmin\nGod's Programming Language\nOct 22, 2018\n\n60\nmin\nConcurrency and Functional Programming\nOct 03, 2018\n\n62\nmin\nTest in Production\nAug 31, 2018\n\n47\nmin\nDomain Driven Design and Micro Services\nAug 17, 2018\n\n49\nmin\nTypeful Functional Streaming HTTP\nJul 27, 2018\n\n50\nmin\nMoves and Borrowing In Rust\nJul 03, 2018\n\n64\nmin\nDependent Types in Haskell\nJun 13, 2018\n\n58\nmin\nMicroservices Architecture\nJun 06, 2018\n\n66\nmin\nRust And Bitter C++ Developers\nMay 16, 2018\n\n62\nmin\nDistributed Systems\nMay 02, 2018\n\n66\nmin\nGraphql\nApr 18, 2018\n\n55\nmin\nPureScript\nApr 04, 2018\n\n51\nmin\nThrowaway the Irrelevant\nMar 21, 2018\n\n68\nmin\nGeneric Programming\nMar 07, 2018\n\n60\nmin\nTotal Programming Using Swift\nFeb 12, 2018\n\n53\nmin\nType Driven Development and Idris\nJan 29, 2018\n\n59\nmin\nAlgebraic Domain Modelling using Functions\nJan 22, 2018\n\n58\nmin\nDesign Principles From Functional Programming\nJan 10, 2018\n\n51\nmin\nScala at Duolingo\nJan 07, 2018\n\n53\nmin\nIncident Response\nJan 05, 2018\n\n51\nmin\nScala Native\nJan 01, 2018\n\n48\nmin\n"
    },
    {
        "id": 37,
        "person_name": "https://www.linkedin.com/in/softwaredoug",
        "text_description": "I like the idea of taking a minimalist approach to search as the trend now is to layer upon layer (vector DB + reranker + ... + whatever else). Erring towards simplicity as opposed to eeking out 0.0001 NDCG gain matters.",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 38,
        "person_name": "https://www.linkedin.com/in/julienchaumond",
        "text_description": "Some people said that closed APIs were winning...\n\nbut we will never give up the fight for open source AI ‚öîÔ∏è‚öîÔ∏è\n\nToday is a big day as we launch the first open source alternative to ChatGPT:\n\nHuggingChat üí¨\n\nPowered by Open Assistant's latest model ‚Äì the best open source chat model right now ‚Äì and Inference API.\n\nTry it out now",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 39,
        "person_name": "https://www.linkedin.com/in/muhammad-hammad-khan-b84822142",
        "text_description": "Attention fellow professionals!üëÄ Curious about Self-Supervised Learning but unsure where to start? Look no further!üîç\n\nThe release of a comprehensive cookbook of SSL recipes is here, featuring contributions from a large crowd of experts from Meta-FAIR and various academic collaborators, led by the brilliant and .\n\nThis amazing resource has everything you need to know about SSL, whether you're a seasoned practitioner or just starting out.\n\nSo, if you're ready to take your understanding of Self-Supervised Learning to the next level, be sure to check out this incredible resource!üìöüí°",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 40,
        "person_name": "https://www.linkedin.com/in/omarsar",
        "text_description": "Not sure how I missed it, but this is a great overview of efficient NLP methods.\n\nWhile models are scaled for performance, there is also a need to increase efficiency in modern NLP models. This survey paper covers methods and findings in efficient NLP.\n\n\n\nh/t",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 41,
        "person_name": "https://www.linkedin.com/in/alexstephany",
        "text_description": "üéâ YOU are invited to Beam's next community event!\n\nüëÄ Go behind the scenes at Beam HQ and meet our beneficiaries and the Beam team.\n\nüôã‚Äç‚ôÄÔ∏è Whether you‚Äôre a supporter, interested in working here or just curious - come on down!\n\nüìÖ Tues 23 May | 6 PM\n\nüëá Want to join? Add a comment and RSVP (link below)",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 42,
        "person_name": "https://www.linkedin.com/in/azin-asgarian",
        "text_description": "Hey Everyone, üëã\n\nWe are super excited to announce a special Transferred Learning session happening next Thursday, focusing on \"Multilingual Semantic Search.\" üîçüåç\n\nOur amazing keynote speaker, , will guide you on how to enhance your search quality while significantly reducing engineering complexity. üåü\n\nNils currently serves as the Head of Machine Learning at . He has a wealth of experience in this area from working as a research scientist at ü§ó to founding several companies! ‚ú®\n\nDon't miss out on this opportunity to learn from an industry expert. If you are already a TL member, you will receive an invite shortly. If you're not a member yet, please RSVP here ‚û°Ô∏è [] to secure your spot!\n\nAlso, feel free to share this event with colleagues who might be interested!",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 43,
        "person_name": "https://www.linkedin.com/in/momal-ijaz",
        "text_description": "I have been learning and practicing MLOps for almost 2.5 years now.\nHere are some key areas I have identified that one needs to work on to excel for ML Engineering or AI SWE roles.\n1Ô∏è‚É£ Cloud: AWS / Azure / GCP\nMost companies primarily use most services from AWS. You could try exploring S3, ECR, EKS, Terraform, SQS, EC2, CloudWatch and other vision/NLP specific services like Lex, Transcribe etc.\nFor starters check out this free AWS beginners course (my personal favourite instructor on Udemy) :\n>>> YouTube AWS Hands-on Demos Playlist\n\n2Ô∏è‚É£ System Design: By far this is one of the MOST important skills required to excel at MLOps, as it makes you capable of taking alot of important design decisions, while keeping important metrics and trade off in considerations.\nI have recently started reading: Designing Machine Learning Systems Production Ready by\n\nBesides you can also try building / drawing out system flow charts, for a dummy ML system, that you deploy to production for training and real time inference.\n\n3Ô∏è‚É£ All rounder view: There are tons of components to MLOps that have been amazingly and precisely covered in this course by in , it's text based absolutely free course and one of the best resources on MLOps out there to start from.\n\n\n4Ô∏è‚É£ Docker and Kubernetes: One thing you cannot deny the importance of in MLOps like DevOps is monitoring, shipping and autoscaling of your ML models or applications. That is exactly where\nDocker and kubernetes jumps in. Understanding images, containers, pods, clusters, services, deployments, and being able to write proper configurations for your k8s clusters, running you ML app, is super important skill.\nHands down, one of the best courses I just completed on Docker and kubernetes is from on Udemy, called Docker and Kubernetes, only for Rs. 3000/- but totally worth it.\n\nLemme what I missed in the comments!\nHappy Learning ‚ù§Ô∏è",
        "url_links": [],
        "url_texts": ""
    },
    {
        "id": 44,
        "person_name": "https://www.linkedin.com/in/lejinvarghese",
        "text_description": "..training deep learning models in the llm era requires efficient scaling strategies..this article from huggingface is a great introduction, starting with the efficient training on a single gpu..\n\nhttps://lnkd.in/gVYnWajU",
        "url_links": [
            "https://lnkd.in/gVYnWajU"
        ],
        "url_texts": "Models \t\t\t\t\tDatasets \t\t\t\t\tSpaces \t\t\t\t\tDocs  \n\t\t\tSolutions\n\t\t Pricing\n\t\t\t\t   \nLog In\n\t\t\t\t\nSign Up\nTransformers documentation\nEfficient Training on a Single GPU\n\nTransformers\nModels \t\t\t\t\tDatasets \t\t\t\t\tSpaces \t\t\t\t\tDocs  \t\t\tSolutions\t\t Pricing\t\t\t\t   Log In\t\t\t\tSign Up\nTransformers documentation\nEfficient Training on a Single GPU\nTransformers\nand get access to the augmented documentation experience\nto get started\nEfficient Training on a Single GPU\nThis guide focuses on training large models efficiently on a single GPU. These approaches are still valid if you have access to a machine with multiple GPUs but you will also have access to additional methods outlined in the multi-GPU section.\nIn this section we have a look at a few tricks to reduce the memory footprint and speed up training for large models and how they are integrated in the Trainer and ü§ó Accelerate. Each method can improve speed or memory usage which is summarized in the table below:\nA bracket means that it might not be strictly the case but is usually either not a main concern or negligible. Before we start make sure you have installed the following libraries:\nThe nvidia-ml-py3 library allows us to monitor the memory usage of the models from within Python. You might be familiar with the nvidia-smi command in the terminal - this library allows to access the same information in Python directly.\nThen we create some dummy data. We create random token IDs between 100 and 30000 and binary labels for a classifier. In total we get 512 sequences each with length 512 and store them in a Dataset with PyTorch format.\nWe want to print some summary statistics for the GPU utilization and the training run with the Trainer. We setup a two helper functions to do just that:\nLet's verify that we start with a free GPU memory:\nThat looks good: the GPU memory is not occupied as we would expect before we load any models. If that's not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU also the kernels are loaded which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well.\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the model uses.\nLoad Model\nFirst, we load the bert-large-uncased model. We load the model weights directly to the GPU so that we can check how much space just weights use.\nWe can see that the model weights alone take up 1.3 GB of the GPU memory. The exact number depends on the specific GPU you are using. Note that on newer GPUs a model can sometimes take up more space since the weights are loaded in an optimized fashion that speeds up the usage of the model. Now we can also quickly check if we get the same result as with nvidia-smi CLI:\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments that we will use across all our experiments:\nNote: In order to properly clear the memory after experiments we need restart the Python kernel between experiments. Run all steps above and then just one of the experiments below.\nVanilla Training\nAs a first experiment we will use the Trainer and train the model without any further modifications and a batch size of 4:\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our model's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model. To understand a bit better why this is the case let's have look at a model's operations and memory needs.\nAnatomy of Model's Operations\nTransformers architecture includes 3 main groups of operations grouped below by compute-intensity.\nTensor Contractions\nLinear layers and components of Multi-Head Attention all do batched matrix-matrix multiplications. These operations are the most compute-intensive part of training a transformer.\nStatistical Normalizations\nSoftmax and layer normalization are less compute-intensive than tensor contractions, and involve one or more reduction operations, the result of which is then applied via a map.\nElement-wise Operators\nThese are the remaining operators: biases, dropout, activations, and residual connections. These are the least compute-intensive operations.\nThis knowledge can be helpful to know when analyzing performance bottlenecks.\nThis summary is derived from Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020\nAnatomy of Model's Memory\nA typical model trained in mixed precision with AdamW requires 18 bytes per model parameter plus activation memory. For inference there are no optimizer states and gradients, so we can subtract those. And thus we end up with 6 bytes per model parameter for mixed precision inference, plus activation memory.\nLet's look at the details.\nModel Weights:\n4 bytes * number of parameters for fp32 training6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)\n4 bytes * number of parameters for fp32 training\n6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)\nOptimizer States:\n8 bytes * number of parameters for normal AdamW (maintains 2 states)2 bytes * number of parameters for 8-bit AdamW optimizers like bitsandbytes4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)\n8 bytes * number of parameters for normal AdamW (maintains 2 states)\n2 bytes * number of parameters for 8-bit AdamW optimizers like bitsandbytes\n4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)\nGradients\n4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)\n4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)\nForward Activations\nsize depends on many factors, the key ones being sequence length, hidden size and batch size.\nsize depends on many factors, the key ones being sequence length, hidden size and batch size.\nThere are the input and output that are being passed and returned by the forward and the backward functions and the forward activations saved for gradient computation.\nTemporary Memory\nAdditionally there are all kinds of temporary variables which get released once the calculation is done, but in the moment these could require additional memory and could push to OOM. Therefore when coding it's crucial to think strategically about such temporary variables and sometimes to explicitly free those as soon as they are no longer needed.\nFunctionality-specific memory\nThen your software could have special memory needs. For example, when generating text using beam search, the software needs to maintain multiple copies of inputs and outputs.\nforward vs backward Execution Speed\nFor convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually bandwidth-limited, and it's typical for an activation to have to read more data in the backward than in the forward (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward, and writes once, gradInput).\nSo there are potentially a few places where we could save GPU memory or speed up operations. Let's start with a simple optimization: choosing the right batch size.\nBatch sizes\nOne gets the most efficient performance when batch sizes and input/output neuron counts are divisible by a certain number, which typically starts at 8, but can be much higher as well. That number varies a lot depending on the specific hardware being used and the dtype of the model.\nFor example for fully connected layers (which correspond to GEMMs), NVIDIA provides recommendations for input/output neuron counts and batch size.\nTensor Core Requirements define the multiplier based on the dtype and the hardware. For example, for fp16 a multiple of 8 is recommended, but on A100 it's 64!\nFor parameters that are small, there is also Dimension Quantization Effects to consider, this is where tiling happens and the right multiplier can have a significant speedup.\nGradient Accumulation\nThe idea behind gradient accumulation is to instead of calculating the gradients for the whole batch at once to do it in smaller steps. The way we do that is to calculate the gradients iteratively in smaller batches by doing a forward and backward pass through the model and accumulating the gradients in the process. When enough gradients are accumulated we run the model's optimization step. This way we can easily increase the overall batch size to numbers that would never fit into the GPU's memory. In turn, however, the added forward and backward passes can slow down the training a bit.\nWe can use gradient accumulation in the Trainer by simply adding the gradient_accumulation_steps argument to TrainingArguments. Let's see how it impacts the models memory footprint:\nWe can see that the memory footprint was dramatically reduced at the cost of being only slightly slower than the vanilla run. Of course, this would change as you increase the number of accumulation steps. In general you would want to max out the GPU usage as much as possible. So in our case, the batch_size of 4 was already pretty close to the GPU's limit. If we wanted to train with a batch size of 64 we should not use per_device_train_batch_size=1 and gradient_accumulation_steps=64 but instead per_device_train_batch_size=4 and gradient_accumulation_steps=16 which has the same effective batch size while making better use of the available GPU resources.\nFor more details see the benchmarks for RTX-3090\nand A100.\nNext we have a look at another trick to save a little bit more GPU memory called gradient checkpointing.\nGradient Checkpointing\nEven when we set the batch size to 1 and use gradient accumulation we can still run out of memory when working with large models. In order to compute the gradients during the backward pass all activations from the forward pass are normally saved. This can create a big memory overhead. Alternatively, one could forget all activations during the forward pass and recompute them on demand during the backward pass. This would however add a significant computational overhead and slow down training.\nGradient checkpointing strikes a compromise between the two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. See this great article explaining the ideas behind gradient checkpointing.\nTo enable gradient checkpointing in the Trainer we only need to pass it as a flag to the TrainingArguments. Everything else is handled under the hood:\nWe can see that this saved some more memory but at the same time training became a bit slower. A general rule of thumb is that gradient checkpointing slows down training by about 20%. Let's have a look at another method with which we can regain some speed: mixed precision training.\nFloating Data Types\nThe idea of mixed precision training is that not all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variables and their computations are faster. Here are the commonly used floating point data types choice of which impacts both memory usage and throughput:\nfp32 (float32)fp16 (float16)bf16 (bfloat16)tf32 (CUDA internal data type)\nfp32 (float32)\nfp16 (float16)\nbf16 (bfloat16)\ntf32 (CUDA internal data type)\nHere is a diagram that shows how these data types correlate to each other.\n(source: NVIDIA Blog)\nWhile fp16 and fp32 have been around for quite some time, bf16 and tf32 are only available on the Ampere architecture GPUS and TPUs support bf16 as well. Let's start with the most commonly used method which is FP16 training/\nFP16 Training\nThe idea of mixed precision training is that not all variables need to be stored in full (32-bit) floating point precision. If we can reduce the precision the variales and their computations are faster. The main advantage comes from saving the activations in half (16-bit) precision. Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. Since the model is present on the GPU in both 16-bit and 32-bit precision this can use more GPU memory (1.5x the original model is on the GPU), especially for small batch sizes. Since some computations are performed in full and some in half precision this approach is also called mixed precision training. Enabling mixed precision training is also just a matter of setting the fp16 flag to True:\nWe can see that this is almost twice as fast as the vanilla training. Let's add it to the mix of the previous methods:\nWe can see that with these tweaks we use about half the GPU memory as at the beginning while also being slightly faster.\nBF16\nYou can enable BF16 in the ü§ó Trainer with:\nTF32\nIt's magical in the sense that you can use the normal fp32 training and/or inference code and by enabling tf32 support you can get up to 3x throughput improvement. All you need to do is to add this to your code:\nWhen this is done CUDA will automatically switch to using tf32 instead of fp32 where it's possible. This, of course, assumes that the used GPU is from the Ampere series.\nLike all cases with reduced precision this may or may not be satisfactory for your needs, so you have to experiment and see. According to NVIDIA research the majority of machine learning training shouldn't be impacted and showed the same perplexity and convergence as the fp32 training.\nIf you're already using fp16 or bf16 mixed precision it may help with the throughput as well.\nYou can enable this mode in the ü§ó Trainer with:\nBy default the PyTorch default is used.\nNote: tf32 mode is internal to CUDA and can't be accessed directly via tensor.to(dtype=torch.tf32) as torch.tf32 doesn't exist.\nNote: you need torch>=1.7 to enjoy this feature.\nYou can also see a variety of benchmarks on tf32 vs other precisions:\nRTX-3090 and\nA100.\nWe've now seen how we can change the floating types to increase throughput, but we are not done, yet! There is another area where we can save GPU memory: the optimizer.\nOptimizer\nThe most common optimizer used to train transformer model is Adam or AdamW (Adam with weight decay). Adam achieves good convergence by storing the rolling average of the previous gradients which, however, adds an additional memory footprint of the order of the number of model parameters. One remedy to this is to use an alternative optimizer such as Adafactor, which works well for some models but often it has instability issues.\nHF Trainer integrates a variety of optimisers that can be used out of box. To activate the desired optimizer simply pass the --optim flag to the command line.\nTo see which optimizers are currently supported:\nFor example, if you have NVIDIA/apex installed --optim adamw_apex_fused will give you the fastest training experience among all supported AdamW optimizers.\nOn the other hand 8bit BNB optimizer can save 3/4 of memory normally used by a typical AdamW optimizer if it is configured to quantize all optimizer states, but in some situations only some optimizer states are quintized and then more memory is used.\nLet's get a feel for the numbers and use for example use a 3B-parameter model, like t5-3b. Note that since a Gigabyte correpsonds to a billion bytes we can simply multiply the parameters (in billions) with the number of necessary bytes per parameter to get Gigabytes of GPU memory usage:\nA standard AdamW uses 8 bytes for each parameter, here the optimizer will need (8*3) 24GB of GPU memory.Adafactor uses slightly more than 4 bytes, so (4*3) 12GB and then some extra.8bit BNB quantized optimizer will use only (2*3) 6GB if all optimizer states are quantized.\nA standard AdamW uses 8 bytes for each parameter, here the optimizer will need (8*3) 24GB of GPU memory.\nAdafactor uses slightly more than 4 bytes, so (4*3) 12GB and then some extra.\n8bit BNB quantized optimizer will use only (2*3) 6GB if all optimizer states are quantized.\nLet's have a look at Adafactor first.\nAdafactor\nInstead of keeping the rolling average for each element in the weight matrices Adafactor only stores aggregated information (row- and column-wise sums of the rolling averages) which reduces the footprint considerably. One downside of Adafactor is that in some instances convergence can be slower than Adam's so some experimentation is advised here. We can use Adafactor simply by setting optim=\\\"adafactor\\\":\nWe can see that this saves a few more GB on the GPU. Let's see how it looks when we add it to the other methods we introduced earlier:\nWe went from 15 GB memory usage to 5 GB - a 3x improvement while maintaining the throughput! However, as mentioned before, the convergence of Adafactor can be worse than Adam. There is an alternative to Adafactor called 8-bit Adam that takes a slightly different approach.\n8-bit Adam\nInstead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind FP16 training where using variables with lower precision saves memory.\nIn contrast to the previous approaches is this one not integrated into the Trainer as a simple flag. We need to install the 8-bit optimizer and then pass it as a custom optimizer to the Trainer. Follow the installation guide in the Github repo to install the bitsandbytes library that implements the 8-bit Adam optimizer.\nOnce installed, we just need to initialize the the optimizer. Although this looks like a considerable amount of work it actually just involves two steps: first we need to group the model's parameters into two groups where to one group we apply weight decay and to the other we don't. Usually, biases and layer norm parameters are not weight decayed. Then in a second step we just do some argument housekeeping to use the same parameters as the previously used AdamW optimizer.\nWe can now pass the custom optimizer as an argument to the Trainer:\nWe can see that we get a similar memory improvement as with Adafactor while keeping the full rolling average of the gradients. Let's repeat the experiment with the full settings:\nAgain, we get about a 3x memory improvement and even slightly higher throughput as using Adafactor. So we have seen how we can optimize the memory footprint of large models. The following plot summarizes all our experiments:\n\n_multi_tensor\nUsing ü§ó Accelerate\nSo far we have used the Trainer to run the experiments but a more flexible alternative to that approach is to use ü§ó Accelerate. With ü§ó Accelerate you have full control over the training loop and can essentially write the loop in pure PyTorch with some minor modifications. In turn it allows you to easily scale across different infrastructures such as CPUs, GPUs, TPUs, or distributed multi-GPU setups without changing any code. Let's see what it takes to implement all of the above tweaks in ü§ó Accelerate. We can still use the TrainingArguments to wrap the training settings:\nThe full example training loop with ü§ó Accelerate is only a handful of lines of code long:\nFirst we wrap the dataset in a DataLoader. Then we can enable gradient checkpointing by calling the model's gradient_checkpointing_enable() method. When we initialize the Accelerator we can specify if we want to use mixed precision training and it will take care of it for us in the prepare call. During the prepare call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same 8-bit optimizer from the earlier experiments.\nFinally, we can write the main training loop. Note that the backward call is handled by ü§ó Accelerate. We can also see how gradient accumulation works: we normalize the loss so we get the average at the end of accumulation and once we have enough steps we run the optimization. Now the question is: does this use the same amount of memory as the previous steps? Let's check:\nIndeed it does. Implementing these optimization techniques with ü§ó Accelerate only takes a handful of lines of code and comes with the benefit of more flexiblity in the training loop. For a full documentation of all features have a look at the Accelerate documentation.\nDataLoader\nOne of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default everything happens in the main process and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization.\nDataLoader(pin_memory=True, ...) which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.DataLoader(num_workers=4, ...) - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it's far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won't necessarily lead to a better performance.\nDataLoader(pin_memory=True, ...) which ensures that the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\nDataLoader(num_workers=4, ...) - spawn several workers to pre-load data faster - during training watch the GPU utilization stats and if it‚Äôs far from 100% experiment with raising the number of workers. Of course, the problem could be elsewhere so a very big number of workers won‚Äôt necessarily lead to a better performance.\nDeepSpeed ZeRO\nThe in-depth details on how to use Deepspeed can be found here.\nFirst, a quick decision tree:\nNow if the decision tree suggested you use DeepSpeed first you need to install it, then follow one of the following guides to create a configuration file and launch DeepSpeed.\nActivation:\nHF Trainer-based examples: see this guide.Custom HF Trainer-based program: Same as above, but pass: \tCopiedTrainingArguments(deepspeed=\\\"/path/to/ds_config.json\\\")Deployment in Notebooks: see this guide.Custom training loop: This is somewhat complex but you can study how this is implemented in HF Trainer - simply search for deepspeed in the code.\nHF Trainer-based examples: see this guide.\nCustom HF Trainer-based program: Same as above, but pass:\n\n\n\tCopied\nTrainingArguments(deepspeed=\"/path/to/ds_config.json\")\nDeployment in Notebooks: see this guide.\nCustom training loop: This is somewhat complex but you can study how this is implemented in HF Trainer - simply search for deepspeed in the code.\nHF Trainer-based examples: see this guide.\nCustom HF Trainer-based program: Same as above, but pass:\nDeployment in Notebooks: see this guide.\nCustom training loop: This is somewhat complex but you can study how this is implemented in HF Trainer - simply search for deepspeed in the code.\nChoice of GPU\nNow, let's take a step back and discuss what we should optimize for when scaling the training of large models.\nHow to scale\nWhen we train models there are a two aspects we want to optimize at the same time:\nData throughput/training timeModel performance\nData throughput/training time\nModel performance\nWe have seen that each method changes the memory usage and throughput. In general we want to maximize the throughput (samples/second) to minimize the training cost. This is generally achieved by utilizing the GPU as much as possible and thus filling GPU memory to its limit. For example, as mentioned earlier, we only employ gradient accumulation when we want to use a batch size beyond the size of the GPU memory. If the desired batch size fits into memory then there is no reason to apply gradient accumulation which will only slow down training.\nThe second objective is model performance. Just because we can does not mean we should use a large batch size. As part of hyperparameter tuning you should determine which batch size yields the best result and then optimize the throughput accordingly.\nEfficient Software Prebuilds\nPyTorch's pip and conda builds come prebuit with the cuda toolkit which is enough to run PyTorch, but it is insufficient if you need to build cuda extensions.\nAt times it may take an additional effort to pre-build some components, e.g., if you're using libraries like apex that don't come pre-compiled. In other situations figuring out how to install the right cuda toolkit system-wide can be complicated. To address these users' needs PyTorch and NVIDIA release a new version of NGC docker container which already comes with everything prebuilt and you just need to install your programs on it and it will run out of the box.\nThis approach is also useful if you want to tweak the pytorch source and/or make a new customized build.\nTo find the docker image version you want start here, choose one of the latest monthly releases. Go into the release's notes for the desired release, check that the environment's components are matching your needs (including NVIDIA Driver requirements!) and then at the very top of that document go to the corresponding NGC page. If for some reason you get lost, here is the index of all PyTorch NGC images.\nNext follow the instructions to download and deploy the docker image.\nSparsity\nMixture of Experts\nQuite a few of the recent papers reported a 4-5x training speedup and a faster inference by integrating\nMixture of Experts (MoE) into the Transformer models.\nSince it has been discovered that more parameters lead to better performance, this technique allows to increase the number of parameters by an order of magnitude without increasing training costs.\nIn this approach every other FFN layer is replaced with a MoE Layer which consists of many experts, with a gated function that trains each expert in a balanced way depending on the input token's position in a sequence.\n\n(source: GLAM)\nYou can find exhaustive details and comparison tables in the papers listed at the end of this section.\nThe main drawback of this approach is that it requires staggering amounts of GPU memory - almost an order of magnitude larger than its dense equivalent. Various distillation and approaches are proposed to how to overcome the much higher memory requirements.\nThere is direct trade-off though, you can use just a few experts with a 2-3x smaller base model instead of dozens or hundreds experts leading to a 5x smaller model and thus increase the training speed moderately while increasing the memory requirements moderately as well.\nMost related papers and implementations are built around Tensorflow/TPUs:\nGShard: Scaling Giant Models with Conditional Computation and Automatic ShardingSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient SparsityGLaM: Generalist Language Model (GLaM)\nGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nGLaM: Generalist Language Model (GLaM)\nAnd for Pytorch DeepSpeed has built one as well: DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale, Mixture of Experts - blog posts:  1, 2 and specific deployment with large transformer-based natural language generation models: blog post, Megatron-Deepspeed branch.\nScaling beyond a single GPU\nFor some applications, such as pretraining large language models, applying all the approaches above might still not be fast enough. In this case you want to scale your experiment to several GPUs.\nAnother use case for training on many GPUs is if the model does not fit on a single GPU with all the mentioned tricks. There are still more methods we can apply although life starts to get a bit more complicated. This usually involves some form of pipeline or tensor parallelism where the model itself is distributed across several GPUs. One can also make use of DeepSpeed which implements some of these parallelism strategies along with some more optimization to reduce the memory footprint such as partitioning the optimizer states. You can read more about this in the \"Multi-GPU training\" section.\nUsing torch.compile\nPyTorch 2.0 introduces a new compile function, you can learn more about it in their documentation. It uses Python's frame evaluation API to automatically create a graph from existing PyTorch programs. After capturing the graph, different backends can be deployed to lower the graph to an optimized engine. You can choose one option below for performance boost.\ntorch.compile has a growing list of backends, which can be found in backends.py\nor torchdynamo.list_backends() each of which with its optional dependencies.\nSome of the most commonly used backends are\nDebugging backends:\ndynamo.optimize(\\\"eager\\\") - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.dynamo.optimize(\\\"aot_eager\\\") - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd's extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.\ndynamo.optimize(\"eager\") - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.\ndynamo.optimize(\"aot_eager\") - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd‚Äôs extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.\nTraining & inference backends:\ndynamo.optimize(\\\"inductor\\\") - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernels  Read moredynamo.optimize(\\\"nvfuser\\\") -  nvFuser with TorchScript. Read moredynamo.optimize(\\\"aot_nvfuser\\\") -  nvFuser with AotAutograd. Read moredynamo.optimize(\\\"aot_cudagraphs\\\") - cudagraphs with AotAutograd. Read more\ndynamo.optimize(\"inductor\") - Uses TorchInductor backend with AotAutograd and cudagraphs by leveraging codegened Triton kernels  Read more\ndynamo.optimize(\"nvfuser\") -  nvFuser with TorchScript. Read more\ndynamo.optimize(\"aot_nvfuser\") -  nvFuser with AotAutograd. Read more\ndynamo.optimize(\"aot_cudagraphs\") - cudagraphs with AotAutograd. Read more\nInference-only backends:\ndynamo.optimize(\\\"ofi\\\") -  Uses Torchscript optimize_for_inference.  Read moredynamo.optimize(\\\"fx2trt\\\") -  Uses Nvidia TensorRT for inference optimizations.  Read moredynamo.optimize(\\\"onnxrt\\\") -  Uses ONNXRT for inference on CPU/GPU.  Read moredynamo.optimize(\\\"ipex\\\") -  Uses IPEX for inference on CPU.  Read more\ndynamo.optimize(\"ofi\") -  Uses Torchscript optimize_for_inference.  Read more\ndynamo.optimize(\"fx2trt\") -  Uses Nvidia TensorRT for inference optimizations.  Read more\ndynamo.optimize(\"onnxrt\") -  Uses ONNXRT for inference on CPU/GPU.  Read more\ndynamo.optimize(\"ipex\") -  Uses IPEX for inference on CPU.  Read more\n"
    },
    {
        "id": 45,
        "person_name": "https://www.linkedin.com/in/aleksagordic",
        "text_description": "Interesting AI (ü§ñ) progress that happened over the past couple of days! üëá\n\n1) Meta released DINOv2 - version 2 of their DINO system that uses self-supervised learning to learn features useful for various computer vision tasks - thus can be used as a backbone for different tasks. Open-source!\n\nGitHub:\nBlog:\n\n2) 's \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\" paper: introduces a methodology for converting text to image models (LDMs in particular) into video generators! Looks really good. Progress!\n\nBlog:\nTwitter thread by the author:\n\n3) MiniGPT-4: a multimodal AI chat bot! Builds upon Vicuna-13B LLM (LLaMA derivative). This is something that I'm deeply interested in, as I used to work on visual language models back at (Flamingo).\n\nBlog:\n\n4) RedPajama - \"yet another\" effort to create the best possible open-source LLM! They just released a 1.2 trillion token dataset. Keep an eye out for them if this is something you care about.\n\nBlog:\n\nKeep learning ‚ù§Ô∏è",
        "url_links": [],
        "url_texts": ""
    }
]